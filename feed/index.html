<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jide Qian</title>
    <link href="http://www.geogebra.com.cn/feed/" rel="self" />
    <link href="http://qianjiye.de" />
    <lastbuilddate>2015-06-23T22:22:01+08:00</lastbuilddate>
    <webmaster>ccf.developer@gmail.com</webmaster>
    
    <item>
      <title>OpenCV(一)</title>
      <link href="http://www.geogebra.com.cn/2015/06/OpenCV(%E4%B8%80)" />
      <pubdate>2015-06-18T00:00:00+08:00</pubdate>
      <author>Jide Qian</author>
      <guid>http://www.geogebra.com.cn/2015/06/OpenCV(一)</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">功能模块介绍</h2>
<p>$Highgui:$包含图片和视频的读写显示函数，视频编码解码，还有图形界面鼠标用户交互函数.</p>

<p>$Core:$基本数据结构、动态数据结构、绘图函数、数组操作相关函数、辅助功能与系统函数和红定义、与OpenGL的互操作。</p>

<p>$Imgproc:$图像滤波、图像几何变换与转换、直方图相关、结构分析与形状描述、运动分析和对象跟踪、特征检测、目标检测等。</p>

<p>$Features2d:$包含特征点检测器、描述符提取器、特征点匹配框架、关键点绘制函数和匹配功能绘制函数。</p>

<p>$Objdetect:$包含了对象检测函数，比如脸部和人的检测.</p>

<p>$Calib3d:$包含了相机矫正、双视角的几何估计和立体函数.</p>

<p>$Video:$包含了动作估计，特征跟踪和前景提取函数与类.</p>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>Machine Learning(一)</title>
      <link href="http://www.geogebra.com.cn/2015/06/Machine-Learning" />
      <pubdate>2015-06-10T00:00:00+08:00</pubdate>
      <author>Jide Qian</author>
      <guid>http://www.geogebra.com.cn/2015/06/Machine-Learning</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">学习分类</h2>

<p><strong>有监督学习:</strong>回归、分类</p>

<p><strong>无监督学习:</strong>异常检测、聚类</p>

<h2 id="section-1">学习方法</h2>

<p><strong>判别式分类:</strong>计算后验概率 
$p(y|x)=\frac{p(x,y)}{p(x)}＝\frac{p(x,y)}{\sum_{y}{p(x,y)}}\varpropto p(x,y)$</p>

<p><strong>生成式分类:</strong>计算生成概率 $p(\textbf{x},y)$.</p>

<p><strong>关系：</strong>相对于后验概率
$p(y|x)$，数据生成概率 $p(x,y)$ 的计算时解决一般性(求解更加困难)的问题。然后，进行模式识别时只需计算出后验概率就足够了。在解决实际问题时，经常能够获得有关数据生成概率的一些先验知识。</p>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>基于ICA方法的低对比度表面图像缺陷检测</title>
      <link href="http://www.geogebra.com.cn/2015/05/Low-constrast" />
      <pubdate>2015-05-19T00:00:00+08:00</pubdate>
      <author>Jide Qian</author>
      <guid>http://www.geogebra.com.cn/2015/05/Low-constrast</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">论文链接</h2>
<p><a href="http://pan.baidu.com/s/1gd6HMIf">An independent component analysis-based filter design for defect detection in low-contrast surface images 
(2006,Pattern Recognition)</a> </p>

<h2 id="section-1">摘要</h2>

<blockquote>
  <p>©在这篇文章中，我们提出一个用卷积滤波器检测低对比度均匀图像表面中的小缺陷，并重点关注在LCD制造中的背光板和玻璃基板中的缺陷检测应用。成像表明在低对比度中图像中的缺陷与邻域没有明显的强度差异，更糟糕的是采集到的图像有不均匀的亮度。所有的这些情况使得在低对比度图像中的缺陷检测变得极端困难。<br />
©在这项研究中，我们提出了基于受约束的独立成分分析(ICA)模型设计一个最优滤波器，该卷积滤波器能够对没有噪声的背景光强产生最明显的响应。事先结合ICA模型约束所有没有缺陷的训练图像块初始像素值边界在一个控制限制允许的小间隔内。在这个检测过程中，相同的控制约束参数也被用在设置使对在完美的区域的像素发出脉冲响应的阈值在控制范围，同样在缺陷区域的响应阈值超越了这个控制范围。一个随机进化计算粒子群算法(PSO)被应用在去解决约束ICA模型。实验结果表明我们提出的模型能够有效的检测低对比度的背光板和玻璃基板图像中的小缺陷。</p>
</blockquote>

<ol>
  <li>主要检测对象：背光板、LCD制造中的玻璃基板</li>
  <li>检测对象成像特点：整幅图像对比度低、画面不均匀(边缘漏光). </li>
  <li></li>
</ol>

<h2 id="section-2">背景</h2>

<h3 id="section-3">需求</h3>

<h3 id="section-4">现有算法</h3>
<ol>
  <li>基于局部区域的统计特性的自适应的多级别阈值的点缺陷分割(2004)。<sup id="fnref:class_label_method_1"><a href="#fn:class_label_method_1" class="footnote">1</a></sup></li>
  <li>用遗传算法提取亮度不均匀图像中的连续边缘，实现在噪声中检测缺陷(1999)。 <sup id="fnref:class_label_method_2"><a href="#fn:class_label_method_2" class="footnote">2</a></sup></li>
  <li>采用亮度传感器而不是CCD相机作为信号信息采集设备，利用差方分析和指数加权移动平均技术去确定区域缺陷的存在(2005)。<sup id="fnref:class_label_method_3"><a href="#fn:class_label_method_3" class="footnote">3</a></sup></li>
  <li>建立一个无缺陷的标准参考模板，和待检测检测图像通过比较背景亮度分布进行缺陷查找(1992)。<sup id="fnref:class_label_method_4"><a href="#fn:class_label_method_4" class="footnote">4</a></sup></li>
  <li>应用低阶多项式数据拟合估计背景，减去估计的背景；然后找到最佳的二值化分割阈值，通过中值滤波和形态学关和开运算去处噪声。<strong>优点</strong>:效果比较理想。<strong>缺点</strong>:计算量巨大（消除一个像素需要递归遍历整幅图像），实用价值不高，急需改进提高。(2004)<sup id="fnref:class_label_method_5"><a href="#fn:class_label_method_5" class="footnote">5</a></sup></li>
</ol>

<h3 id="icaindependent-components-analysis">ICA(Independent components analysis)</h3>
<blockquote>
  <p>ICA:一种利用统计原理进行计算的方法,它是一个线性变换,把数据或信号分离成统计独立的非高斯的信号源的线性组合。(<strong>最重要的假设就是信号源统计独立</strong>)</p>

  <p>ICA已有的一些应用：</p>

  <ul>
    <li>医学型号处理(EEG、fMRI、MEG等)</li>
    <li>声音信号处理(鸡尾酒会问题)</li>
    <li>人脸识别、纹理分析、图像去噪等</li>
  </ul>
</blockquote>

<h3 id="section-5">难点</h3>
<ol>
  <li>局部的不均匀亮度也是平滑的改变没有清晰的边缘，不能应用基于梯度的缺陷检测方法</li>
  <li>检测对象的低对比度很难应用阈值分割方法</li>
</ol>

<h2 id="section-6">参考文献</h2>
<div class="footnotes">
  <ol>
    <li id="fn:class_label_method_1">
      <p>Detection of spot-type defects on liquid crystal display modules <a href="#fnref:class_label_method_1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:class_label_method_2">
      <p>Boundary extraction of brightness unevenness on LCD display using genetic algorithm based on perceptive grouping factors. <a href="#fnref:class_label_method_2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:class_label_method_3">
      <p>Liquid crystal display surface uniformity defect inspection using analysis of variance and exponentially weighted moving average techniques. <a href="#fnref:class_label_method_3" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:class_label_method_4">
      <p>Automatic vision system for final test of liquid crystal displays.(<a href="http://anton.treskunov.net/publications/IEEE92.pdf">PDF</a>) <a href="#fnref:class_label_method_4" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:class_label_method_5">
      <p>Automatic detection of region-mura defect in TFT-LCD. <a href="#fnref:class_label_method_5" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>基于局部均值的屏幕缺陷检测算法</title>
      <link href="http://www.geogebra.com.cn/2015/03/LCD" />
      <pubdate>2015-03-02T00:00:00+08:00</pubdate>
      <author>Jide Qian</author>
      <guid>http://www.geogebra.com.cn/2015/03/LCD</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">论文链接</h2>
<p><a href="http://pan.baidu.com/s/1pnTBS">Inspection of detect on LCD panel using local mean algorithm based on similarity</a></p>

<h2 id="section-1">主要内容</h2>
<ol>
  <li>计算发光LCD单元之间水平和垂直间距px,py</li>
  <li>通过临域差分和差分阈值参数查找缺陷</li>
  <li>局部均值算法寻找边缘缺陷点</li>
</ol>

<h2 id="pxpy">px,py计算</h2>
<p>首先，对于任意一个新的检测型号，我们需要获取这类型屏幕的LCD元件之间px,py距离参数，这是这个检测算法的关键，作者分别计算水平差分和、垂直差分和，然后得到差分和的与px、py的关系，分析得到参数px、py。
原文计算方法如下：
\begin{equation}
\begin{aligned}
diff_x = \sum_{px=1}^{n/2}\left( I_{x,y} - I_{x+px,y} \right) \newline
diff_y = \sum_{py=1}^{n/2}\left( I_{x,y} - I_{x,y+py} \right)
\end{aligned}
\end{equation}
得到积分和如下：</p>

<div class="image_line">
<div class="image_card">
<img src="/assets/images/2015-03-02-LCD_1.png" alt="差分和示意图" />
<div class="caption">差分和示意图</div>
</div>
</div>

<p>获取到屏幕的px、py参数，我们就可以通过对图像的屏幕区域进行缺陷检测，对于屏幕图像采用四领域的差分运算，</p>

<p>\begin{equation}
\begin{aligned}
D_{x,y} = \left| 4I_{x,y} - \left( _{+I_{x,y+py}+I_{x,y-py}}^{I_{x+px,y}\ +\ \ I_{x-px,y}} \right) \right|
\end{aligned}
\end{equation}</p>

<p>然后对差分图进行归一化处理，最后通过阈值$T$对归一化图像进行判断是否为缺陷。参数$T$是决定检测精确度的关键，这是一个需要通过大量试验得到的参数，作者给出的是$0.4\leq T\leq0.5$。</p>

<h2 id="section-2">局部均值缺陷检测算法</h2>
<p>首先，采用最小二乘法进行屏幕边界的定位；然后，通过局部均值算法进行缺陷检测，具体步骤如下:</p>

<blockquote>
  <ol>
    <li>遍历边缘点$I_{x,y}$，以当前点为中心选取大小为$N*N$窗口$R_w$</li>
    <li>遍历$R_w$的临域，同样选择大小为$N*N$的窗口$S_w$</li>
    <li>计算$R_w$与$S_w$的相似度，计算方法如下：
\begin{equation}
\begin{aligned}
\sum_{i=0}^{N}\sum_{j=0}^{N}\left( | R_{i,j} - S_{i,j} | \leq \Delta I\right) 
\end{aligned}
\end{equation}</li>
    <li>如果存在相似度大于$P_w$的窗口$S_w$,结束遍历,$R_w$为正常点，否则认为当前窗口$R_w$为缺陷</li>
  </ol>
</blockquote>

<p>说明：$\Delta I$:允许存在的差分误差。</p>

<h2 id="section-3">思考</h2>
<ol>
  <li>局部均值算法中，相对于$R_w$的对比临域$S_w$选取范围没有说清楚。</li>
  <li>主要参数$P_w$、$N$是一个经验值，对于不同的检测型号很可能不一样</li>
  <li>临域相似度计算复杂度比较高</li>
</ol>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习: 神经网络</title>
      <link href="http://www.geogebra.com.cn/2014/11/machine-learning-neural-networks" />
      <pubdate>2014-11-09T11:09:00+08:00</pubdate>
      <author>Jide Qian</author>
      <guid>http://www.geogebra.com.cn/2014/11/machine-learning-neural-networks</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">简介</h2>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-10-21-neural-networks_1.png"><img src="/assets/images/2014-10-21-neural-networks_1.png" alt="神经网络结构" /></a><div class="caption">Figure 1:  神经网络结构 [<a href="/assets/images/2014-10-21-neural-networks_1.png">PNG</a>]</div></div></div>

<p>神经网络是神经元分层级联构成的网络，除输入层外每个神经元对应一个计算模型。最左边和最右边的层分别称为输入（input）和输出（output）层，中间两层为隐藏层（hidden layer）。</p>

<p>当特征数目巨大时，简单的Logistic回归无法满足需求。神经网络用于解决复杂的非线性问题，可以看成是Logistic回归的组合，上图中每个橙色的神经元（除输入层之外）都对应一个Logistic方程。</p>

<p>对于分类问题，输入层输入原始数据，隐藏层的每个神经元可视为提取一种特征，输出层的每个神经元对应所属类别的概率（不是类别标签）。输入数据所属的类别是输出层概率最大神经元对应的类别。</p>

<p>神经网络通过前向传播计算给定输入对应的输出，通过误差反向传播估计权值矩阵。</p>

<h2 id="section-1">前向传播计算</h2>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-10-21-neural-networks_2.png"><img src="/assets/images/2014-10-21-neural-networks_2.png" alt="神经网络前向传播计算" /></a><div class="caption">Figure 2:  神经网络前向传播计算 [<a href="/assets/images/2014-10-21-neural-networks_2.png">PNG</a>]</div></div></div>

<p>神经网络前向传播，从输入到输出，逐层计算。上图所示<a href="#ng_ml_nnr_2014">[1, P. 23]</a>，假设权值矩阵$\Theta^{(l-1)}$已知，第$l$层可通过第$l-1$层和权值矩阵前向计算，</p>

<p>\begin{equation}
\mathbf a^{(l)} = g\left(\mathbf\Theta^{(l-1)}\mathbf a^{(l-1)}\right),
\label{eq:forward-propagation}
\end{equation}</p>

<p>$g$是<a href="/2014/10/machine-learning-logistic-regression/#mjx-eqn-eqsigmoid-function">Logistic函数</a>，每层额外增加了一个$\mathbf a_0^{(l)}= 1$的偏移（bias），$\mathbf\Theta^{(l-1)}$的行数为第$i$层神经元个数，列数为第$l-1$层神经元个数加$1$。</p>

<p>输出层（第$L$层）神经元的输出$\mathbf a^{(L)}$确定输入特征所属的类别。</p>

<p>如果上图去掉隐藏层只有输入和输出层，这就相当于一个Logistic回归模型。</p>

<h2 id="section-2">反向参数估计</h2>

<p>神经网络通过反向传播估计权值矩阵$\Theta$，参数估计仍然是最小化代价函数。通过BP算法（BackPropagation algorithm），输出层的误差向输入层逐层反向传播，利用梯度下降法，估计权值矩阵。</p>

<h3 id="section-3">代价函数</h3>

<p>神经网络的神经元是Logistic模型，存在和Logistic模型类似的代价函数</p>

<p>\begin{equation}
\begin{aligned}
J(\Theta) = &amp;-\frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}\left(y_k^{(i)}\log \left(h_\Theta\left(\mathbf x^{(i)} \right) \right)_k + \left(1 - y_k^{(i)}\right)\log\left(1 -  \left(h_\Theta\left(\mathbf x^{(i)} \right) \right)_k \right) \right) \\ 
&amp;+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}\left(\Theta_{ji}^{(l)}\right)^2.
\end{aligned}
\label{eq:cf_nn}
\end{equation}</p>

<p>$h_\Theta (x) \in \mathbb{R}^K$，$\left(h_\Theta (x)\right)_k = \mathbf a_k^{(L)} $是输出层第$k$个神经元的输出，可由前向传播公式\eqref{eq:forward-propagation}计算；$s_l$表示第$l$层神经元的个数（不含bias unit）；神经网络有$L$层，$m$个样本，$K$个输出。</p>

<p>如果代价函数\eqref{eq:cf_nn}是非凸（non-convex）函数<sup id="fnref:if_no_global_minimum"><a href="#fn:if_no_global_minimum" class="footnote">1</a></sup>，理论上可能会陷入局部极值，事实上，即使不能保证取得全局极值，梯度下降法也能很好的最小化代价函数，使得神经网络工作良好<a href="#ng_ml_nnl_2014">[2, P. 30]</a>。</p>

<p>对比<a href="/2014/10/machine-learning-regularization/#mjx-eqn-eqcf-logistic-regression-r">正则化Logistic回归的代价函数</a>，由于神经网络有$K$个输出，前半部分相当于$K$个Logistic回归的代价函数之和，后半部分是非bias神经元洗漱组成的正则化项，$\mathbf a_0^{(l)} = 1$对应的系数$\mathbf \Theta_{j0}^{(l)}$和Logistic回归一样，不包含在正则化系数中。</p>

<h3 id="section-4">参数估计</h3>

<p>通过最小化代价函数$\min_\Theta J(\Theta)$估计模型的所有参数矩阵$\Theta^{(l)}$，采用梯度下降法时需计算代价函数$J(\Theta)$及其梯度$\mathbf D_{ij}^{(l)}$，
\begin{equation*}
\mathbf D_{ij}^{(l)} = \frac{\partial J(\Theta)}{\partial\Theta_{ij}^{(l)}}.
\end{equation*}</p>

<p>第$l$层的误差记为$\mathbf\delta^{(l)}$，$\mathbf\delta_j^{(l)}$表示第$l$层的第$j$个神经元的误差，对于$\mathbf a_0^{(l)} = 1$的bias节点$\mathbf\delta_0^{(l)}=0$。输出层（$l=L$）的误差为
\begin{equation}
\mathbf\delta^{(L)} = \mathbf a^{(L)} - \mathbf y, 
\label{eq:error-bp-1}
\end{equation}
对于隐藏层$(l = L-1, L-2, \ldots, 2)$，误差通过权值矩阵$\Theta^{(l)}$从输出层向各隐藏层反向传播，
\begin{equation}
\mathbf\delta^{(l)} = \left(\Theta^{(l)}\right)^T\mathbf\delta^{(l+1)} .* g’\left(\mathbf z^{(l)}\right), 
\label{eq:error-bp-2}
\end{equation}</p>

<p>其中，$\mathbf z^{(l)} = \Theta^{(l)}\mathbf a^{(l)}$，$g’\left(\mathbf z^{(l)}\right) = \mathbf a^{(l)} .* \left(1 - \mathbf a^{(l)}\right)$。<code>.*</code>借用了Matlab中对应元素相乘的运算符。</p>

<p>Coursera的<a href="https://share.coursera.org/wiki/index.php/ML:Neural_Networks:_Learning">课程Wiki</a>和Michael Nielsen<a href="#nielsen_nndl_2014">[3]</a>的第二章给出了BP算法的推导过程<sup id="fnref:parameter_estimation"><a href="#fn:parameter_estimation" class="footnote">2</a></sup>。</p>

<blockquote>
  <h4 id="bp">BP算法</h4>
  <hr />
  <p>训练集：$\left\{\left(\mathbf x^{(1)}, \mathbf y^{(1)}\right),\ldots,\left(\mathbf x^{(m)}, \mathbf y^{(m)}\right)\right\}$。 </p>

  <p>初始化： <br />
1. $\Delta_{ij}^{(l)} = 0$； <br />
2. 随机数初始化$\Theta_{ij}^{(l)}$为$[-\epsilon, \epsilon]$的值，$\epsilon=\frac{\sqrt{6}}{\sqrt{L_{in} + L_{out}}}$由神经元数目确定，其中，$L_{in} = s_l$，$L_{out}=s_{l+1}$。<sup id="fnref:initial_theta"><a href="#fn:initial_theta" class="footnote">3</a></sup>  </p>

  <p>for $i=1$ to $m$ {  <br />
1. 初始化输入层，$\mathbf a^{(1)} = \mathbf x^{(i)}$； <br />
2. 利用前向传播\eqref{eq:forward-propagation}，计算各层神经元的值$\mathbf a^{(l)}~~(l = 2, 3,\ldots, L)$；<br />
3. 利用反向传播\eqref{eq:error-bp-1}和\eqref{eq:error-bp-2}，计算各层误差$\mathbf \delta^{(l)}$； <br />
4. 更新$\Delta_{ij}^{(l)}$，$\Delta_{ij}^{(l)} := \Delta_{ij}^{(l)} + \mathbf a_j^{(l)}\delta_i^{(l+1)}$<sup id="fnref:vector_update_Delta"><a href="#fn:vector_update_Delta" class="footnote">4</a></sup>（$\mathbf a^{(l)}$也须补$\mathbf a_0^{(l)}=1$）； <br />
5. 计算梯度$\mathbf D_{ij}^{(l)}$，
\begin{equation}
\mathbf D_{ij}^{(l)} := \left\{
\begin{aligned}
&amp; \frac{1}{m}\left(\Delta_{ij}^{(l)} + \lambda\Theta_{ij}^{(l)}\right) &amp; (j \neq 0) \\
&amp; \frac{1}{m}\Delta_{ij}^{(l)} &amp; (j = 0)
\end{aligned} 
\right. .
\end{equation}
}</p>
</blockquote>

<h3 id="section-5">实现细节</h3>

<p>算法技巧：矩阵展成（unroll）向量：</p>

<ol>
  <li><code class="highlight language-matlab"><span class="n">thetaVec</span> <span class="p">=</span> <span class="p">[</span><span class="n">Theta1</span><span class="p">(:);</span> <span class="n">Theta2</span><span class="p">(:);</span> <span class="n">Theta3</span><span class="p">(:)]</span></code>；</li>
  <li>将向量化的待估参数作为costFunction的参数；</li>
  <li>costFunction内部再将向量还原为矩阵计算梯度；</li>
  <li>梯度向量化输出<code class="highlight language-matlab"><span class="n">DVec</span> <span class="p">=</span> <span class="p">[</span><span class="n">D1</span><span class="p">(:);</span> <span class="n">D2</span><span class="p">(:);</span> <span class="n">D3</span><span class="p">(:)]</span></code>。</li>
</ol>

<p>算法技巧：梯度检查（gradient checking）：</p>

<p>梯度检测方法也可推广到其它需要计算代价函数及其梯度的地方，比如logistic回归的代价函数。</p>

<div class="highlight"><pre><code class="language-matlab"><span class="k">for</span> <span class="nb">i</span> <span class="p">=</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">n</span>
   <span class="n">thetaPlus</span> <span class="p">=</span> <span class="n">theta</span><span class="p">;</span>
   <span class="n">thetaPlus</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="n">thetaPlus</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="o">+</span> <span class="n">EPSILON</span><span class="p">;</span>
   <span class="n">thetaMinus</span> <span class="p">=</span> <span class="n">theta</span><span class="p">;</span>
   <span class="n">thetaMinus</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="n">thetaMinus</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> – <span class="n">EPSILON</span><span class="p">;</span>
   <span class="n">gradApprox</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="p">(</span><span class="n">J</span><span class="p">(</span><span class="n">thetaPlus</span><span class="p">)</span> – <span class="n">J</span><span class="p">(</span><span class="n">thetaMinus</span><span class="p">));</span>
<span class="k">end</span></code></pre></div>

<blockquote>
  <p>theta 是$\Theta$的向量化，正常情况有gradApprox$\approx$DVec，通过比较gradApprox 与BP 算法所得DVec 的差距判断BP 算法的代价函数及其优化算法是否有subtle bugs。</p>
</blockquote>

<div class="highlight"><pre><code class="language-matlab"><span class="c">% If your backpropagation implementation is correct, then the relative difference will be small (less than 1e-9). </span>
<span class="n">diff</span> <span class="p">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">gradApprox</span> <span class="o">-</span> <span class="n">DVec</span><span class="p">)</span> <span class="o">/</span> <span class="n">norm</span><span class="p">(</span><span class="n">gradApprox</span> <span class="o">+</span> <span class="n">DVec</span><span class="p">);</span></code></pre></div>

<blockquote>
  <p>梯度检查应当在训练神经网络之前，可以通过构造一个新的较小规模的神经网络进行检验；若每次训练都检测梯度，速度很慢。</p>
</blockquote>

<p>注意事项：</p>

<p>不可将$\Theta_{ij}^{(l)}$初始化为$0$，若初始化为$0$，每层的所有神经元都是一样的。随机数初始化$-\epsilon\leq\Theta_{ij}^{(l)}\leq\epsilon$，选择$\epsilon$的有效策略是根据每层神经元的数目取$\epsilon=\frac{\sqrt{6}}{\sqrt{L_{in} + L_{out}}}~(L_{in} = s_l,L_{out}=s_{l+1})$，例如：</p>

<div class="highlight"><pre><code class="language-matlab"><span class="n">Theta1</span> <span class="p">=</span>  <span class="nb">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">INIT_EPSILON</span><span class="p">)</span> <span class="o">-</span> <span class="n">INIT_EPSILON</span><span class="p">;</span>
<span class="n">Theta2</span> <span class="p">=</span>  <span class="nb">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">INIT_EPSILON</span><span class="p">)</span> <span class="o">-</span> <span class="n">INIT_EPSILON</span><span class="p">;</span></code></pre></div>

<h2 id="section-6">应用</h2>

<p>卡内基梅隆大学基于神经网络的自动驾驶系统<a href="#pomerleau_alvinn_1989">[5]</a>，一些Matlab代码和数据还可以从<a href="http://www.cs.cmu.edu/afs/cs/academic/class/15782-f06/matlab/alvinn/">这里</a>找到。</p>

<h2 id="section-7">参考资料</h2>

<ol class="bibliography"><li><span id="ng_ml_nnr_2014">[1]A. Ng, “Neural Networks: Representation.” Coursera, 2014.</span>

[<a href="https://class.coursera.org/ml-007">Online</a>]


</li>
<li><span id="ng_ml_nnl_2014">[2]A. Ng, “Neural Networks: Learning.” Coursera, 2014.</span>

[<a href="https://class.coursera.org/ml-007">Online</a>]


</li>
<li><span id="nielsen_nndl_2014">[3]M. A. Nielsen, <i>Neural Networks and Deep Learning</i>. Determination Press, 2014.</span>

[<a href="http://neuralnetworksanddeeplearning.com">Online</a>]


</li>
<li><span id="ng_ml_nnl_pe_2014">[4]A. Ng, “Programming Exercise 4: Neural Networks Learning.” Coursera, 2014.</span>

[<a href="https://class.coursera.org/ml-007">Online</a>]


</li>
<li><span id="pomerleau_alvinn_1989">[5]D. A. Pomerleau, “ALVINN, an autonomous land vehicle in a neural network,” Carnegie Mellon University, 1989.</span>

[<a href="http://repository.cmu.edu/cgi/viewcontent.cgi?article=2874&amp;context=compsci">Online</a>]


</li></ol>

<h3 id="section-8">脚注</h3>
<div class="footnotes">
  <ol>
    <li id="fn:if_no_global_minimum">
      <p>如果非凸函数，梯度下降法不能确定取得的是全局还是局部极值，可通过<a href="https://class.coursera.org/ml-007/forum/thread?thread_id=1089#comment-3416">取不同初始值多次求解增强鲁棒性</a>。 <a href="#fnref:if_no_global_minimum" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:parameter_estimation">
      <p>这两篇博客（<a href="http://blog.csdn.net/abcjennifer/article/details/7758797">1</a>、<a href="http://blog.csdn.net/sheng_ai/article/details/19931347">2</a>）也给出了BP算法的推导过程，但是采用了形如<a href="/2014/10/machine-learning-linear-regression/#mjx-eqn-eqcost_function_linear_regression">线性回归的代价函数</a>。 <a href="#fnref:parameter_estimation" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:initial_theta">
      <p>不可将$\Theta_{ij}^{(l)}$初始化为$0$。若初始化为$0$，每层的所有神经元都一样<a href="#ng_ml_nnl_2014">[2, Pp. 25-26]</a>，每层只能学习到一种特征。$\epsilon$的取值方案参考课程习题脚注<a href="#ng_ml_nnl_pe_2014">[4, P. 7]</a>或<a href="https://share.coursera.org/wiki/index.php/ML:Neural_Networks:_Learning">课程Wiki</a>。 <a href="#fnref:initial_theta" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:vector_update_Delta">
      <p>更新的向量形式$\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}\left(\mathbf a^{(l)}\right)^T$。这一步是怎么来的？有何意义？ <a href="#fnref:vector_update_Delta" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>CSS Essential</title>
      <link href="http://www.geogebra.com.cn/2014/10/css-essential" />
      <pubdate>2014-10-25T15:37:41+08:00</pubdate>
      <author>Jide Qian</author>
      <guid>http://www.geogebra.com.cn/2014/10/css-essential</guid>
      <content:encoded>&lt;![CDATA[<blockquote>
  <h4 id="what-is-css">What is CSS?</h4>
  <hr />
  <p>The basic goal of the Cascading Stylesheet (CSS) language is to allow a browser engine to paint elements of the page with specific features, like colors, positioning, or decorations. </p>

  <p>CSS主要作用是设定元素（通常称为盒子）的属性，并将设定这些元素在页面的位置关系。CSS解决的主要问题是：    </p>

  <ol>
    <li>如何选择指定的元素？选择器（Selector）   </li>
    <li>如何设定元素的属性？盒子模型（Box Model）   </li>
    <li>如何设置元素间的位置关系？定位机制（Positioning Scheme）</li>
  </ol>
</blockquote>

<p>本文主要介绍CSS的基本用法，包括如何使用选择器，如何设定样式以及如何布局定位，这些涉及到选择器、盒子模型、普通流、可视化格式模型和块格式化环境等核心概念，理解这些核心概念是熟练运用CSS的基础。</p>

<p>若要详细了解相关内容，可参考文中链接，以及<a href="https://developer.mozilla.org/en-US/docs/Web/CSS" title="Cascading Style Sheets">MDN</a>、<a href="http://www.w3.org/Style/CSS/" title="Cascading Style Sheets home page">W3</a>和<a href="http://www.w3school.com.cn/css/index.asp" title="CSS 教程">w3school</a>关于CSS的详细介绍。</p>

<h2 id="section">基础知识</h2>

<h3 id="section-1">基本语法</h3>

<p>CCS语法元素主要由选择器，属性和值构成，通过规则给选择器范围内的属性赋值。   </p>

<div class="image_line">
<div class="image_card">
<img src="/assets/images/2014-10-25-css-essential_css-syntax-ruleset.png" alt="CSS语法" />
<div class="caption">CSS语法（规则）</div>
</div>
</div>

<!---
<div class="image_card">
<img src="/assets/images/2014-10-21-pla_1.png" alt="CSS语法">
<div class="caption">CSS语法（规则）</div>
</div>
--->

<h4 id="section-2">各部分元素含义：</h4>

<ul>
  <li><code>div p</code>：属性选择器</li>
  <li><code>#id</code>：id选择器</li>
  <li><code>first_line</code>：伪元素（pseudo-element）</li>
  <li><code>background-color</code>：属性（property）</li>
  <li><code>red</code>：值（value）</li>
</ul>

<h3 id="section-3">基本选择器</h3>

<p>选择器相当于作用域规则，选择属性设置起作用的范围。另一个相关概念是<a href="#block-formatting-context">块格式化环境</a>。</p>

<h4 id="section-4">常用选择器</h4>

<table>
  <tbody>
    <tr>
      <td>*</td>
      <td>通用选择器，匹配任何元素。</td>
    </tr>
    <tr>
      <td>E</td>
      <td>标签选择器，匹配所有使用E标签的元素。</td>
    </tr>
    <tr>
      <td>.info</td>
      <td>class选择器，匹配所有class属性中包含info的元素。</td>
    </tr>
    <tr>
      <td>#footer</td>
      <td>id选择器，匹配所有id属性等于footer的元素。</td>
    </tr>
  </tbody>
</table>

<p><strong>id选择器和class选择器的区别：</strong></p>

<ul>
  <li>在CSS文件中，id加前缀<code>#</code>，class用<code>.</code>；</li>
  <li>同一个页面id只可使用一次，class可多次引用：id是一个标签，用于区分不同的结构和内容，就象名字；class是一个样式，可套在任何结构和内容上，就象衣服，同一内容也可以套多个class的样式；</li>
  <li>从概念上说不一样：id是先找到结构/内容，再给它定义样式；class是先定义好一种样式，再套给多个结构/内容。</li>
</ul>

<div class="highlight"><pre><code class="language-html" data-lang="html"><span class="nt">&lt;style&gt;</span>
<span class="nc">.content</span> <span class="p">{</span><span class="k">font-size</span><span class="o">:</span> <span class="m">1.2em</span><span class="p">;</span> <span class="k">line-height</span><span class="o">:</span> <span class="m">200%</span><span class="p">}</span>
<span class="nc">.emphasis</span> <span class="p">{</span><span class="k">font-weight</span><span class="o">:</span> <span class="k">bold</span><span class="p">}</span>
<span class="nf">#lily</span> <span class="p">{</span><span class="k">color</span><span class="o">:</span> <span class="nb">white</span><span class="p">;</span> <span class="k">background</span><span class="o">:</span> <span class="nb">black</span><span class="p">}</span>
<span class="nt">&lt;/style&gt;</span>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;content emphasis&quot;</span> <span class="na">id=</span><span class="s">&quot;lily&quot;</span><span class="nt">&gt;</span>The flower of lily of the valley is like tinkler, be born at spending cauline top to show raceme.<span class="nt">&lt;/div&gt;</span>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;content&quot;</span> <span class="na">id=</span><span class="s">&quot;lavender&quot;</span><span class="nt">&gt;</span>The air was fragrant with lavender.<span class="nt">&lt;/div&gt;</span></code></pre></div>

<h4 id="section-5">多选择器组合</h4>

<table>
  <tbody>
    <tr>
      <td>E,F</td>
      <td>多元素选择器，同时匹配所有E元素或F元素，E和F之间用逗号分隔。</td>
    </tr>
    <tr>
      <td>E F</td>
      <td>后代元素选择器，匹配所有属于E元素后代的F元素，E和F之间用空格分隔。</td>
    </tr>
    <tr>
      <td>E &gt; F</td>
      <td>子元素选择器，匹配所有E元素的子元素F。</td>
    </tr>
    <tr>
      <td>E + F</td>
      <td>毗邻元素选择器，匹配所有紧随E元素之后的同级元素F。</td>
    </tr>
  </tbody>
</table>

<p>如果两个选择器紧连，表示同时满足两个条件的内容。</p>

<div class="highlight"><pre><code class="language-html" data-lang="html"><span class="nt">&lt;style&gt;</span>
<span class="nt">div</span><span class="nc">.content</span> <span class="p">{</span><span class="k">font-size</span><span class="o">:</span> <span class="m">1.2em</span><span class="p">;</span> <span class="k">line-height</span><span class="o">:</span> <span class="m">200%</span><span class="p">}</span>
<span class="nt">div</span><span class="nc">.emphasis</span> <span class="p">{</span><span class="k">font-weight</span><span class="o">:</span> <span class="k">bold</span><span class="p">}</span>
<span class="nt">div</span><span class="nf">#lily</span> <span class="p">{</span><span class="k">color</span><span class="o">:</span> <span class="nb">white</span><span class="p">;</span> <span class="k">background</span><span class="o">:</span> <span class="nb">black</span><span class="p">}</span>
<span class="nt">&lt;/style&gt;</span>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;content emphasis&quot;</span> <span class="na">id=</span><span class="s">&quot;lily&quot;</span><span class="nt">&gt;</span>
The flower of lily of the valley is like tinkler, be born at spending cauline top to show raceme.
<span class="nt">&lt;/div&gt;</span></code></pre></div>

<p>上例中，<code>div.emphasis</code>表示套用<code>.emphasis</code>样式的<code>div</code>标签。</p>

<h3 id="section-6">优先级别</h3>

<p>基本规则是<code>行内样式 &gt; id样式 &gt; class样式 &gt; 标签名样式</code>，也就是，选择越具体优先级越高。如下元素：   </p>

<div class="highlight"><pre><code class="language-css" data-lang="css"><span class="o">&lt;</span><span class="nt">div</span> <span class="nt">id</span><span class="o">=</span><span class="s2">&quot;ID&quot;</span> <span class="nt">class</span><span class="o">=</span><span class="s2">&quot;CLASS&quot;</span> <span class="nt">style</span><span class="o">=</span><span class="s2">&quot;color:black;&quot;</span><span class="o">&gt;&lt;/</span><span class="nt">div</span><span class="o">&gt;</span></code></pre></div>

<p>作用在其上样式的优先级从低到高是<code>div &lt; .CLASS &lt; div.CLASS &lt; #ID &lt; div#ID &lt; #ID.CLASS &lt; div#ID.CLASS</code>。</p>

<p><strong>继承与覆盖：</strong></p>

<p>子元素没有设置的属性会从父元素继承而来。同一元素的同一属性如有多次设置，优先级高的覆盖优先级低的；若是同一优先级，后设设置覆盖前设置。</p>

<div class="highlight"><pre><code class="language-html" data-lang="html"><span class="nt">&lt;style&gt;</span>
<span class="nt">div</span> <span class="p">{</span><span class="k">font-size</span><span class="o">:</span> <span class="m">1em</span><span class="p">}</span>
<span class="nt">div</span><span class="nc">.content</span> <span class="p">{</span><span class="k">font-size</span><span class="o">:</span> <span class="m">1.2em</span><span class="p">;</span> <span class="k">line-height</span><span class="o">:</span> <span class="m">200%</span><span class="p">}</span>
<span class="nt">div</span><span class="nc">.emphasis</span> <span class="p">{</span><span class="k">font-size</span><span class="o">:</span> <span class="m">1.4em</span><span class="p">;</span> <span class="k">font-weight</span><span class="o">:</span> <span class="k">bold</span><span class="p">}</span>
<span class="nt">div</span><span class="nf">#lily</span> <span class="p">{</span><span class="k">color</span><span class="o">:</span> <span class="nb">white</span><span class="p">;</span> <span class="k">background</span><span class="o">:</span> <span class="nb">black</span><span class="p">}</span>
<span class="nt">&lt;/style&gt;</span>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;content emphasis&quot;</span> <span class="na">id=</span><span class="s">&quot;lily&quot;</span><span class="nt">&gt;</span>
The flower of lily of the valley is like tinkler, be born at spending cauline top to show raceme.
<span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">&quot;water_lily&quot;</span><span class="nt">&gt;&lt;/div&gt;</span>
<span class="nt">&lt;/div&gt;</span></code></pre></div>

<p>上例中，<code>div</code>设定的<code>font-size</code>优先级太低，不会生效；<code>.emphasis</code>覆盖<code>.content</code>的<code>font-size</code>，<code>#lily</code>的<code>font-size</code>最终为<code>1.4em</code>；<code>#water_lily</code>继承了<code>#lily</code>的所有属性。</p>

<h3 id="section-7">属性赋值</h3>

<p>确定属性最终取值要<a href="http://www.w3.org/TR/2001/WD-css3-values-20010713/#specified" title="Specified, computed, and actual values">经历3步</a>（<a href="http://www.w3.org/TR/CSS2/cascade.html#value-stages" title="Specified, computed, and actual values">CSS2经历4步</a>）：首先获取CSS样式中的指定值（specified value），然后如有必要则转换为绝对值或计算值（computed value），最后根据局部环境的约束再转换为实际值（actual value）。</p>

<p>指定值可能是绝对值（例如：<code>2mm</code>、<code>red</code>等），也可能是相对值（例如：<code>auto</code>、<code>1.2em</code>、<code>12%</code>等）。对于绝对值而言不需要计算即可获得计算值，相对值需要再借助参考值计算获得计算值。如果属性没有指定值，它的取值继承父元素。</p>

<p>在特定的客户端环境，可能还需要将计算值转换为实际值，比如可能需要将小数的边界近似取整。</p>

<blockquote>
  <h4 id="section-8">常用的赋值规则</h4>
  <hr />
  <ol>
    <li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/color_value" title="&lt;color&gt;">颜色</a>赋值的三种形式：关键字、RGB空间、HSL空间；</li>
    <li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/length" title="&lt;length&gt;">长度</a>赋值的两种形式：以<code>em</code>、<code>ex</code>、<code>ch</code>、<code>rem</code>、<code>vw</code>、<code>vh</code>、<code>vmin</code>、<code>vmax</code>为单位的相对赋值，以<code>cm</code>、<code>mm</code>、<code>in</code>、<code>pt</code>、<code>pc</code>、<code>px</code>为单位的绝对赋值；</li>
    <li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/percentage" title="&lt;percentage&gt;">百分数</a>赋值：<code>width</code>、<code>margin</code>和<code>padding</code>等接受百分数赋值。</li>
  </ol>

  <p>示例：    </p>

  <p><code>em</code>等单位和百分数都是相对赋值，需要继承父属性的参考值。<code>em</code>是针对字体大小的值，$w$ <code>em</code> = $w$ $\times$ <code>font-size</code>。</p>

  <div class="highlight"><pre><code class="language-html" data-lang="html"><span class="nt">&lt;div</span> <span class="na">style=</span><span class="s">&quot;font-size:18px;&quot;</span><span class="nt">&gt;</span>
  Full size text (18px)
  <span class="nt">&lt;span</span> <span class="na">style=</span><span class="s">&quot;font-size:50%;&quot;</span><span class="nt">&gt;</span>50%<span class="nt">&lt;/span&gt;</span>
  <span class="nt">&lt;span</span> <span class="na">style=</span><span class="s">&quot;font-size:200%;&quot;</span><span class="nt">&gt;</span>200%<span class="nt">&lt;/span&gt;</span>
<span class="nt">&lt;/div&gt;</span></code></pre></div>
  <p>上述代码中，<code>50%</code>从父属性继承而来的值是<code>18px</code>，然后再乘以<code>50%</code>（等价于<code>0.5em</code>），实际大小是<code>9px</code>（<a href="https://developer.mozilla.org/en-US/docs/Web/CSS/percentage" title="&lt;percentage&gt;">查看效果</a>）。</p>
</blockquote>

<p>当一个属性可以有多个方向可设置值时，存在形如以下简写的赋值规则：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">位置</th>
      <th style="text-align: left">赋值规则</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/assets/images/2014-10-25-css-essential_border1.png" alt="1" /></td>
      <td style="text-align: left"><code>border-width: 1em</code></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/assets/images/2014-10-25-css-essential_border2.png" alt="2" /></td>
      <td style="text-align: left"><code>border-width: 1em 2em</code></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/assets/images/2014-10-25-css-essential_border3.png" alt="3" /></td>
      <td style="text-align: left"><code>border-width: 1em 2em 3em</code></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/assets/images/2014-10-25-css-essential_border4.png" alt="4" /></td>
      <td style="text-align: left"><code>border-width: 1em 2em 3em 4em</code></td>
    </tr>
  </tbody>
</table>

<h2 id="section-9">盒子模型</h2>

<p>CSS将HTML的元素（可认为是标签）定义为适合CSS处理的矩形盒（rectangular box）。<a href="https://developer.mozilla.org/en-US/docs/Web/CSS/box_model">盒子模型（box model）</a>描述了这些矩形盒的尺寸、属性（颜色、背景和边框等）和位置特性，浏览器根据盒子模型实现页面的渲染与显示。</p>

<p>按照HTML的元素是否新开一行可分为块（block-level）元素和内联（inline）元素，这一特性对设置元素的布局至关重要。</p>

<blockquote>
  <h4 id="htmlblock-levelinline">HTML的块（block-level）元素和内联（inline）元素</h4>
  <hr />
  <p><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Block-level_elements">块元素</a>充满父元素的所有空间，每个块元素都会另起一个新行显示。块元素之内还可以包含块元素和行内元素。HTML定义的块元素包括<code>&lt;div&gt;, &lt;span&gt;, &lt;p&gt;</code>等。<a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Inline_elemente">内联元素</a>只占据标签包含内容的空间，不会另起新行显示。内联元素通常只包含内联元素。HTML定义的内联元素包括<code>&lt;span&gt;, &lt;code&gt;, &lt;textarea&gt;</code>等。   </p>

  <p><img src="/assets/images/2014-10-25-css-essential_css_box-intro.png" alt="d" /> </p>

  <p>上例中，<code>&lt;p&gt;</code>是块元素，新起一行显示，并且撑满了行，<code>&lt;em&gt;</code>是内联元素，紧接上一元素显示。</p>
</blockquote>

<p>CSS的<code>display</code>属性可设定HTML元素生成盒子的类型是块还是内联，常用的属性值有<code>block, inline, inline-block, none</code>。将<code>display</code>属性设为<code>block</code>，可将内联元素转为块元素。盒子模型有四类边：margin edge，border edge，padding edge，content edge。    </p>

<div class="image_line">
<div class="image_card">
<img src="/assets/images/2014-10-25-css-essential_box_model.gif" alt="CSS语法" />
<div class="caption">盒子模型的四类边</div>
</div>
</div>

<p><code>.box {with: ...; height: ...}</code>设置的是只是<code>Content</code>尺寸。相关边的的设置方法是：</p>

<div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nc">.box</span> <span class="p">{</span>
    <span class="k">width</span><span class="o">:</span> <span class="o">...</span><span class="p">;</span>
    <span class="k">height</span><span class="o">:</span> <span class="o">...</span><span class="p">;</span>
    <span class="k">padding</span><span class="o">:</span> <span class="o">...</span><span class="p">;</span>
    <span class="k">border</span><span class="o">:</span> <span class="o">...</span><span class="p">;</span>
    <span class="k">margin</span><span class="o">:</span> <span class="o">...</span><span class="p">;</span>
<span class="p">}</span></code></pre></div>

<p>内联元素设置属性<code>height</code>和<code>width</code>是没有用的，致使它变宽变大的原因是内部元素的宽高<code>+padding</code>。</p>

<p>满足一定条件俩个盒子的外边届（margin）会叠加，使得估计这俩个盒子的位置关系变得复杂。</p>

<blockquote>
  <h4 id="margin-collapsing">外边叠加（margin collapsing）</h4>
  <hr />
  <p>上边距（top margin）和下边距（bottom margin）有时会叠加，叠加后两者最大值作为两者间的边距。发生外边叠加的3种情况（<a href="http://www.cnblogs.com/cuishengli/archive/2012/06/22/2558859.html#CSS 外边距合并">图解</a>）<sup id="fnref:ccs_master"><a href="#fn:ccs_master" class="footnote">1</a></sup>：</p>

  <ol>
    <li>兄弟（Adjacent siblings）块：相邻块中，兄的下边界和弟的上边界会叠加；</li>
    <li>父与首尾孩子（Parent and first/last child）块：父块和首个孩子的margin-top之间不被任何东西分隔，则它们会叠加；父块和最后孩子的margin-bottom之间不被任何东西分隔，则它们会叠加；</li>
    <li>空块（Empty blocks）：不存在border、padding、inline content、height或min-height分隔块的margin-top和margin-bottom，上下边界会叠加。</li>
  </ol>

  <p>注意事项：</p>

  <ul>
    <li>当与负边界叠加时，叠加后的边界是最大正边界和最小边界之和；</li>
    <li>浮动的（floating）和绝对定位的（absolutely positioned）元素不参与外边叠加（这是因为创建了新的<a href="#block-formatting-context">块格式化环境</a>，而边界叠加只发生在同一块格式化环境）。</li>
  </ul>

</blockquote>

<h2 id="section-10">可视化格式模型</h2>

<blockquote>
  <h4 id="section-11">可视化格式模型解决的主要问题</h4>
  <hr />
  <ol>
    <li>如何生成盒子？</li>
    <li>盒子如何在页面布局？</li>
  </ol>
</blockquote>

<p><a href="https://developer.mozilla.org/en-US/docs/Web/Guide/CSS/Visual_formatting_model">可视化格式模型（Visual formatting model）</a>是用于处理网页文档并显示到虚拟设备上的算法，该模型将文档中的每个元素生成符合盒子模型的盒子，然后对这些模型进行布局。可视化格式模型包括盒子的生成和定位两部分。</p>

<h3 id="box-generation">生成机制（Box Generation）</h3>

<p>生成盒子的类型通过CSS属性<code>display</code>设定，当设定属性为<code>block</code>、<code>list-item</code>和<code>table</code>时，生成块盒子，当设定属性为<code>inline</code>、<code>inline-block</code>和<code>inline-table</code>时，生成内联盒子。<a href="#htmlblock-levelinline">HTML元素的属性</a>决定了该元素生成盒子类型的默认值，块元素默认生成块盒子，内联元素默认生成内联盒子。不同类型盒子的定位机制不同。</p>

<h3 id="positioning-scheme">定位机制（Positioning Scheme）</h3>

<p>CSS的盒子有3种基本定位机制：普通流（normal flow）、浮动（floats）和绝对定位（absolute position），默认定位机制是普通流。在普通流中，盒子一个接一个排列，floats算法可以将盒子从普通流中抽出来，绝对定位通过包含它盒子的坐标系统来定位。</p>

<h4 id="normal-flow">Normal Flow</h4>

<blockquote>
  <h4 id="normal-flow-1">如何进入普通流（normal flow）</h4>
  <hr />
  <p>CCS将盒子的<code>position</code>属性设置为<code>static</code>或<code>relative</code>，并且将<code>float</code>属性设置为<code>none</code>。默认值<code>position: static</code>，<code>float: none</code>。</p>

  <p>普通流可以理解为盒子按照读入的先后次序依次处理，就像水流一样连续有序，每个页面对应一个流。浮动和绝对定位，可将读入序列中的某些盒子从这个流中抽取出来，单独处理。</p>
</blockquote>

<p>在普通流中，块盒子（block-level boxes ）和内联盒子（inline boxes）分别从纵向和横向对元素进行布局。块盒子通过垂直的方式一个接一个的排列，盒子之间的距离通过垂直方向的边界（margin-top和margin-bottom）控制（计算距离时要注意盒子<a href="#margin-collapsing">外边叠加问题</a>）。内联盒子通过水平方式排列，设置垂自方向的padding、borders和margins无效。水平排列的内联盒子通过行盒子（line box）组织在一起，行盒子的高度总是足够容纳包含在其中的所有盒子，可以通过设置行高（line height）控制行盒子的高度。因此，改变内联盒子尺寸的参数只有水平方向上的borders、padding、margins以及line height。    </p>

<div class="image_line">
<div class="image_card">
<img src="/assets/images/2014-10-25-css-essential_css-syntax-ruleset_line_box.jpeg" alt="CSS语法" />
<div class="caption">包含在行盒子中的内联元素</div>
</div>
</div>

<p>CSS2.1可以设置<code>display</code>属性为<code>inline-block</code>，融合inline和block的属性。在水平方向按内联盒子的方式布局，同时可以像块盒子一样设置widths、heights和垂自方向的 margins、padding。 </p>

<div class="image_line">
<div class="image_card">
<img src="/assets/images/2014-10-25-css-essential_relative.png" alt="设置relative的效果" />
<div class="caption">设置relative的效果"position: relative; left: 20px; top: 20px;"</div>
</div>
</div>

<h4 id="floats">Floats</h4>

<blockquote>
  <h4 id="floats-1">如何使用浮动模式（floats）</h4>
  <hr />
  <p>CCS将盒子的<code>float</code>属性设置为<code>left</code>或<code>right</code>，并且将<code>position</code>属性设置为<code>static</code>或<code>relative</code>。</p>

  <p><code>float</code>的非<code>none</code>属性暗示了盒子是<code>block</code>类型，因此，非块类型盒子的<code>display</code>属性会因为设置了<code>float</code>属性而改变为块类型。</p>
</blockquote>

<p>当盒子采用floats算法定位时，盒子会从普通流中被抽取出来，向左或向右移动，直到遇到父盒子或设置了float属性盒子的边界。</p>

<div class="image_line">
<div class="image_card">
<img src="/assets/images/2014-10-25-css-essential_floats.png" alt="设置floats的效果" />
<div class="caption">设置<code>float</code>的效果</div>
</div>
</div>

<p>上图3个红色的方块，两个左浮（<code>float: left</code>），一个右浮。第二个左浮窗口位于第一个左浮动窗口右边，如果再增加左浮窗口，会不断的照此叠加，充满父盒子后会换行继续显示。</p>

<p>一个盒子设置了浮动后，会影响它的兄弟元素，具体的影响方式较为复杂，这要视乎这些兄弟元素是块级元素还是内联元素，若是块级元素会无视这个浮动的块框，使自身尽可能与这个浮动元素处于同一行，导致被浮动元素覆盖，除非这些<code>div</code>设置了宽度，并且父元素的宽度不足以包含它们，这样兄弟元素才会被强制换行；若是内联元素，则会尽可能围绕浮动元素。</p>

<p>浮动元素脱离了普通流，因此包含它的父元素不会因为这个浮动元素的存在而自动撑高，这就造成了高度塌陷。下图所示，由于左边盒子浮动而脱离了普通流，父元素的高度只是由<code>span</code>盒子决定，看起来就像父元素高度塌陷了。</p>

<div class="image_line">
<div class="image_card">
<img src="/assets/images/2014-10-25-css-essential_clearfloat1.png" alt="浮动的影响" />
<div class="caption">浮动的影响</div>
</div>
</div>

<p>当浮动影响到盒子的布局时，需要<a href="#clearing-floats">清除浮动</a>。</p>

<h4 id="absolute-positioning">Absolute Positioning</h4>

<blockquote>
  <h4 id="absolute-positioning-1">如何采用绝对定位（absolute positioning）</h4>
  <hr />
  <p>CCS将盒子的<code>position</code>属性设置为<code>absolute</code>或<code>fixed</code>。</p>

  <p>当设置为<code>fixed</code>的时候，盒子的位置相对于浏览器可见的视窗固定，即使拖动浏览器的滚动条，盒子的位置也固定不变。</p>

</blockquote>

<div class="image_line">
<div class="image_card">
<img src="/assets/images/2014-10-25-css-essential_absolute.png" alt="设置absolute的效果" />
<div class="caption">设置absolute的效果"position: absolute; left: 20px; top: 20px;"</div>
</div>
</div>

<h4 id="clearing-floats">清除浮动（clearing floats）</h4>

<p>由于浮动元素会影响它的兄弟元素的位置和父元素产生高度塌陷，需要清除浮动。</p>

<p>常用的清除浮动方法是<code>clear: both</code>，<code>clear</code>的属性值<code>both</code>、<code>left</code>、<code>right</code>、<code>none</code>、<code>inherit</code> 分别代表在元素左右两侧不允许出现浮动元素、左侧不允许出现浮动元素、右侧不允许出现浮动元素、不清除浮动、继承父元素的值。</p>

<p>但是，<code>clear</code>只是清除了浮动对兄弟元素的影响，而高度塌陷问题还没有解决，需要更高级的清除浮动——闭合浮动。为什么叫闭合浮动？因为浮动的元素脱离了普通流，对于它的父元素，它并没有闭合，这时候就需要闭合浮动了。</p>

<blockquote>
  <h4 id="section-12">闭合浮动的3种方法</h4>
  <hr />
  <p>（1）空<code>div</code>方法</p>

  <div class="highlight"><pre><code class="language-html" data-lang="html"><span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;box&quot;</span><span class="nt">&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;main left&quot;</span><span class="nt">&gt;</span>我设置了左浮动 float: left<span class="nt">&lt;/div&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">style=</span><span class="s">&quot;clear: both;&quot;</span><span class="nt">&gt;&lt;/div&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;aside&quot;</span><span class="nt">&gt;</span>我是页脚，我的上面添加了一个设置了 clear: both 的空 div<span class="nt">&lt;/div&gt;</span>
<span class="nt">&lt;/div&gt;</span></code></pre></div>
  <p>空<code>div</code>方法很方便，但是加入了没有涵义的<code>div</code>，这违背了结构与表现分离的原则，并且后期维护也不方便。</p>

  <p>（2）<code>overflow</code>方法</p>

  <div class="highlight"><pre><code class="language-html" data-lang="html"><span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;box&quot;</span> <span class="na">style=</span><span class="s">&quot;overflow: hidden; *zoom: 1;&quot;</span><span class="nt">&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;main left&quot;</span><span class="nt">&gt;</span>我设置了左浮动 float: left<span class="nt">&lt;/div&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;aside left&quot;</span><span class="nt">&gt;</span>我是页脚，但是我也设置了左浮动。<span class="nt">&lt;/div&gt;</span>
<span class="nt">&lt;/div&gt;</span></code></pre></div>
  <p>当元素内包含会超出父元素边界的子元素时，<code>overflow</code>方法可能会覆盖掉有用的子元素，或是产生了多余的滚动条。</p>

  <p>（3）<code>:after</code>伪元素的方法</p>

  <div class="highlight"><pre><code class="language-html" data-lang="html"><span class="nt">&lt;style&gt;</span>
    <span class="nc">.clearfix</span> <span class="p">{</span><span class="c">/* 触发 hasLayout */</span> <span class="n">zoom</span><span class="o">:</span> <span class="m">1</span><span class="p">;</span> <span class="p">}</span>
    <span class="nc">.clearfix</span><span class="nd">:after</span> <span class="p">{</span><span class="k">content</span><span class="o">:</span> <span class="s1">&#39;.&#39;</span><span class="p">;</span> <span class="k">display</span><span class="o">:</span> <span class="k">block</span><span class="p">;</span> <span class="k">height</span><span class="o">:</span> <span class="m">0</span><span class="p">;</span> <span class="k">clear</span><span class="o">:</span> <span class="k">both</span><span class="p">;</span> <span class="k">visibility</span><span class="o">:</span> <span class="k">hidden</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">&lt;/style&gt;</span>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;box clearfix&quot;</span><span class="nt">&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;main left&quot;</span><span class="nt">&gt;</span>我设置了左浮动 float: left<span class="nt">&lt;/div&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;aside left&quot;</span><span class="nt">&gt;</span>我是页脚，但是我也设置了左浮动。<span class="nt">&lt;/div&gt;</span>
<span class="nt">&lt;/div&gt;</span></code></pre></div>
  <p>这个办法不但完美兼容主流浏览器，并且也很方便，使用重用的类，可以减轻代码编写，另外网页的结构也会更加清晰。</p>
</blockquote>

<p>清除浮动的详细介绍可参考<a href="http://kayosite.com/remove-floating-style-in-detail.html">《详说清除浮动》</a>（<a href="/assets/images/../html/clearfloat.html">示例</a>）。</p>

<h2 id="section-13">块格式化环境</h2>

<blockquote>
  <h4 id="block-formatting-context">块格式化环境（block formatting context）</h4>
  <hr />
  <p>块格式化环境是CCS渲染Web页面的一块区域，块盒子在该区域内布局。</p>

  <p>块格式化环境对（<code>float</code>）定位和清除（<code>clear</code>）浮动至关重要，定位和清除浮动只对同一块格式化环境中的对象有效。<code>float</code>不会影响到其它块格式化环境中的盒子，<code>clear</code>只清除同一块格式化环境中之前的float效果。</p>

  <p>简单来说，块格式化环境是一种属性，这种属性会影响着元素的定位以及与其兄弟元素之间的相互作用。</p>
</blockquote>

<p>块格式化环境就是一个作用范围，可理解为一个独立的容器，这个容器的里盒子的布局与这个容器外的不相干。</p>

<p>块格式化环境不存在嵌套包含关系，块格式化环境只包含该环境内的对象，不会再包含该环境中对象再创建的块格式化环境。（A block formatting context contains everything inside of the element creating it that is not also inside a descendant element that creates a new block formatting context.）</p>

<blockquote>
  <h4 id="section-14">创建块格式化环境的条件（满足任意一条即可）</h4>
  <hr />
  <ul>
    <li>the root element or something that contains it；</li>
    <li><code>float</code>设置为<strong>非</strong><code>none</code>；</li>
    <li><code>position</code>设置为<code>absolute</code>或<code>fixed</code>；</li>
    <li><code>display</code>设置为<code>inline-block</code>、<code>table-cell</code>、<code>table-caption</code>、<code>flex</code>或<code>inline-flex</code>；</li>
    <li><code>overflow</code>设置为<strong>非</strong><code>visible</code>。</li>
  </ul>
</blockquote>

<p>块格式化环境主要有三个特性（详见<a href="http://www.cnblogs.com/leejersey/p/3991400.html">《详说 Block Formatting Contexts (块级格式化上下文)》</a>，<a href="/assets/images/../html/bfc.html">示例</a>）：</p>

<ol>
  <li>阻止<a href="#margin-collapsing">外边距折叠</a>；    </li>
  <li>包含浮动的元素（<a href="#clearing-floats">清除浮动</a>之<code>overflow</code>方法）；   </li>
  <li>阻止元素被浮动元素覆盖。 </li>
</ol>

<h2 id="section-15">高级选择器</h2>

<h4 id="css-21-">CSS 2.1 属性选择器</h4>

<table>
  <tbody>
    <tr>
      <td>E[att]</td>
      <td>匹配所有具有att属性的E元素，不考虑它的值。（注意：E在此处可以省略，比如”[cheacked]”。以下同。）</td>
    </tr>
    <tr>
      <td>E[att=val]</td>
      <td>匹配所有att属性等于”val”的E元素。</td>
    </tr>
    <tr>
      <td>E[att~=val]</td>
      <td>匹配所有att属性具有多个空格分隔的值、其中一个值等于”val”的E元素。</td>
    </tr>
    <tr>
      <td>E[att|=val]</td>
      <td>匹配所有att属性具有多个连字号分隔（hyphen-separated）的值、其中一个值以”val”开头的E元素，主要用于lang属性，比如”en”、”en-us”、”en-gb”等等。</td>
    </tr>
  </tbody>
</table>

<p>示例：</p>

<div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">p</span><span class="o">[</span><span class="nt">title</span><span class="o">]</span> <span class="p">{</span> <span class="k">color</span><span class="o">:</span><span class="m">#f00</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">div</span><span class="o">[</span><span class="nt">class</span><span class="o">=</span><span class="nt">error</span><span class="o">]</span> <span class="p">{</span> <span class="k">color</span><span class="o">:</span><span class="m">#f00</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">td</span><span class="o">[</span><span class="nt">headers</span><span class="o">~=</span><span class="nt">col1</span><span class="o">]</span> <span class="p">{</span> <span class="k">color</span><span class="o">:</span><span class="m">#f00</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">p</span><span class="o">[</span><span class="nt">lang</span><span class="o">|=</span><span class="nt">en</span><span class="o">]</span> <span class="p">{</span> <span class="k">color</span><span class="o">:</span><span class="m">#f00</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">blockquote</span><span class="o">[</span><span class="nt">class</span><span class="o">=</span><span class="nt">quote</span><span class="o">][</span><span class="nt">cite</span><span class="o">]</span> <span class="p">{</span> <span class="k">color</span><span class="o">:</span><span class="m">#f00</span><span class="p">;</span> <span class="p">}</span></code></pre></div>

<h4 id="css-21-pseudo-classes">CSS 2.1 伪类（pseudo-classes）</h4>

<table>
  <tbody>
    <tr>
      <td>E:first-child</td>
      <td>匹配父元素的第一个子元素。</td>
    </tr>
    <tr>
      <td>E:link</td>
      <td>匹配所有未被点击的链接。</td>
    </tr>
    <tr>
      <td>E:visited</td>
      <td>匹配所有已被点击的链接。</td>
    </tr>
    <tr>
      <td>E:active</td>
      <td>匹配鼠标已经其上按下、还没有释放的E元素。</td>
    </tr>
    <tr>
      <td>E:hover</td>
      <td>匹配鼠标悬停其上的E元素。</td>
    </tr>
    <tr>
      <td>E:focus</td>
      <td>匹配获得当前焦点的E元素。</td>
    </tr>
    <tr>
      <td>E:lang(c)</td>
      <td>匹配lang属性等于c的E元素。</td>
    </tr>
  </tbody>
</table>

<p>示例：</p>

<div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">p</span><span class="nd">:first-child</span> <span class="p">{</span> <span class="k">font-style</span><span class="o">:</span><span class="k">italic</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">input</span><span class="o">[</span><span class="nt">type</span><span class="o">=</span><span class="nt">text</span><span class="o">]</span><span class="nd">:focus</span> <span class="p">{</span> <span class="k">color</span><span class="o">:</span><span class="m">#000</span><span class="p">;</span> <span class="k">background</span><span class="o">:</span><span class="m">#ffe</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">input</span><span class="o">[</span><span class="nt">type</span><span class="o">=</span><span class="nt">text</span><span class="o">]</span><span class="nd">:focus:hover</span> <span class="p">{</span> <span class="k">background</span><span class="o">:</span><span class="m">#fff</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">q</span><span class="nd">:lang</span><span class="o">(</span><span class="nt">sv</span><span class="o">)</span> <span class="p">{</span> <span class="k">quotes</span><span class="o">:</span> <span class="s2">&quot;\201D&quot;</span> <span class="s2">&quot;\201D&quot;</span> <span class="s2">&quot;\2019&quot;</span> <span class="s2">&quot;\2019&quot;</span><span class="p">;</span> <span class="p">}</span></code></pre></div>

<h4 id="css-21-pseudo-elements">CSS 2.1 伪元素（pseudo-elements）</h4>

<table>
  <tbody>
    <tr>
      <td>E:first-line</td>
      <td>匹配E元素的第一行。</td>
    </tr>
    <tr>
      <td>E:first-letter</td>
      <td>匹配E元素的第一个字母。</td>
    </tr>
    <tr>
      <td>E:before</td>
      <td>在E元素之前插入生成的内容。</td>
    </tr>
    <tr>
      <td>E:after</td>
      <td>在E元素之后插入生成的内容。</td>
    </tr>
  </tbody>
</table>

<p>示例：</p>

<div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">p</span><span class="nd">:first-line</span> <span class="p">{</span> <span class="k">font-weight</span><span class="o">:</span><span class="k">bold</span><span class="p">;</span> <span class="k">color</span><span class="p">;</span><span class="m">#600</span><span class="p">;</span> <span class="p">}</span>
<span class="nc">.preamble</span><span class="nd">:first-letter</span> <span class="p">{</span> <span class="k">font-size</span><span class="o">:</span><span class="m">1.5em</span><span class="p">;</span> <span class="k">font-weight</span><span class="o">:</span><span class="k">bold</span><span class="p">;</span> <span class="p">}</span>
<span class="nc">.cbb</span><span class="nd">:before</span> <span class="p">{</span> <span class="k">content</span><span class="o">:</span><span class="s2">&quot;&quot;</span><span class="p">;</span> <span class="k">display</span><span class="o">:</span><span class="k">block</span><span class="p">;</span> <span class="k">height</span><span class="o">:</span><span class="m">17px</span><span class="p">;</span> <span class="k">width</span><span class="o">:</span><span class="m">18px</span><span class="p">;</span> <span class="k">background</span><span class="o">:</span><span class="sx">url(top.png)</span> <span class="k">no-repeat</span> <span class="m">0</span> <span class="m">0</span><span class="p">;</span> <span class="k">margin</span><span class="o">:</span><span class="m">0</span> <span class="m">0</span> <span class="m">0</span> <span class="m">-18px</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">a</span><span class="nd">:link:after</span> <span class="p">{</span> <span class="k">content</span><span class="o">:</span> <span class="s2">&quot; (&quot;</span> <span class="n">attr</span><span class="p">(</span><span class="n">href</span><span class="p">)</span> <span class="s2">&quot;) &quot;</span><span class="p">;</span> <span class="p">}</span></code></pre></div>

<p>更多关于CSS选选择器类容可参考<a href="https://developer.mozilla.org/en-US/docs/Web/CSS/Reference">CSS reference</a>。</p>

<h2 id="section-16">使用技巧</h2>

<blockquote>
  <h4 id="important">!important规则</h4>
  <hr />
  <p>多条CSS语句互相冲突时，具有!important的语句将覆盖其他语句。由于IE不支持!important，所以也可以利用它区分不同的浏览器。</p>

  <div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">h1</span> <span class="p">{</span><span class="k">color</span><span class="o">:</span> <span class="nb">red</span> <span class="cp">!important</span><span class="p">;</span> <span class="k">color</span><span class="o">:</span> <span class="nb">blue</span><span class="p">;}</span></code></pre></div>
</blockquote>

<blockquote>
  <h4 id="section-17">容器水平居中</h4>
  <hr />
  <p>先为该容器设置一个明确宽度，然后将margin的水平值设为auto即可。</p>

  <div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">div</span><span class="nf">#container</span> <span class="p">{</span><span class="k">width</span><span class="o">:</span> <span class="m">760px</span><span class="p">;</span> <span class="k">margin</span><span class="o">:</span> <span class="m">0</span> <span class="k">auto</span><span class="p">;}</span></code></pre></div>
</blockquote>

<blockquote>
  <h4 id="section-18">禁止自动换行</h4>
  <hr />
  <p>文字在一行中显示完成，不要自动换行。</p>

  <div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">h1</span> <span class="p">{</span> <span class="k">white-space</span><span class="o">:</span> <span class="k">nowrap</span><span class="p">;</span> <span class="p">}</span></code></pre></div>
</blockquote>

<blockquote>
  <h4 id="section-19">图片宽度自适应</h4>
  <hr />
  <p>如何使得较大的图片，能够自动适应小容器的宽度？</p>

  <div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">img</span> <span class="p">{</span><span class="k">max-width</span><span class="o">:</span> <span class="m">100%</span><span class="p">}</span></code></pre></div>
</blockquote>

<blockquote>
  <h4 id="link">设置link状态的顺序</h4>
  <hr />
  <p>link的四种状态，需要按照下面的前后顺序进行设置。</p>

  <div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">a</span><span class="nd">:link</span> 
<span class="nt">a</span><span class="nd">:visited</span> 
<span class="nt">a</span><span class="nd">:hover</span> 
<span class="nt">a</span><span class="nd">:active</span></code></pre></div>
</blockquote>

<blockquote>
  <h4 id="text-transformfont-variant">Text-transform和Font Variant</h4>
  <hr />
  <p>Text-transform用于将所有字母变成小写字母、大写字母或首字母大写。</p>

  <div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">p</span> <span class="p">{</span><span class="k">text-transform</span><span class="o">:</span> <span class="k">uppercase</span><span class="p">}</span> 
<span class="nt">p</span> <span class="p">{</span><span class="k">text-transform</span><span class="o">:</span> <span class="k">lowercase</span><span class="p">}</span> 
<span class="nt">p</span> <span class="p">{</span><span class="k">text-transform</span><span class="o">:</span> <span class="k">capitalize</span><span class="p">}</span></code></pre></div>
  <p>Font Variant用于将字体变成小型的大写字母（即与小写字母等高的大写字母）。</p>

  <div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">p</span> <span class="p">{</span><span class="k">font-variant</span><span class="o">:</span> <span class="k">small-caps</span><span class="p">}</span></code></pre></div>
</blockquote>

<blockquote>
  <h4 id="section-20">用图片充当列表标志</h4>
  <hr />
  <p>默认情况下，浏览器使用一个黑圆圈作为列表标志，可以用图片取代它。</p>

  <div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">ul</span> <span class="p">{</span><span class="k">list-style</span><span class="o">:</span> <span class="k">none</span><span class="p">}</span>
<span class="nt">ul</span> <span class="nt">li</span> <span class="p">{</span>
    <span class="k">background-image</span><span class="o">:</span> <span class="sx">url(&quot;path-to-your-image&quot;)</span><span class="p">;</span>
    <span class="k">background-repeat</span><span class="o">:</span> <span class="k">none</span><span class="p">;</span>
    <span class="k">background-position</span><span class="o">:</span> <span class="m">0</span> <span class="m">0.5em</span><span class="p">;</span> 
<span class="p">}</span></code></pre></div>
</blockquote>

<blockquote>
  <h4 id="section-21">用图片替换文字</h4>
  <hr />
  <p>在标题栏中使用图片，但是又必须保证搜索引擎能够读到标题。</p>

  <div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">h1</span> <span class="p">{</span> 
    <span class="k">text-indent</span><span class="o">:</span> <span class="m">-9999px</span><span class="p">;</span> 
    <span class="k">background</span><span class="o">:</span> <span class="sx">url(&quot;h1-image.jpg&quot;)</span> <span class="k">no-repeat</span><span class="p">;</span> 
    <span class="k">width</span><span class="o">:</span> <span class="m">200px</span><span class="p">;</span>
    <span class="k">height</span><span class="o">:</span> <span class="m">50px</span><span class="p">;</span>
<span class="p">}</span></code></pre></div>
</blockquote>

<h2 id="section-22">预处理器</h2>

<p>CSS预处理器（css preprocessor）的基本思想是，用一种专门的编程语言，进行网页样式设计，然后再编译成CSS文件。常用的CSS预处理器有<a href="http://lesscss.org">Less</a>、<a href="http://sass-lang.com">Sass</a>等。</p>

<h3 id="less">Less</h3>

<p><a href="http://lesscss.org">Less</a>使用变量（variables）、混合（mixins）、函数（functions）和许多其他的技术，让你的CSS更具维护性、主题性、扩展性。Less可运行在Node环境，浏览器环境和Rhino环境，同时也有3种可选工具供你编译文件和监视任何改变。</p>

<p>具体用法可参考Bootstrap中文网的<a href="http://www.bootcss.com/p/lesscss/" title="LESS « 一种动态样式语言">Less教程</a>。</p>

<h3 id="sass--compass">Sass &amp; Compass</h3>

<p><a href="http://sass-lang.com">Sass</a>使用变量（variables）、混合（mixins）、嵌套规则（nested rules）、内联导入（inline imports）等，让CSS更加优雅和强大。Sass让CSS更好的组织、体积更小、速度更快。</p>

<p><a href="http://compass-style.org">Compass</a>是Sass的工具库（toolkit）。Sass本身只是一个编译器，Compass在它的基础上，封装了一系列有用的模块和模板，补充Sass的功能。它们之间的关系，有点像Javascript和jQuery、Ruby和Rails、python和Django的关系。</p>

<p>具体使用可参考阮一峰的<a href="http://www.ruanyifeng.com/blog/2012/06/sass.html" title="SASS用法指南">Sass</a>和<a href="http://www.ruanyifeng.com/blog/2012/11/compass.html" title="Compass用法指南">Compass</a>用法指南。</p>

<h2 id="section-23">参考资料</h2>

<ol>
  <li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS" title="Cascading Style Sheets">MDN: CSS</a></li>
  <li><a href="http://www.w3.org/Style/CSS/" title="Cascading Style Sheets home page">W3: Cascading Style Sheets home page</a></li>
  <li><a href="http://www.w3school.com.cn/css/index.asp" title="CSS 教程">w3school: CSS 教程</a></li>
  <li><a href="http://www.w3.org/TR/2001/WD-css3-values-20010713/#specified" title="Specified, computed, and actual values">W3: Specified, computed, and actual values</a></li>
  <li><a href="http://www.w3.org/TR/css3-values/" title="CSS Values and Units Module Level 3">W3: CSS Values and Units Module Level 3</a></li>
  <li><a href="http://www.w3.org/TR/2011/REC-css3-color-20110607/">W3: CSS Color Module Level 3</a></li>
  <li><a href="http://dev.w3.org/csswg/css-box/">W3: CSS basic box model</a></li>
  <li><a href="http://dev.w3.org/csswg/css-grid/">W3: CSS Grid Layout Module Level 1</a></li>
  <li><a href="http://dev.w3.org/csswg/css-flexbox/">W3: CSS Flexible Box Layout Module Level 1</a></li>
  <li><a href="http://www.ruanyifeng.com/blog/2010/03/css_cookbook.html">阮一峰：CSS使用技巧</a>    </li>
  <li><a href="http://www.ruanyifeng.com/blog/2009/03/css_selectors.html">阮一峰：CSS选择器笔记</a></li>
  <li><a href="http://www.cnblogs.com/cuishengli/archive/2012/06/22/2558859.html">cuishengli：CSS 框模型概述</a></li>
  <li><a href="http://www.cnblogs.com/leejersey/p/3991400.html">leejersey：详说 Block Formatting Contexts (块级格式化上下文)</a></li>
  <li><a href="http://kayosite.com/remove-floating-style-in-detail.html">Kayo：详说清除浮动</a></li>
  <li><a href="http://www.catswhocode.com/blog/8-css-preprocessors-to-speed-up-development-time" title="8 CSS preprocessors to speed up development time">Jean: 8 CSS preprocessors to speed up development time</a></li>
  <li><a href="http://www.bootcss.com/p/lesscss/" title="LESS « 一种动态样式语言">LESS « 一种动态样式语言</a></li>
  <li><a href="http://www.ruanyifeng.com/blog/2012/06/sass.html" title="SASS用法指南">阮一峰：SASS用法指南</a></li>
  <li><a href="http://www.ruanyifeng.com/blog/2012/11/compass.html" title="Compass用法指南">阮一峰：Compass用法指南</a></li>
</ol>

<h3 id="section-24">脚注</h3>
<div class="footnotes">
  <ol>
    <li id="fn:ccs_master">
      <p>也可参考”CSS Mastery: Advanced Web Standards Solutions, Second Edition”一书的”Chapter 3: Visual Formatting Model Overview”。 <a href="#fnref:ccs_master" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习: 感知器算法</title>
      <link href="http://www.geogebra.com.cn/2014/10/machine-learning-perceptron-learning-algorithm" />
      <pubdate>2014-10-21T18:09:00+08:00</pubdate>
      <author>Jide Qian</author>
      <guid>http://www.geogebra.com.cn/2014/10/machine-learning-perceptron-learning-algorithm</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">问题描述</h2>

<p>对于线性二分类问题<br />
\begin{equation}
y = \left\{
\begin{aligned}
&amp; +1 &amp; \sum\nolimits_{i=1}^dw_ix_i &gt; \mbox{threshold} \\
&amp; -1 &amp; \sum\nolimits_{i=1}^dw_ix_i &lt; \mbox{threshold}
\end{aligned}
\right. 
\end{equation}</p>

<p>也可以记为
\begin{equation}
h(x) = \mbox{sign}\left(\sum_{i=1}^dw_ix_i - \mbox{threshold}\right) = \mbox{sign}\left(\sum_{i=0}^dw_ix_i\right) = \mbox{sign}\left(\mathbf{w^Tx}\right)
\end{equation} <br />
其中，$w_0 = -\mbox{threshold}, x_0 = +1$。</p>

<p>问：如何估计模型模型$h(x)$的参数$\mathbf{w}$？  <br />
答：感知器算法（PLA，Perceptron Learning Algorithm）。</p>

<h2 id="section-1">感知器算法</h2>

<p>感知器算法通过迭代更新模型参数，直到没有错分的样本点。  </p>

<blockquote>
  <h4 id="pla">PLA</h4>
  <hr />
  <p>repeat until a full cycle of not encountering mistakes { <br />
1. 找到参数$\mathbf w_t$时对应错分的样本点$\left(\mathbf x_{n(t)}, y_{n(t)}\right)$，$\mbox{sign}\left(\mathbf w^T \mathbf x_{n(t)}\right) \neq y_{n(t)}$；     <br />
2. 修正参数，$\mathbf w_{t+1}\leftarrow \mathbf w_t + y_{n(t)}\mathbf x_{n(t)}$。  <br />
}</p>
</blockquote>

<div class="image_line">
<div class="image_card">
<img src="/assets/images/2014-10-21-pla_1.png" alt="PLA更新模型参数示意图" />
<div class="caption">PLA更新模型参数示意图</div>
</div>
</div>

<p>PLA更新模型参数解释： <br />
1. 当$y=+1$时，若分错，$\mathbf w^T \mathbf x &lt; 0$，表示$\mathbf{w}$和$\mathbf x$的夹角太大（大于90度），需要调整$\mathbf w$，使其与$\mathbf x$的夹角变小；<br />
2. 当$y=-1$时，若分错，$\mathbf w^T \mathbf x &gt; 0$，表示$\mathbf{w}$和$\mathbf x$的夹角太小（小于90度），需要调整$\mathbf w$，使其与$\mathbf x$的夹角变大。   </p>

<p>通过参数更新规则可知  <br />
\[
\mathbf w_{t+1} ＝ \mathbf w_t + y_{n}\mathbf x_{n}
\]<br />
两边转置后同时乘上$y_n\mathbf x_n$，可得
\[
y_n\mathbf w^T_{t+1} \mathbf x_n \geq y_n\mathbf w^T_t\mathbf x_n
\]
这表明，参数更新始终在试图纠正模型参数。  </p>

<p><strong>注意事项</strong>：$\mathbf w$是决策界（判别面）的法向量。</p>

<h2 id="section-2">理论分析</h2>

<blockquote>
  <ol>
    <li>PLA算法会终止么？</li>
    <li>能否从候选模式$h$中学习到目标模式$f$？</li>
  </ol>
</blockquote>

<p>如果数据集$\mathcal D$线性可分，存在一个完美$\mathbf w_f$，使得对数据集中所有样本点$y_n = \mbox{sign}\left(\mathbf w_f^T\mathbf x_n\right)$，对于任意一个样本点总有</p>

<p>\begin{equation}
y_{n(t)}\mathbf w_f^T\mathbf x_{n(t)} \geq \min_n y_n\mathbf x_f^T\mathbf x_n &gt; 0
\end{equation}</p>

<p>根据感知器算法的更新规则可知
\begin{equation}
\begin{aligned}
\mathbf w_f^T\mathbf w_{t+1} 
&amp; =  \mathbf w_f^T\left(\mathbf w_t + y_{n(t)}\mathbf x_{n(t)}\right) \\
&amp; \geq \mathbf w_f^T\mathbf w_t + \min_n y_n\mathbf x_f^T\mathbf x_n \\
&amp; &gt; \mathbf w_f^T\mathbf w_t 
\end{aligned}
\end{equation}
由此可见，参数更新后，$\mathbf w_{t+1}$<em>可能会</em>更加接近$\mathbf w_f$。但是，还不能确定是由于向量间夹角变小还是$\mathbf w_{t+1}$长度变大导致的内积增加。
\begin{equation}
\begin{aligned}
\|\mathbf w_{t+1}\|^2
&amp; = \|\mathbf w_t + y_{n(t)}\mathbf x_{n(t)}\|^2 \\
&amp; = \|\mathbf w_t\|^2 + \|y_{n(t)}\mathbf x_{n(t)}\|^2 + 2y_{n(t)}\mathbf w_t^T\mathbf x_{n(t)} \\
&amp; \leq \|\mathbf w_t\|^2 + \|y_{n(t)}\mathbf x_{n(t)}\|^2 \\
&amp; \leq \|\mathbf w_t\|^2 + \max_n\|\mathbf x_{n}\|^2
\end{aligned}
\end{equation}
由此可见，每次更新的时候，向量长度的增加是有限的，最多增加样本最长向量的长度。</p>

<p>事实上，从$\mathbf w_0$开始，经过$T$次迭代后有
\begin{equation}
\frac{\mathbf w_f^T}{\|\mathbf w_f\|}\frac{\mathbf w_T}{\|\mathbf w_T\|}\geq
\sqrt T \times \mbox{constant}
\label{eq:need_to_be_done_1}
\end{equation}
PLA算法迭代次数的上界满足$T\leq R^2/\rho^2$，其中
\begin{equation}
R^2 = \max_n \|\mathbf x_n\|^2,~~\rho=\min_n y_n\frac{\mathbf w_f^T}{\|\mathbf w_f\|}\mathbf x_n
\label{eq:need_to_be_done_2}
\end{equation}</p>

<p>相关的证明如下：  <br />
<img src="/assets/images/2014-10-21-pla_2.jpg" alt="相关的证明" /></p>

<p><strong>结论：</strong>     <br />
1. 对于线性可分的样本集合，通过PLA修正模型参数，$\mathbf w_f$和$\mathbf w_t$的内积增长快，$\mathbf w_t$的长度增长慢，$\mathbf w_t$越来越靠近$\mathbf w_f$，最终算最终收敛； <br />
2. 对于未知的样本集，作用在其上的PLA算法可能长时间不收敛，导致这样的情况可能是迭代次数不够（理论上的参数$T$由于目标模型$f$未知而难以估计）或者样本集存在噪声（线性不可分）。</p>

<h2 id="pocket-pla">Pocket PLA</h2>

<p>判断样本集是否线性可分的复杂度为NP-hard。实际上，通常样本集存在一些噪声，不可能严格的线性可分，PLA算法无法满足收敛条件。Pocket PLA通过改变PLA迭代结束的条件和参数更新规则仍然可以估计模型参数。</p>

<blockquote>
  <h4 id="pocket-pla-1">Pocket PLA</h4>
  <hr />
  <p>repeat until enough iterations {<br />
1. 随机抽取参数$\mathbf w_t$时对应错分的样本点$\left(\mathbf x_{n(t)}, y_{n(t)}\right)$，$\mbox{sign}\left(\mathbf w^T \mathbf x_{n(t)}\right) \neq y_{n(t)}$；     <br />
2. 修正参数，$\mathbf w_{t+1}\leftarrow \mathbf w_t + y_{n(t)}\mathbf x_{n(t)}$；<br />
3. 如果$\mathbf w_{t+1}$在样本集上的表现优于$\hat{\mathbf w}$，$\hat{\mathbf w}\leftarrow \mathbf w_{t+1}$。 <br />
}</p>
</blockquote>

<p>从第3步可知，Pocket PLA的算法时间复杂度要高于PLA，如果对于线性可分的样本集，Pocket PLA比PLA慢。</p>

<h2 id="section-3">思考问题</h2>

<ol>
  <li>PLA和梯度下降法有无联系？</li>
  <li>在分类性能上PLA求解的线性模型和Logistic回归有何差别？</li>
  <li>如何推导公式\eqref{eq:need_to_be_done_1}\eqref{eq:need_to_be_done_2}？ <a href="https://class.coursera.org/ntumlone-002/forum/thread?thread_id=28">答案</a></li>
</ol>

<h2 id="section-4">參考資料</h2>

<ol>
  <li><a href="https://class.coursera.org/ntumlone-002">機器學習基石(Machine Learning Foundations)</a>    </li>
  <li><a href="http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf">Convergence Proof for the Perceptron Algorithm</a></li>
</ol>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习: 正则化</title>
      <link href="http://www.geogebra.com.cn/2014/10/machine-learning-regularization" />
      <pubdate>2014-10-20T00:00:00+08:00</pubdate>
      <author>Jide Qian</author>
      <guid>http://www.geogebra.com.cn/2014/10/machine-learning-regularization</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">为什么需要正则化</h2>

<p>过拟合（overfitting）是指模型可以在训练集上表现出色，在新数据上性能却很差。解决过拟合问题的方法：</p>

<ul>
  <li>减少特征数目：手工选择特征、利用模型选择；</li>
  <li>正则化（regularization）：保留所有特征，但是减小$\theta_j$，使特征对预测$y$贡献小。</li>
</ul>

<p>正则化通过在代价函数中加入正则化项限制模型参数，避免过拟合。</p>

<p>线性回归和Logistic回归的代价函数追加的正则化项是$\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2$。</p>

<ul>
  <li>$\lambda$称为正则化参数，增大$\lambda$以减小过拟合；</li>
  <li>$\lambda$过大（$10^{10}$）时，有$\theta_j\approx 0~~(j = 1,2,\ldots,n)$，则$h_\theta(x) = \theta_0$，这会导致欠拟合（underfitting）；</li>
  <li>正则化只作用于$j\ge 1$的非常数项，实际上，正则化常数项对结果影响也不大。</li>
</ul>

<p>资料来源：<a href="#ng_ml_r_2014">[1]</a>。</p>

<p>bias &amp; variance:</p>

<ul>
  <li>high bias: 欠拟合；</li>
  <li>high variance: 过拟合。</li>
</ul>

<h2 id="section-1">正则化线性回归</h2>

<h3 id="section-2">代价函数</h3>

<p>\begin{equation}
J(\theta) = \frac{1}{2m}\left( \sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)^2 + \lambda\sum_{j=1}^n\theta_j^2 \right)
\label{eq:cf-linear-regression-r}
\end{equation}</p>

<h3 id="section-3">梯度下降法估计参数</h3>

<p>repeat until convergence {
\begin{equation*}
\begin{aligned}
\theta_0 &amp; := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_0^{(i)} \\ <br />
\theta_j &amp; := \theta_j - \alpha\frac{1}{m}\left(\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_j^{(i)} + \lambda\theta_j\right)~~(j = 1, 2, \ldots, n)
\end{aligned}
\end{equation*}
}</p>

<p>迭代过程可以化为如下形式：
\begin{equation*}
\theta_j := \theta_j\left(1 - \alpha\frac{\lambda}{m} \right) - \alpha\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_j^{(i)};~~(j = 1, 2, \ldots, n)
\end{equation*}</p>

<p>通常$1 - \alpha\frac{\lambda}{m} &lt; 1$，与非正则化的梯度下降法比较，$\theta_j$减小更快。</p>

<h3 id="section-4">正规方程估计参数</h3>

<p>\begin{equation}
\theta = \left(X^TX + \lambda
\begin{bmatrix}
0  &amp;   &amp;        &amp; \\
   &amp; 1 &amp;        &amp; \\
   &amp;   &amp; \ddots &amp; \\
   &amp;   &amp;        &amp; 1
\end{bmatrix}
\right)^{-1}X^Ty
\end{equation}</p>

<p>可以证明，加入正则化项后矩阵始终可逆。</p>

<h2 id="logistic">正则化Logistic回归</h2>

<h3 id="section-5">代价函数</h3>

<p>\begin{equation}
\begin{aligned}
J(\theta)  = &amp;-\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right)\log \left(1-h_\theta\left(x^{(i)}\right)\right)\right) \\
&amp; + \frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2
\end{aligned}
\label{eq:cf-logistic-regression-r}
\end{equation}</p>

<h3 id="section-6">梯度下降法估计参数</h3>

<p>repeat until convergence {
\begin{equation*}
\begin{aligned}
\theta_0 &amp; := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_0^{(i)} \\ <br />
\theta_j &amp; := \theta_j - \alpha\frac{1}{m}\left(\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_j^{(i)} + \lambda\theta_j\right)~~(j = 1, 2, \ldots, n)
\end{aligned}
\end{equation*}
}</p>

<h3 id="matlab">Matlab实现</h3>

<p>第一步：实现Logistic函数</p>

<div class="highlight"><pre><code class="language-matlab"><span class="k">function</span><span class="w"> </span>g <span class="p">=</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span>z<span class="p">)</span><span class="w"></span>
<span class="n">g</span> <span class="p">=</span> <span class="mf">1.0</span> <span class="o">./</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="nb">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">));</span>
<span class="k">end</span></code></pre></div>

<p>第二步：实现代价函数（包含梯度计算）</p>

<div class="highlight"><pre><code class="language-matlab"><span class="k">function</span><span class="w"> </span>[J, grad] <span class="p">=</span><span class="w"> </span><span class="nf">costFunctionReg</span><span class="p">(</span>theta, X, y, lambda<span class="p">)</span><span class="w"></span>
<span class="n">m</span> <span class="p">=</span> <span class="nb">length</span><span class="p">(</span><span class="n">y</span><span class="p">);</span> <span class="c">% number of training examples</span>

<span class="n">h</span> <span class="p">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="p">);</span>
<span class="n">J</span> <span class="p">=</span> <span class="o">-</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span> <span class="o">.*</span> <span class="nb">log</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">.*</span> <span class="nb">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">))</span> <span class="o">+</span> <span class="c">...</span>
    <span class="n">lambda</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">sum</span><span class="p">(</span><span class="n">theta</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">.^</span> <span class="mi">2</span><span class="p">);</span>
<span class="n">grad</span> <span class="p">=</span> <span class="p">(</span><span class="n">X</span><span class="o">&#39;</span> <span class="o">*</span> <span class="p">(</span><span class="n">h</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">lambda</span> <span class="o">*</span> <span class="p">[</span><span class="mi">0</span><span class="p">;</span> <span class="n">theta</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)])</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>

<span class="k">end</span></code></pre></div>

<p>第三步：估计参数</p>

<div class="highlight"><pre><code class="language-matlab"><span class="n">initial_theta</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">1</span><span class="p">);</span>
<span class="n">lambda</span> <span class="p">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="n">options</span> <span class="p">=</span> <span class="n">optimset</span><span class="p">(</span><span class="s">&#39;GradObj&#39;</span><span class="p">,</span> <span class="s">&#39;on&#39;</span><span class="p">,</span> <span class="s">&#39;MaxIter&#39;</span><span class="p">,</span> <span class="mi">400</span><span class="p">);</span>
<span class="p">[</span><span class="n">theta</span><span class="p">,</span> <span class="n">J</span><span class="p">,</span> <span class="n">exit_flag</span><span class="p">]</span> <span class="p">=</span> <span class="c">...</span>
	<span class="n">fminunc</span><span class="p">(@(</span><span class="n">t</span><span class="p">)(</span><span class="n">costFunctionReg</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lambda</span><span class="p">)),</span> <span class="n">initial_theta</span><span class="p">,</span> <span class="n">options</span><span class="p">);</span></code></pre></div>

<h2 id="section-7">思考问题</h2>

<ol>
  <li>如何推导正则化线性回归的正规方程解？</li>
  <li>求解正则化线性回归的正规方程时，为何矩阵始终可逆？</li>
  <li>如何选取合适的$\lambda$？</li>
</ol>

<h2 id="section-8">参考资料</h2>

<ol class="bibliography"><li><span id="ng_ml_r_2014">[1]A. Ng, “Regularization: The problem of overfitting.” Coursera, 2014.</span>

[<a href="https://class.coursera.org/ml-007">Online</a>]


</li></ol>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习: 多分类问题</title>
      <link href="http://www.geogebra.com.cn/2014/10/machine-learning-multiple-classification" />
      <pubdate>2014-10-20T00:00:00+08:00</pubdate>
      <author>Jide Qian</author>
      <guid>http://www.geogebra.com.cn/2014/10/machine-learning-multiple-classification</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">问题描述</h2>

<p>许多分类器主要是为了解决二分类问题，对只有1和0两个类别标签对数据分类，比如Logistic回归和SVM。多分类问题是对两个以上类别标签的数据集分类。</p>

<p>解决多分类问题的思路：</p>

<ol>
  <li>多个二分类器的组合；</li>
  <li>直接采用多分类器。</li>
</ol>

<h2 id="one-vs-all">one-vs-all</h2>

<p>针对每类训练一个它与其余所有数据的二分类器，若有$N(N &gt; 2)$个类别，就需要训练$N$个分类器。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-10-20-machine-learning-multiple-classification_1.png"><img src="/assets/images/2014-10-20-machine-learning-multiple-classification_1.png" alt="one-vs-all也叫one-vs-rest" /></a><div class="caption">Figure 1:  one-vs-all也叫one-vs-rest [<a href="/assets/images/2014-10-20-machine-learning-multiple-classification_1.png">PNG</a>]</div></div></div>

<p>Logistic回归解决多分类问题<a href="#ng_ml_lr_2014">[1, Pp. 30-31]</a>：</p>

<ol>
  <li>针对每一类$i$训练一个Logistic分类器$h_\theta^{(i)}(x)$，这是第$i$类与其它类别的二分类问题；</li>
  <li>$x$所属的类别$c$满足$h_\theta^{(c)}(x) = \max_ih_\theta^{(i)}(x)$（属于概率最大的那个类别）。</li>
</ol>

<h3 id="section-1">存在的问题</h3>

<p>one-vs-all可能导致数据不均衡，其中一类的数据很少，其余的数据很多。</p>

<h2 id="section-2">神经网络</h2>

<p>神经网络是扩展Logistic模型组合的one-vs-all方法。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-10-20-multiple-classification.png"><img src="/assets/images/2014-10-20-multiple-classification.png" alt="神经网络的one-vs-all策略" /></a><div class="caption">Figure 2:  神经网络的one-vs-all策略 [<a href="/assets/images/2014-10-20-multiple-classification.png">PNG</a>]</div></div></div>

<p>注意其中的输出向量是0和1组成的向量，<strong>而非</strong>输出类别标签<sup id="fnref:class_label_method"><a href="#fn:class_label_method" class="footnote">1</a></sup>，$K(K \geq 3)$类则输出层需要$K$个神经元，$K＝2$时输出层只需要一个神经元<a href="#ng_ml_nnl_2014">[2, P. 2]</a>。</p>

<h2 id="section-3">思考问题</h2>

<ol>
  <li>神经网络能避免one-vs-all导致的数据不平衡问题吗？</li>
</ol>

<h2 id="section-4">参考资料</h2>

<ol class="bibliography"><li><span id="ng_ml_lr_2014">[1]A. Ng, “Logistic Regression.” Coursera, 2014.</span>

[<a href="https://class.coursera.org/ml-007">Online</a>]


</li>
<li><span id="ng_ml_nnl_2014">[2]A. Ng, “Neural Networks: Learning.” Coursera, 2014.</span>

[<a href="https://class.coursera.org/ml-007">Online</a>]


</li></ol>

<h3 id="section-5">脚注</h3>
<div class="footnotes">
  <ol>
    <li id="fn:class_label_method">
      <p>神经网络的输出不是$1,2,3,\ldots$这样的类别标签，Logistic回归模型输出属于某一类的概率，输入特征属于输出概率最靠近$1$的类别。 <a href="#fnref:class_label_method" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习: Logistic回归</title>
      <link href="http://www.geogebra.com.cn/2014/10/machine-learning-logistic-regression" />
      <pubdate>2014-10-19T00:00:00+08:00</pubdate>
      <author>Jide Qian</author>
      <guid>http://www.geogebra.com.cn/2014/10/machine-learning-logistic-regression</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">模型介绍</h2>

<div class="image_line">
<div class="image_card">
<img src="/assets/images/2014-10-19-logistic_regression_0.png" alt="Logistic函数" />
<div class="caption">图1 Logistic函数</div>
</div>
</div>

<p>Logistic回归（Logistic Regression）用于解决二分类问题，而非回归问题。Logistic函数将回归问题转化为了分类问题。</p>

<p>Logistic回归模型：
\begin{equation}
h_\theta(x) = g\left(\theta^Tx\right)
\end{equation}
其中
\begin{equation}
g\left(z\right) = \frac{1}{1 + e^{-z}}
\label{eq:sigmoid-function}
\end{equation}</p>

<blockquote>
  <p>上式也称为sigmoid function、logistic function，如<a href="#fig1">图1</a>所示。</p>
</blockquote>

<p>分类问题的判别条件如下：
\begin{equation}
y = \left\{
\begin{aligned}
&amp; 1 &amp; h_\theta(x) \ge 0.5 \\
&amp; 0 &amp; h_\theta(x) &lt; 0.5
\end{aligned}
\right. 
\end{equation}
也就是，若$\theta^Tx \ge 0$，则$y = 1$；若$\theta^Tx &lt; 0$，则$y = 0$。Logistic回归模型可以看作是计算属于类别1的概率
\begin{equation}
h_\theta\left(x\right) = \frac{1}{1 + e^{-\theta^Tx}} = P\left(y=1\big|x; \theta\right)
\end{equation}</p>

<p>因此，对于而分类问题，有
\begin{equation}
P\left(y=0\big|x; \theta\right) ＝ 1 - P\left(y=1\big|x; \theta\right) ＝ 1 - h_\theta\left(x\right)
\end{equation}</p>

<p>$\theta^Tx = 0$称为决策界（Decision Boundary）。决策界可以时线性的，也可以是非线性的。</p>

<p><img src="/assets/images/2014-10-19-logistic_regression_2.png" alt="线性决策界" /> </p>

<div class="caption">图2 线性决策界</div>

<p><img src="/assets/images/2014-10-19-logistic_regression_3.png" alt="非线性决策界" /></p>

<div class="caption">图3 非线性决策界</div>

<h2 id="section-1">代价函数</h2>

<p>线性回归的代价函数框架为：</p>

<p>\begin{equation}
J(\theta) = \frac{1}{m}\sum_{i=1}^{m}\frac{1}{2}{\left(h_{\theta}\left(x^{(i)}\right) - y^{(i)}\right)^2}
\end{equation}</p>

<p>令</p>

<p>\begin{equation}
\mbox{Cost}\left(h_\theta(x), y\right) = \frac{1}{2}\left( h_\theta(x) - y \right) ^ 2
\end{equation}</p>

<p>当$h_\theta(x)$为Logistic回归模型（<a href="#fig4">图4</a>左）和线性回归模型（<a href="#fig4">图4</a>右）时，分别为非凸函数和凸函数，非凸函数不能用梯度下降法找到全局最优解。因此，Logistic回归需要采用新的代价函数才能用梯度下降法求解。</p>

<div class="image_line" id="cost_function_curve"><div class="image_card"><a href="/assets/images/2014-10-19-logistic_regression_1.png"><img src="/assets/images/2014-10-19-logistic_regression_1.png" alt="代价函数" /></a><div class="caption">Figure 1:  代价函数 [<a href="/assets/images/2014-10-19-logistic_regression_1.png">PNG</a>]</div></div></div>

<p>Logistic回归采用</p>

<p>\begin{equation}
\mbox{Cost}\left(h_\theta(x), y\right) = \left\{
\begin{aligned}
&amp; -\log\left(h_\theta(x)\right) &amp; y=1 \\
&amp; -\log\left(1-h_\theta(x)\right) &amp; y=0
\end{aligned}
\right. 
\end{equation}</p>

<p>上式等价于</p>

<p>\begin{equation}
\mbox{Cost}\left(h_\theta(x), y\right) = -y\log\left(h_\theta(x)\right) - (1 - y)\log\left(1-h_\theta(x)\right)
\end{equation}</p>

<p>因此，Logistic回归的代价函数为</p>

<p>\begin{equation}
\begin{aligned}
J(\theta)<br />
= &amp; \frac{1}{m}\sum_{i=1}^{m}\mbox{Cost}\left(h_\theta\left(x^{(i)}\right), y^{(i)}\right) \\
= &amp; -\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right)\log \left(1-h_\theta\left(x^{(i)}\right)\right)\right)
\end{aligned}
\label{eq:cost_function_logistic_regression}
\end{equation}</p>

<blockquote>
  <p>若套用线性回归的代价函数，则$J(\theta)$非凸，不利于优化算法。该代价函数可从统计中最大似然估计（maximum likehood estimation）的角度推导。</p>
</blockquote>

<h2 id="section-2">参数估计</h2>

<p>参数估计是通过最小化代价函数\eqref{eq:cost_function_logistic_regression}，求解模型参数$\theta$。</p>

<p>\[
\min_\theta J(\theta)
\]</p>

<h3 id="section-3">梯度下降法</h3>

<p>貌似线性回归的梯度下降法的结构，但注意$h_\theta$的定义不同。</p>

<p>repeat until convergence {
\[
\theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)x_j^{(i)}~~~~~~(j = 0, 1, \ldots, n)
\]
}</p>

<p><strong>注意事项</strong></p>

<ul>
  <li>特征尺度规范化对加速Logistic回归的梯度下降法依然有效；</li>
  <li>其它注意事项同梯度下降法求解线性回归参数。</li>
</ul>

<p><img src="/assets/images/2014-10-19-logistic_regression_4.png" alt="采用Matlab的优化框架求解模型参数" /></p>

<div class="caption">图5 采用Matlab的优化框架求解模型参数</div>

<h3 id="section-4">其它算法</h3>

<ul>
  <li>Conjugate gradient</li>
  <li>BFGS</li>
  <li>L-BFGS</li>
</ul>

<blockquote>
  <p>这几种算法不需要手工选择学习率$\alpha$，通常比梯度下降法快，但是更复杂。</p>
</blockquote>

<h2 id="section-5">思考问题</h2>

<ol>
  <li>代价函数如何求偏导？</li>
</ol>

<h2 id="section-6">应用范例</h2>

<h2 id="section-7">参考资料</h2>

<ol>
  <li><a href="https://class.coursera.org/ml-007">Machine Learning （Andrew Ng）</a>  </li>
  <li><a href="http://en.wikipedia.org/wiki/Logistic_regression">Wikipedia: Logistic regression</a></li>
</ol>

]]&gt;</content:encoded>
    </item>
    
  </channel>
</rss>
