<?xml version="1.0" encoding="utf-8"?>
  <rss version="2.0"
        xmlns:content="http://purl.org/rss/1.0/modules/content/"
        xmlns:atom="http://www.w3.org/2005/Atom"
  >
  <channel>
    <title>Jiye Qian</title>
    <link href="http://jiyeqian.bitbucket.org/feed/" rel="self" />
    <link href="http://jiyeqian.github.io" />
    <lastBuildDate>2014-11-03T04:04:47+08:00</lastBuildDate>
    <webMaster>ccf.developer@gmail.com</webMaster>
    
    <item>
      <title>CSS Essential</title>
      <link href="http://jiyeqian.bitbucket.org/2014/10/css-essential"/>
      <pubDate>2014-10-25T15:37:41Z</pubDate>
      <author>Jiye Qian</author>
      <guid>http://jiyeqian.bitbucket.org/2014/10/css-essential</guid>
      <content:encoded><![CDATA[<blockquote>
  <h4 id="what-is-css">What is CSS?</h4>
  <hr />
  <p>The basic goal of the Cascading Stylesheet (CSS) language is to allow a browser engine to paint elements of the page with specific features, like colors, positioning, or decorations. </p>

  <p>CSS主要作用是设定元素（通常称为盒子）的属性，并将设定这些元素在页面的位置关系。CSS解决的主要问题是：    </p>

  <ol>
    <li>如何选择指定的元素？选择器   </li>
    <li>如何设定元素的属性？盒模型（Box Model）   </li>
    <li>如何设置元素间的位置关系？定位机制（Positioning Scheme）</li>
  </ol>
</blockquote>

<h2 id="section">基础技术</h2>

<h3 id="section-1">基本语法</h3>

<p>CCS语法元素主要由选择器，属性和值构成，通过规则给选择器范围内的属性赋值。   </p>

<div class="image_line">
<div class="image_card">
<img src="../../../assets/images/2014-10-25-css-essential_css-syntax-ruleset.png" alt="CSS语法" />
<div class="caption">CSS语法（规则）</div>
</div>
</div>

<!---
<div class="image_card">
<img src="../../../assets/images/2014-10-21-pla_1.png" alt="CSS语法">
<div class="caption">CSS语法（规则）</div>
</div>
--->

<h4 id="section-2">各部分元素含义：</h4>

<ul>
  <li><code>div p</code>：属性选择器</li>
  <li><code>#id</code>：id选择器</li>
  <li><code>first_line</code>：伪元素（pseudo-element）</li>
  <li><code>background-colr</code>：属性（property）</li>
  <li><code>red</code>：值（value）</li>
</ul>

<h3 id="section-3">基本选择器</h3>

<p>选择器相当于作用域规则，选择属性设置起作用的范围。另一个相关概念是<a href="#block-formatting-context">块格式化环境</a>。</p>

<h4 id="section-4">常用选择器</h4>

<table>
  <tbody>
    <tr>
      <td>*</td>
      <td>通用元素选择器，匹配任何元素。</td>
    </tr>
    <tr>
      <td>E</td>
      <td>标签选择器，匹配所有使用E标签的元素。</td>
    </tr>
    <tr>
      <td>.info</td>
      <td>class选择器，匹配所有class属性中包含info的元素。</td>
    </tr>
    <tr>
      <td>#footer</td>
      <td>id选择器，匹配所有id属性等于footer的元素。</td>
    </tr>
  </tbody>
</table>

<h4 id="section-5">多元素的组合选择器</h4>

<table>
  <tbody>
    <tr>
      <td>E,F</td>
      <td>多元素选择器，同时匹配所有E元素或F元素，E和F之间用逗号分隔。</td>
    </tr>
    <tr>
      <td>E F</td>
      <td>后代元素选择器，匹配所有属于E元素后代的F元素，E和F之间用空格分隔。</td>
    </tr>
    <tr>
      <td>E &gt; F</td>
      <td>子元素选择器，匹配所有E元素的子元素F。</td>
    </tr>
    <tr>
      <td>E + F</td>
      <td>毗邻元素选择器，匹配所有紧随E元素之后的同级元素F。</td>
    </tr>
  </tbody>
</table>

<h3 id="css">CSS的优先级别</h3>

<p>基本规则是<code>行内样式 &gt; id样式 &gt; class样式 &gt; 标签名样式</code>。
如下元素   </p>

<div class="highlight"><pre><code class="css"><span class="o">&lt;</span><span class="nt">div</span> <span class="nt">id</span><span class="o">=</span><span class="s2">&quot;ID&quot;</span> <span class="nt">class</span><span class="o">=</span><span class="s2">&quot;CLASS&quot;</span> <span class="nt">style</span><span class="o">=</span><span class="s2">&quot;color:black;&quot;</span><span class="o">&gt;&lt;/</span><span class="nt">div</span><span class="o">&gt;</span>
</code></pre></div>

<p>作用在其上样式的优先级从低到高是<code>div &lt; .class &lt; div.class &lt; #id &lt; div#id &lt; #id.class &lt; div#id.class</code>。</p>

<h3 id="section-6">属性赋值</h3>

<p>确定属性最终取值要<a href="http://www.w3.org/TR/2001/WD-css3-values-20010713/#specified" title="Specified, computed, and actual values">经历3步</a>（<a href="http://www.w3.org/TR/CSS2/cascade.html#value-stages" title="Specified, computed, and actual values">CSS2经历4步</a>）：首先获取CSS样式中的指定值（specified value），然后如有必要则转换为绝对值或计算值（computed value），最后根据局部环境的约束再转换为实际值（actual value）。</p>

<p>指定值可能是绝对值（例如：<code>2mm</code>、<code>red</code>等），也可能是相对值（例如：<code>auto</code>、<code>1.2em</code>、<code>12%</code>等）。对于绝对值而言不需要计算即可获得计算值，相对值需要再借助参考值计算获得计算值。如果属性没有指定值，它的取值继承父元素。</p>

<p>在特定的客户端环境，可能还需要将计算值转换为实际值，比如可能需要将小数的边界近似取整。</p>

<blockquote>
  <h4 id="section-7">常用的赋值规则</h4>
  <hr />
  <ol>
    <li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/color_value" title="&lt;color&gt;">颜色</a>赋值的三种形式：关键字、RGB空间、HSL空间；</li>
    <li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/length" title="&lt;length&gt;">长度</a>赋值的两种形式：以<code>em</code>、<code>ex</code>、<code>ch</code>、<code>rem</code>、<code>vw</code>、<code>vh</code>、<code>vmin</code>、<code>vmax</code>为单位的相对赋值，以<code>cm</code>、<code>mm</code>、<code>in</code>、<code>pt</code>、<code>pc</code>、<code>px</code>为单位的绝对赋值；</li>
    <li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/percentage" title="&lt;percentage&gt;">百分数</a>赋值：<code>width</code>、<code>margin</code>和<code>padding</code>等接受百分数赋值。</li>
  </ol>

  <p>示例：    </p>

  <p><code>em</code>等单位和百分数都是相对赋值，需要继承父属性的参考值。<code>em</code>是针对字体大小的值，$w$ <code>em</code> = $w$ $\times$ <code>font-size</code>。</p>

  <div class="highlight"><pre><code class="html"><span class="nt">&lt;div</span> <span class="na">style=</span><span class="s">&quot;font-size:18px;&quot;</span><span class="nt">&gt;</span>
  Full size text (18px)
  <span class="nt">&lt;span</span> <span class="na">style=</span><span class="s">&quot;font-size:50%;&quot;</span><span class="nt">&gt;</span>50%<span class="nt">&lt;/span&gt;</span>
  <span class="nt">&lt;span</span> <span class="na">style=</span><span class="s">&quot;font-size:200%;&quot;</span><span class="nt">&gt;</span>200%<span class="nt">&lt;/span&gt;</span>
<span class="nt">&lt;/div&gt;</span>
</code></pre></div>
  <p>上述代码中，<code>50%</code>从父属性继承而来的值是<code>18px</code>，然后再乘以<code>50%</code>（等价于<code>0.5em</code>）后实际大小是<code>9px</code>（<a href="https://developer.mozilla.org/en-US/docs/Web/CSS/percentage" title="&lt;percentage&gt;">查看效果</a>）。</p>
</blockquote>

<p>当一个属性可以有多个方向可设置值时，存在形如以下简写的赋值规则：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">位置</th>
      <th style="text-align: left">赋值规则</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="../../../assets/images/2014-10-25-css-essential_border1.png" alt="1" /></td>
      <td style="text-align: left"><code>border-width: 1em</code></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="../../../assets/images/2014-10-25-css-essential_border2.png" alt="2" /></td>
      <td style="text-align: left"><code>border-width: 1em 2em</code></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="../../../assets/images/2014-10-25-css-essential_border3.png" alt="3" /></td>
      <td style="text-align: left"><code>border-width: 1em 2em 3em</code></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="../../../assets/images/2014-10-25-css-essential_border4.png" alt="4" /></td>
      <td style="text-align: left"><code>border-width: 1em 2em 3em 4em</code></td>
    </tr>
  </tbody>
</table>

<h2 id="section-8">盒模型</h2>

<p>CSS将HTML的元素（可认为是标签）定义为适合CSS处理的矩形盒（rectangular box）。<a href="https://developer.mozilla.org/en-US/docs/Web/CSS/box_model">盒模型（box model）</a>描述了这些矩形盒的尺寸、属性（颜色、背景和边框等）和位置特性，浏览器根据盒模型实现页面的渲染与显示。</p>

<p>按照HTML的元素是否新开一行可分为块（block-level）元素和内联（inline）元素，这一特性对设置元素的布局至关重要。</p>

<blockquote>
  <h4 id="htmlblock-levelinline">HTML的块（block-level）元素和内联（inline）元素</h4>
  <hr />
  <p><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Block-level_elements">块元素</a>充满父元素的所有空间，每个块元素都会另起一个新行显示。块元素之内还可以包含块元素和行内元素。HTML定义的块元素包括<code>&lt;div&gt;, &lt;span&gt;, &lt;p&gt;</code>等。<a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Inline_elemente">内联元素</a>只占据标签包含内容的空间，不会另起新行显示。内联元素通常只包含内联元素。HTML定义的内联元素包括<code>&lt;span&gt;, &lt;code&gt;, &lt;textarea&gt;</code>等。   </p>

  <p><img src="../../../assets/images/2014-10-25-css-essential_css_box-intro.png" alt="d" /> </p>

  <p>上例中，<code>&lt;p&gt;</code>是块元素，新起一行显示，并且撑满了行，<code>&lt;em&gt;</code>是内联元素，紧接上一元素显示。</p>
</blockquote>

<p>CSS的<code>display</code>属性可设定HTML元素生成盒子的类型是块还是内联，常用的属性值有<code>block, inline, inline-block, none</code>。将<code>display</code>属性设为<code>block</code>，可将内联元素转为块元素。盒模型有四类边：margin edge，border edge，padding edge，content edge。    </p>

<div class="image_line">
<div class="image_card">
<img src="../../../assets/images/2014-10-25-css-essential_box_model.gif" alt="CSS语法" />
<div class="caption">盒模型的四类边</div>
</div>
</div>

<p><code>.box {with: ...; height: ...}</code>设置的是只是<code>Content</code>尺寸。相关边的的设置方法是：</p>

<div class="highlight"><pre><code class="css"><span class="nc">.box</span> <span class="p">{</span>
    <span class="k">width</span><span class="o">:</span> <span class="o">...</span><span class="p">;</span>
    <span class="k">height</span><span class="o">:</span> <span class="o">...</span><span class="p">;</span>
    <span class="k">padding</span><span class="o">:</span> <span class="o">...</span><span class="p">;</span>
    <span class="k">border</span><span class="o">:</span> <span class="o">...</span><span class="p">;</span>
    <span class="k">margin</span><span class="o">:</span> <span class="o">...</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>

<p>内联元素设置属性<code>height</code>和<code>width</code>是没有用的，致使它变宽变大的原因是内部元素的宽高<code>+padding</code>。</p>

<p>满足一定条件俩个盒子的外边届（margin）会叠加，使得估计这俩个盒子的位置关系变得复杂。</p>

<blockquote>
  <h4 id="margin-collapsing">外边叠加（margin collapsing）</h4>
  <hr />
  <p>上边距（top margin）和下边距（bottom margin）有时会叠加，叠加后两者最大值作为两者间的边距。发生外边叠加的3种情况（<a href="http://www.cnblogs.com/cuishengli/archive/2012/06/22/2558859.html#CSS 外边距合并">图解</a>）<sup id="fnref:ccs_master"><a href="#fn:ccs_master" class="footnote">1</a></sup>：</p>

  <ol>
    <li>兄弟（Adjacent siblings）块：相邻块中，兄的下边界和弟的上边界会叠加；</li>
    <li>父与首尾孩子（Parent and first/last child）块：父块和首个孩子的margin-top之间不被任何东西分隔，则它们会叠加；父块和最后孩子的margin-bottom之间不被任何东西分隔，则它们会叠加；</li>
    <li>空块（Empty blocks）：不存在border、padding、inline content、height或min-height分隔块的margin-top和margin-bottom，上下边界会叠加。</li>
  </ol>

  <p>注意事项：</p>

  <ul>
    <li>当与负边界叠加时，叠加后的边界是最大正边界和最小边界之和；</li>
    <li>浮动的（floating）和绝对定位的（absolutely positioned）元素不参与外边叠加（这是因为创建了新的<a href="#block-formatting-context">块格式化环境</a>，而边界叠加只发生在同一块格式化环境）。</li>
  </ul>

</blockquote>

<h2 id="section-9">可视化格式模型</h2>

<blockquote>
  <h4 id="section-10">可视化格式模型解决的主要问题</h4>
  <hr />
  <ol>
    <li>如何生成盒子？</li>
    <li>盒子如何在页面布局？</li>
  </ol>
</blockquote>

<p><a href="https://developer.mozilla.org/en-US/docs/Web/Guide/CSS/Visual_formatting_model">可视化格式模型（Visual formatting model）</a>是用于处理网页文档并显示到虚拟设备上的算法，该模型将文档中的每个元素生成符合盒模型的盒子，然后对这些模型进行布局。可视化格式模型包括盒子的生成和定位两部分。</p>

<h3 id="box-generation">生成机制（Box Generation）</h3>

<p>生成盒子的类型通过CSS属性<code>display</code>设定，当设定属性为<code>block</code>、<code>list-item</code>和<code>table</code>时，生成块盒子，当设定属性为<code>inline</code>、<code>inline-block</code>和<code>inline-table</code>时，生成内联盒子。<a href="#htmlblock-levelinline">HTML元素的属性</a>决定了该元素生成盒子类型的默认值，块元素默认生成块盒子，内联元素默认生成内联盒子。不同类型盒子的定位机制不同。</p>

<h3 id="positioning-scheme">定位机制（Positioning Scheme）</h3>

<p>CSS的盒子有3种基本定位机制：常规流（normal flow）、浮动（floats）和绝对定位（absolute position），默认定位机制是常规流。在常规流中，盒子一个接一个排列，floats算法可以将盒子从常规流中抽出来，绝对定位通过包含它盒子的坐标系统来定位。</p>

<h4 id="normal-flow">Normal Flow</h4>

<blockquote>
  <h4 id="normal-flow-1">如何进入常规流（normal flow）</h4>
  <hr />
  <p>CCS将盒子的<code>position</code>属性设置为<code>static</code>或<code>relative</code>，并且将<code>float</code>属性设置为<code>none</code>。默认值<code>position: static</code>，<code>float: none</code>。</p>

  <p>常规流可以理解为盒子按照读入的先后次序依次处理，就像水流一样连续有序，每个页面对应一个流。浮动和绝对定位，可将读入序列中的某些盒子从这个流中抽取出来，单独处理。</p>
</blockquote>

<p>在常规流中，块盒子（block-level boxes ）和内联盒子（inline boxes）分别从纵向和横向对元素进行布局。块盒子通过垂直的方式一个接一个的排列，盒子之间的距离通过垂直方向的边界（margin-top和margin-bottom）控制（计算距离时要注意盒子<a href="#margin-collapsing">外边叠加问题</a>）。内联盒子通过水平方式排列，设置垂自方向的padding、borders和margins无效。水平排列的内联盒子通过行盒子（line box）组织在一起，行盒子的高度总是足够容纳包含在其中的所有盒子，可以通过设置行高（line height）控制行盒子的高度。因此，改变内联盒子尺寸的参数只有水平方向上的borders、padding、margins以及line height。    </p>

<div class="image_line">
<div class="image_card">
<img src="../../../assets/images/2014-10-25-css-essential_css-syntax-ruleset_line_box.jpeg" alt="CSS语法" />
<div class="caption">包含在行盒子中的内联元素</div>
</div>
</div>

<p>CSS2.1可以设置<code>display</code>属性为<code>inline-block</code>，融合inline和block的属性。在水平方向按内联盒子的方式布局，同时可以像块盒子一样设置widths、heights和垂自方向的 margins、padding。 </p>

<div class="image_line">
<div class="image_card">
<img src="../../../assets/images/2014-10-25-css-essential_relative.png" alt="设置relative的效果" />
<div class="caption">设置relative的效果"position: relative; left: 20px; top: 20px;"</div>
</div>
</div>

<h4 id="floats">Floats</h4>

<blockquote>
  <h4 id="floats-1">如何使用浮动模式（floats）</h4>
  <hr />
  <p>CCS将盒子的<code>float</code>属性设置为<code>left</code>或<code>right</code>，并且将<code>position</code>属性设置为<code>static</code>或<code>relative</code>。</p>

  <p><code>float</code>的非<code>none</code>属性暗示了盒子是<code>block</code>类型，因此，非块类型盒子的<code>display</code>属性会因为设置了<code>float</code>属性而改变为块类型。</p>
</blockquote>

<p>当盒子采用floats算法定位时，盒子会从常规流中被抽取出来，向左或向右移动，直到遇到父盒子或设置了float属性盒子的边界。</p>

<div class="image_line">
<div class="image_card">
<img src="../../../assets/images/2014-10-25-css-essential_floats.png" alt="设置floats的效果" />
<div class="caption">设置<code>float</code>的效果</div>
</div>
</div>

<p>上图3个红色的方块，两个左浮（<code>float: left</code>），一个右浮。第二个左浮窗口位于第一个左浮动窗口右边，如果再增加左浮窗口，会不断的照此叠加，充满父盒子后会换行继续显示。</p>

<p>一个盒子设置了浮动后，会影响它的兄弟元素，具体的影响方式较为复杂，这要视乎这些兄弟元素是块级元素还是内联元素，若是块级元素会无视这个浮动的块框，使自身尽可能与这个浮动元素处于同一行，导致被浮动元素覆盖，除非这些<code>div</code>设置了宽度，并且父元素的宽度不足以包含它们，这样兄弟元素才会被强制换行；若是内联元素，则会尽可能围绕浮动元素。</p>

<p>浮动元素脱离了普通流，因此包含它的父元素不会因为这个浮动元素的存在而自动撑高，这就造成了高度塌陷。下图所示，由于左边盒子浮动而脱离了常规流，父元素的高度只是由<code>span</code>盒子决定，看起来就像父元素高度塌陷了。</p>

<div class="image_line">
<div class="image_card">
<img src="../../../assets/images/2014-10-25-css-essential_clearfloat1.png" alt="浮动的影响" />
<div class="caption">浮动的影响</div>
</div>
</div>

<p>当浮动影响到盒子的布局时，需要<a href="#clearing-floats">清除浮动</a>。</p>

<h4 id="absolute-positioning">Absolute Positioning</h4>

<blockquote>
  <h4 id="absolute-positioning-1">如何采用绝对定位（absolute positioning）</h4>
  <hr />
  <p>CCS将盒子的<code>position</code>属性设置为<code>absolute</code>或<code>fixed</code>。</p>

  <p>当设置为<code>fixed</code>的时候，盒子的位置相对于浏览器可见的视窗固定，即使拖动浏览器的滚动条，盒子的位置也固定不变。</p>

</blockquote>

<div class="image_line">
<div class="image_card">
<img src="../../../assets/images/2014-10-25-css-essential_absolute.png" alt="设置absolute的效果" />
<div class="caption">设置absolute的效果"position: absolute; left: 20px; top: 20px;"</div>
</div>
</div>

<h4 id="clearing-floats">清除浮动（clearing floats）</h4>

<p>由于浮动元素会影响它的兄弟元素的位置和父元素产生高度塌陷，需要清除浮动。</p>

<p>常用的清除浮动方法是<code>clear: both</code>，<code>clear</code>的属性值<code>both</code>、<code>left</code>、<code>right</code>、<code>none</code>、<code>inherit</code> 分别代表在元素左右两侧不允许出现浮动元素、左侧不允许出现浮动元素、右侧不允许出现浮动元素、不清除浮动、继承父元素的值。</p>

<p>但是，<code>clear</code>只是清除了浮动对兄弟元素的影响，而高度塌陷问题还没有解决，需要更高级的清除浮动——闭合浮动。为什么叫闭合浮动？因为浮动的元素脱离了普通流，对于它的父元素，它并没有闭合，这时候就需要闭合浮动了。</p>

<blockquote>
  <h4 id="section-11">闭合浮动的3种方法</h4>
  <hr />
  <p>（1）空<code>div</code>方法</p>

  <div class="highlight"><pre><code class="html"><span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;box&quot;</span><span class="nt">&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;main left&quot;</span><span class="nt">&gt;</span>我设置了左浮动 float: left<span class="nt">&lt;/div&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">style=</span><span class="s">&quot;clear: both;&quot;</span><span class="nt">&gt;&lt;/div&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;aside&quot;</span><span class="nt">&gt;</span>我是页脚，我的上面添加了一个设置了 clear: both 的空 div<span class="nt">&lt;/div&gt;</span>
<span class="nt">&lt;/div&gt;</span>
</code></pre></div>
  <p>空<code>div</code>方法很方便，但是加入了没有涵义的<code>div</code>，这违背了结构与表现分离的原则，并且后期维护也不方便。</p>

  <p>（2）<code>overflow</code>方法</p>

  <div class="highlight"><pre><code class="html"><span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;box&quot;</span> <span class="na">style=</span><span class="s">&quot;overflow: hidden; *zoom: 1;&quot;</span><span class="nt">&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;main left&quot;</span><span class="nt">&gt;</span>我设置了左浮动 float: left<span class="nt">&lt;/div&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;aside left&quot;</span><span class="nt">&gt;</span>我是页脚，但是我也设置了左浮动。<span class="nt">&lt;/div&gt;</span>
<span class="nt">&lt;/div&gt;</span>
</code></pre></div>
  <p>当元素内包含会超出父元素边界的子元素时，<code>overflow</code>方法可能会覆盖掉有用的子元素，或是产生了多余的滚动条。</p>

  <p>（3）<code>:after</code>伪元素的方法</p>

  <div class="highlight"><pre><code class="html"><span class="nt">&lt;style&gt;</span>
    <span class="nc">.clearfix</span> <span class="p">{</span><span class="c">/* 触发 hasLayout */</span> <span class="n">zoom</span><span class="o">:</span> <span class="m">1</span><span class="p">;</span> <span class="p">}</span>
    <span class="nc">.clearfix</span><span class="nd">:after</span> <span class="p">{</span><span class="k">content</span><span class="o">:</span> <span class="s1">&#39;.&#39;</span><span class="p">;</span> <span class="k">display</span><span class="o">:</span> <span class="k">block</span><span class="p">;</span> <span class="k">height</span><span class="o">:</span> <span class="m">0</span><span class="p">;</span> <span class="k">clear</span><span class="o">:</span> <span class="k">both</span><span class="p">;</span> <span class="k">visibility</span><span class="o">:</span> <span class="k">hidden</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">&lt;/style&gt;</span>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;box clearfix&quot;</span><span class="nt">&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;main left&quot;</span><span class="nt">&gt;</span>我设置了左浮动 float: left<span class="nt">&lt;/div&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;aside left&quot;</span><span class="nt">&gt;</span>我是页脚，但是我也设置了左浮动。<span class="nt">&lt;/div&gt;</span>
<span class="nt">&lt;/div&gt;</span>
</code></pre></div>
  <p>这个办法不但完美兼容主流浏览器，并且也很方便，使用重用的类，可以减轻代码编写，另外网页的结构也会更加清晰。</p>
</blockquote>

<p>清除浮动的详细介绍可参考<a href="http://kayosite.com/remove-floating-style-in-detail.html">《详说清除浮动》</a>（<a href="../../../assets/images/../html/clearfloat.html">示例</a>）。</p>

<h2 id="section-12">块格式化环境</h2>

<blockquote>
  <h4 id="block-formatting-context">块格式化环境（block formatting context）</h4>
  <hr />
  <p>块格式化环境是CCS渲染Web页面的一块区域，块盒子在该区域内布局。</p>

  <p>块格式化环境对（<code>float</code>）定位和清除（<code>clear</code>）浮动至关重要，定位和清除浮动只对同一块格式化环境中的对象有效。<code>float</code>不会影响到其它块格式化环境中的盒子，<code>clear</code>只清除同一块格式化环境中之前的float效果。</p>

  <p>简单来说，块格式化环境是一种属性，这种属性会影响着元素的定位以及与其兄弟元素之间的相互作用。</p>
</blockquote>

<p>块格式化环境就是一个作用范围，可理解为一个独立的容器，这个容器的里盒子的布局与这个容器外的不相干。</p>

<p>块格式化环境不存在嵌套包含关系，块格式化环境只包含该环境内的对象，不会再包含该环境中对象再创建的块格式化环境。（A block formatting context contains everything inside of the element creating it that is not also inside a descendant element that creates a new block formatting context.）</p>

<blockquote>
  <h4 id="section-13">创建块格式化环境的条件（满足任意一条即可）</h4>
  <hr />
  <ul>
    <li>the root element or something that contains it；</li>
    <li><code>float</code>设置为<strong>非</strong><code>none</code>；</li>
    <li><code>position</code>设置为<code>absolute</code>或<code>fixed</code>；</li>
    <li><code>display</code>设置为<code>inline-block</code>、<code>table-cell</code>、<code>table-caption</code>、<code>flex</code>或<code>inline-flex</code>；</li>
    <li><code>overflow</code>设置为<strong>非</strong><code>visible</code>。</li>
  </ul>
</blockquote>

<p>块格式化环境主要有三个特性（详见<a href="http://www.cnblogs.com/leejersey/p/3991400.html">《详说 Block Formatting Contexts (块级格式化上下文)》</a>，<a href="../../../assets/images/../html/bfc.html">示例</a>）：</p>

<ol>
  <li>阻止<a href="#margin-collapsing">外边距折叠</a>；    </li>
  <li>包含浮动的元素（<a href="#clearing-floats">清除浮动</a>之<code>overflow</code>方法）；   </li>
  <li>阻止元素被浮动元素覆盖。 </li>
</ol>

<h2 id="section-14">高级选择器</h2>

<h4 id="css-21-">CSS 2.1 属性选择器</h4>

<table>
  <tbody>
    <tr>
      <td>E[att]</td>
      <td>匹配所有具有att属性的E元素，不考虑它的值。（注意：E在此处可以省略，比如”[cheacked]”。以下同。）</td>
    </tr>
    <tr>
      <td>E[att=val]</td>
      <td>匹配所有att属性等于”val”的E元素。</td>
    </tr>
    <tr>
      <td>E[att~=val]</td>
      <td>匹配所有att属性具有多个空格分隔的值、其中一个值等于”val”的E元素。</td>
    </tr>
    <tr>
      <td>E[att|=val]</td>
      <td>匹配所有att属性具有多个连字号分隔（hyphen-separated）的值、其中一个值以”val”开头的E元素，主要用于lang属性，比如”en”、”en-us”、”en-gb”等等。</td>
    </tr>
  </tbody>
</table>

<p>示例：</p>

<div class="highlight"><pre><code class="css"><span class="nt">p</span><span class="o">[</span><span class="nt">title</span><span class="o">]</span> <span class="p">{</span> <span class="k">color</span><span class="o">:</span><span class="m">#f00</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">div</span><span class="o">[</span><span class="nt">class</span><span class="o">=</span><span class="nt">error</span><span class="o">]</span> <span class="p">{</span> <span class="k">color</span><span class="o">:</span><span class="m">#f00</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">td</span><span class="o">[</span><span class="nt">headers</span><span class="o">~=</span><span class="nt">col1</span><span class="o">]</span> <span class="p">{</span> <span class="k">color</span><span class="o">:</span><span class="m">#f00</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">p</span><span class="o">[</span><span class="nt">lang</span><span class="o">|=</span><span class="nt">en</span><span class="o">]</span> <span class="p">{</span> <span class="k">color</span><span class="o">:</span><span class="m">#f00</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">blockquote</span><span class="o">[</span><span class="nt">class</span><span class="o">=</span><span class="nt">quote</span><span class="o">][</span><span class="nt">cite</span><span class="o">]</span> <span class="p">{</span> <span class="k">color</span><span class="o">:</span><span class="m">#f00</span><span class="p">;</span> <span class="p">}</span>
</code></pre></div>

<h4 id="css-21-pseudo-classes">CSS 2.1 伪类（pseudo-classes）</h4>

<table>
  <tbody>
    <tr>
      <td>E:first-child</td>
      <td>匹配父元素的第一个子元素。</td>
    </tr>
    <tr>
      <td>E:link</td>
      <td>匹配所有未被点击的链接。</td>
    </tr>
    <tr>
      <td>E:visited</td>
      <td>匹配所有已被点击的链接。</td>
    </tr>
    <tr>
      <td>E:active</td>
      <td>匹配鼠标已经其上按下、还没有释放的E元素。</td>
    </tr>
    <tr>
      <td>E:hover</td>
      <td>匹配鼠标悬停其上的E元素。</td>
    </tr>
    <tr>
      <td>E:focus</td>
      <td>匹配获得当前焦点的E元素。</td>
    </tr>
    <tr>
      <td>E:lang(c)</td>
      <td>匹配lang属性等于c的E元素。</td>
    </tr>
  </tbody>
</table>

<p>示例：</p>

<div class="highlight"><pre><code class="css"><span class="nt">p</span><span class="nd">:first-child</span> <span class="p">{</span> <span class="k">font-style</span><span class="o">:</span><span class="k">italic</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">input</span><span class="o">[</span><span class="nt">type</span><span class="o">=</span><span class="nt">text</span><span class="o">]</span><span class="nd">:focus</span> <span class="p">{</span> <span class="k">color</span><span class="o">:</span><span class="m">#000</span><span class="p">;</span> <span class="k">background</span><span class="o">:</span><span class="m">#ffe</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">input</span><span class="o">[</span><span class="nt">type</span><span class="o">=</span><span class="nt">text</span><span class="o">]</span><span class="nd">:focus:hover</span> <span class="p">{</span> <span class="k">background</span><span class="o">:</span><span class="m">#fff</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">q</span><span class="nd">:lang</span><span class="o">(</span><span class="nt">sv</span><span class="o">)</span> <span class="p">{</span> <span class="k">quotes</span><span class="o">:</span> <span class="s2">&quot;\201D&quot;</span> <span class="s2">&quot;\201D&quot;</span> <span class="s2">&quot;\2019&quot;</span> <span class="s2">&quot;\2019&quot;</span><span class="p">;</span> <span class="p">}</span>
</code></pre></div>

<h4 id="css-21-pseudo-elements">CSS 2.1 伪元素（pseudo-elements）</h4>

<table>
  <tbody>
    <tr>
      <td>E:first-line</td>
      <td>匹配E元素的第一行。</td>
    </tr>
    <tr>
      <td>E:first-letter</td>
      <td>匹配E元素的第一个字母。</td>
    </tr>
    <tr>
      <td>E:before</td>
      <td>在E元素之前插入生成的内容。</td>
    </tr>
    <tr>
      <td>E:after</td>
      <td>在E元素之后插入生成的内容。</td>
    </tr>
  </tbody>
</table>

<p>示例：</p>

<div class="highlight"><pre><code class="css"><span class="nt">p</span><span class="nd">:first-line</span> <span class="p">{</span> <span class="k">font-weight</span><span class="o">:</span><span class="k">bold</span><span class="p">;</span> <span class="k">color</span><span class="p">;</span><span class="m">#600</span><span class="p">;</span> <span class="p">}</span>
<span class="nc">.preamble</span><span class="nd">:first-letter</span> <span class="p">{</span> <span class="k">font-size</span><span class="o">:</span><span class="m">1.5em</span><span class="p">;</span> <span class="k">font-weight</span><span class="o">:</span><span class="k">bold</span><span class="p">;</span> <span class="p">}</span>
<span class="nc">.cbb</span><span class="nd">:before</span> <span class="p">{</span> <span class="k">content</span><span class="o">:</span><span class="s2">&quot;&quot;</span><span class="p">;</span> <span class="k">display</span><span class="o">:</span><span class="k">block</span><span class="p">;</span> <span class="k">height</span><span class="o">:</span><span class="m">17px</span><span class="p">;</span> <span class="k">width</span><span class="o">:</span><span class="m">18px</span><span class="p">;</span> <span class="k">background</span><span class="o">:</span><span class="sx">url(top.png)</span> <span class="k">no-repeat</span> <span class="m">0</span> <span class="m">0</span><span class="p">;</span> <span class="k">margin</span><span class="o">:</span><span class="m">0</span> <span class="m">0</span> <span class="m">0</span> <span class="m">-18px</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">a</span><span class="nd">:link:after</span> <span class="p">{</span> <span class="k">content</span><span class="o">:</span> <span class="s2">&quot; (&quot;</span> <span class="n">attr</span><span class="p">(</span><span class="n">href</span><span class="p">)</span> <span class="s2">&quot;) &quot;</span><span class="p">;</span> <span class="p">}</span>
</code></pre></div>

<p>更多关于CSS选选择器类容可参考<a href="https://developer.mozilla.org/en-US/docs/Web/CSS/Reference">CSS reference</a>。</p>

<h2 id="section-15">使用技巧</h2>

<blockquote>
  <h4 id="important">!important规则</h4>
  <hr />
  <p>多条CSS语句互相冲突时，具有!important的语句将覆盖其他语句。由于IE不支持!important，所以也可以利用它区分不同的浏览器。</p>

  <div class="highlight"><pre><code class="css"><span class="nt">h1</span> <span class="p">{</span><span class="k">color</span><span class="o">:</span> <span class="nb">red</span> <span class="cp">!important</span><span class="p">;</span> <span class="k">color</span><span class="o">:</span> <span class="nb">blue</span><span class="p">;}</span>
</code></pre></div>
</blockquote>

<blockquote>
  <h4 id="section-16">容器水平居中</h4>
  <hr />
  <p>先为该容器设置一个明确宽度，然后将margin的水平值设为auto即可。</p>

  <div class="highlight"><pre><code class="css"><span class="nt">div</span><span class="nf">#container</span> <span class="p">{</span><span class="k">width</span><span class="o">:</span> <span class="m">760px</span><span class="p">;</span> <span class="k">margin</span><span class="o">:</span> <span class="m">0</span> <span class="k">auto</span><span class="p">;}</span>
</code></pre></div>
</blockquote>

<blockquote>
  <h4 id="section-17">禁止自动换行</h4>
  <hr />
  <p>文字在一行中显示完成，不要自动换行。</p>

  <div class="highlight"><pre><code class="css"><span class="nt">h1</span> <span class="p">{</span> <span class="k">white-space</span><span class="o">:</span> <span class="k">nowrap</span><span class="p">;</span> <span class="p">}</span>
</code></pre></div>
</blockquote>

<blockquote>
  <h4 id="section-18">图片宽度自适应</h4>
  <hr />
  <p>如何使得较大的图片，能够自动适应小容器的宽度？</p>

  <div class="highlight"><pre><code class="css"><span class="nt">img</span> <span class="p">{</span><span class="k">max-width</span><span class="o">:</span> <span class="m">100</span><span class="o">%</span><span class="p">}</span>
</code></pre></div>
</blockquote>

<blockquote>
  <h4 id="link">设置link状态的顺序</h4>
  <hr />
  <p>link的四种状态，需要按照下面的前后顺序进行设置。</p>

  <div class="highlight"><pre><code class="css"><span class="nt">a</span><span class="nd">:link</span> 
<span class="nt">a</span><span class="nd">:visited</span> 
<span class="nt">a</span><span class="nd">:hover</span> 
<span class="nt">a</span><span class="nd">:active</span>
</code></pre></div>
</blockquote>

<blockquote>
  <h4 id="text-transformfont-variant">Text-transform和Font Variant</h4>
  <hr />
  <p>Text-transform用于将所有字母变成小写字母、大写字母或首字母大写。</p>

  <div class="highlight"><pre><code class="css"><span class="nt">p</span> <span class="p">{</span><span class="k">text-transform</span><span class="o">:</span> <span class="k">uppercase</span><span class="p">}</span> 
<span class="nt">p</span> <span class="p">{</span><span class="k">text-transform</span><span class="o">:</span> <span class="k">lowercase</span><span class="p">}</span> 
<span class="nt">p</span> <span class="p">{</span><span class="k">text-transform</span><span class="o">:</span> <span class="k">capitalize</span><span class="p">}</span>
</code></pre></div>
  <p>Font Variant用于将字体变成小型的大写字母（即与小写字母等高的大写字母）。</p>

  <div class="highlight"><pre><code class="css"><span class="nt">p</span> <span class="p">{</span><span class="k">font-variant</span><span class="o">:</span> <span class="k">small-caps</span><span class="p">}</span>
</code></pre></div>
</blockquote>

<blockquote>
  <h4 id="section-19">用图片充当列表标志</h4>
  <hr />
  <p>默认情况下，浏览器使用一个黑圆圈作为列表标志，可以用图片取代它。</p>

  <div class="highlight"><pre><code class="css"><span class="nt">ul</span> <span class="p">{</span><span class="k">list-style</span><span class="o">:</span> <span class="k">none</span><span class="p">}</span>
<span class="nt">ul</span> <span class="nt">li</span> <span class="p">{</span>
    <span class="k">background-image</span><span class="o">:</span> <span class="sx">url(&quot;path-to-your-image&quot;)</span><span class="p">;</span>
    <span class="k">background-repeat</span><span class="o">:</span> <span class="k">none</span><span class="p">;</span>
    <span class="k">background-position</span><span class="o">:</span> <span class="m">0</span> <span class="m">0.5em</span><span class="p">;</span> 
<span class="p">}</span>
</code></pre></div>
</blockquote>

<blockquote>
  <h4 id="section-20">用图片替换文字</h4>
  <hr />
  <p>在标题栏中使用图片，但是又必须保证搜索引擎能够读到标题。</p>

  <div class="highlight"><pre><code class="css"><span class="nt">h1</span> <span class="p">{</span> 
    <span class="k">text-indent</span><span class="o">:</span> <span class="m">-9999px</span><span class="p">;</span> 
    <span class="k">background</span><span class="o">:</span> <span class="sx">url(&quot;h1-image.jpg&quot;)</span> <span class="k">no-repeat</span><span class="p">;</span> 
    <span class="k">width</span><span class="o">:</span> <span class="m">200px</span><span class="p">;</span>
    <span class="k">height</span><span class="o">:</span> <span class="m">50px</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>
</blockquote>

<h2 id="less">预处理器：less</h2>

<p>Less是一个CSS预编译器，意思指的是它可以扩展CSSS语言，添加功能如允许变量（variables），混合（mixins），函数（functions）和许多其他的技术，让你的CSS更具维护性，主题性，扩展性。</p>

<p>Less可运行在Node环境，浏览器环境和Rhino环境，同时也有3种可选工具供你编译文件和监视任何改变。</p>

<h2 id="section-21">参考资料</h2>

<ol>
  <li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS">Mozilla Developer Network: CSS</a></li>
  <li><a href="http://www.ruanyifeng.com/blog/2010/03/css_cookbook.html">CSS使用技巧</a>    </li>
  <li><a href="http://www.ruanyifeng.com/blog/2009/03/css_selectors.html">CSS选择器笔记</a></li>
  <li><a href="http://www.bootcss.com/p/lesscss/">LESS « 一种动态样式语言</a></li>
  <li><a href="http://dev.w3.org/csswg/css-box/">CSS basic box model</a></li>
  <li><a href="http://dev.w3.org/csswg/css-grid/">CSS Grid Layout Module Level 1</a></li>
  <li><a href="http://dev.w3.org/csswg/css-flexbox/">CSS Flexible Box Layout Module Level 1</a></li>
  <li><a href="http://www.cnblogs.com/cuishengli/archive/2012/06/22/2558859.html">CSS 框模型概述</a></li>
  <li><a href="http://www.cnblogs.com/leejersey/p/3991400.html">详说 Block Formatting Contexts (块级格式化上下文)</a></li>
  <li><a href="http://kayosite.com/remove-floating-style-in-detail.html">详说清除浮动</a></li>
  <li><a href="http://www.w3.org/TR/2001/WD-css3-values-20010713/#specified" title="Specified, computed, and actual values">Specified, computed, and actual values</a></li>
  <li><a href="http://www.w3.org/TR/css3-values/" title="CSS Values and Units Module Level 3">CSS Values and Units Module Level 3</a></li>
  <li><a href="http://www.w3.org/TR/2011/REC-css3-color-20110607/">CSS Color Module Level 3</a></li>
</ol>

<h2 id="section-22">脚注</h2>
<div class="footnotes">
  <ol>
    <li id="fn:ccs_master">
      <p>也可参考”CSS Mastery: Advanced Web Standards Solutions, Second Edition”一书的”Chapter 3: Visual Formatting Model Overview”。 <a href="#fnref:ccs_master" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>机器学习: 神经网络</title>
      <link href="http://jiyeqian.bitbucket.org/2014/10/machine-learning-neural-networks"/>
      <pubDate>2014-10-21T19:09:00Z</pubDate>
      <author>Jiye Qian</author>
      <guid>http://jiyeqian.bitbucket.org/2014/10/machine-learning-neural-networks</guid>
      <content:encoded><![CDATA[<h2 id="section">应用背景</h2>

<p>当特征数目巨大时，简单的Logistic回归无法满足需求。神经网络用于解决复杂的非线性问题 </p>

<p>hidden layer 提取特征</p>

<p>hidden layer：除了输入输出层之外的</p>

<h3 id="section-1">代价函数</h3>

<p>\begin{equation}
\begin{aligned}
J(\Theta) = -\frac{1}{m} &amp; \sum_{i=1}^{m}\sum_{k=1}^{K}\left(y_k^{(i)}\log \left(h_\Theta\left(x^{(i)} \right) \right)_k + \left(1 - y_k^{(i)}\right)\log\left(1 -  \left(h_\Theta\left(x^{(i)} \right) \right)_k \right) \right) \\ 
+\frac{\lambda}{2m} &amp;  \sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}\left(\Theta_{ji}^{(l)}\right)^2
\end{aligned}
\end{equation}</p>

<blockquote>
  <p>$h_\Theta (x) \in \mathbb{R}^K$，$(h_\Theta (x))_i$是第$i$个输出；$s_l$表示第$l$层神经元的个数（不含bias unit）；共$m$个样本，$K$个输出，$L$层。</p>
</blockquote>

<h3 id="section-2">参数估计方法</h3>

<p>\[
\min_\Theta J(\Theta)
\]</p>

<p>BP算法（BackPropagation algorithm）</p>

<p>各层误差估计：
<!--
\begin{equation}
\left\\{
\begin{aligned}
\delta^{(L)} &amp;= a^{(L)} - y  \\\
\delta^{(i)} &amp;= \left(\Theta^{(i)} \right)^T\delta^{(i+1)} ~.*~ g'\left(z^{(i)}\right)~~~(i = 2, 4, \ldots, L-1)
\end{aligned}
\right.
\label{eq:error_bpxxx}
\end{equation}
--></p>

<p>\begin{equation}
\delta^{(l)} = \left\{
\begin{aligned}
&amp; a^{(l)} - y &amp; (l = L)~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\
&amp; \left(\Theta^{(l)} \right)^T\delta^{(l+1)} ~.*~ g’\left(z^{(l)}\right) &amp; (l = L-1, L-2, \ldots, 2) 
\end{aligned}
\right. 
\label{eq:error_bp}
\end{equation}</p>

<p>其中，$g’\left(z^{(l)}\right) = a^{(l)} ~.*~ \left(1 - a^{(l)}\right)$；$z^{(l)} = \Theta^{(l)}a^{(l)}$；$\delta_j^{(l)}$表示第$l$层第$j$个节点的误差；对于bias节点$\delta_0^{(l)}=0$。</p>

<blockquote>
  <h4 id="bp-">BP 算法</h4>
  <hr />
  <p>训练集：$\left\{\left(x^{(1)}, y^{(1)}\right),\ldots,\left(x^{(m)}, y^{(m)}\right)\right\}$。 <br />
初始化：$\Delta_{ij}^{(l)} = 0$(对所有的$i,j,l$)。<br />
For $i=1$ to $m$ {  <br />
1. $a^{(1)} = x^{(i)}$； <br />
2. 算$a^{(l)}~~(l = 2, 3,\ldots, L)$；<br />
3. 利用\eqref{eq:error_bp}反向计算误差； <br />
4. $\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}\left(a^{(l)}\right)^T$（$a^{(l)}$向量也须补1）； <br />
5. 计算$D_{ij}^{(l)} = \frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta)$：
\[
\begin{aligned}
D_{ij}^{(l)} &amp; := \frac{1}{m}\Delta_{ij}^{(l)} + \frac{\lambda}{m}\Theta_{ij}^{(l)} &amp; (j \neq 0) \\
D_{ij}^{(l)} &amp; := \frac{1}{m}\Delta_{ij}^{(l)} &amp; (j = 0)
\end{aligned} 
\]
}</p>
</blockquote>

<p>算法技巧：矩阵展成（unroll）向量：</p>

<ol>
  <li>thetaVec = [Theta1(:); Theta2(:); Theta3(:)]；</li>
  <li>将向量化的待估参数作为costFunction的参数；</li>
  <li>costFunction内部再将向量还原为矩阵计算梯度；</li>
  <li>梯度向量化输出DVec = [D1(:); D2(:); D3(:)]。</li>
</ol>

<p>算法技巧：梯度检查（gradient checking）：</p>

<p>梯度检测方法也可推广到其它需要计算代价函数及其梯度的地方，比如logistic回归的代价函数。</p>

<div class="highlight"><pre><code class="matlab"><span class="k">for</span> <span class="nb">i</span> <span class="p">=</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">n</span>
   <span class="n">thetaPlus</span> <span class="p">=</span> <span class="n">theta</span><span class="p">;</span>
   <span class="n">thetaPlus</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="n">thetaPlus</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="o">+</span> <span class="n">EPSILON</span><span class="p">;</span>
   <span class="n">thetaMinus</span> <span class="p">=</span> <span class="n">theta</span><span class="p">;</span>
   <span class="n">thetaMinus</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="n">thetaMinus</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> – <span class="n">EPSILON</span><span class="p">;</span>
   <span class="n">gradApprox</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="p">(</span><span class="n">J</span><span class="p">(</span><span class="n">thetaPlus</span><span class="p">)</span> – <span class="n">J</span><span class="p">(</span><span class="n">thetaMinus</span><span class="p">));</span>
<span class="k">end</span>
</code></pre></div>

<blockquote>
  <p>theta 是$\Theta$的向量化，正常情况有gradApprox$\approx$DVec，通过比较gradApprox 与BP 算法所得DVec 的差距判断BP 算法的代价函数及其优化算法是否有subtle bugs。</p>
</blockquote>

<div class="highlight"><pre><code class="matlab"><span class="c">% If your backpropagation implementation is correct, then the relative difference will be small (less than 1e-9). </span>
<span class="n">diff</span> <span class="p">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">gradApprox</span> <span class="o">-</span> <span class="n">DVec</span><span class="p">)</span> <span class="o">/</span> <span class="n">norm</span><span class="p">(</span><span class="n">gradApprox</span> <span class="o">+</span> <span class="n">DVec</span><span class="p">);</span>
</code></pre></div>

<blockquote>
  <p>梯度检查应当在训练神经网络之前，可以通过构造一个新的较小规模的神经网络进行检验；若每次训练都检测梯度，速度很慢。</p>
</blockquote>

<p>注意事项：</p>

<p>不可将$\Theta_{ij}^{(l)}$初始化为$0$，若初始化为$0$，每层的所有神经元都是一样的。随机数初始化$-\epsilon\leq\Theta_{ij}^{(l)}\leq\epsilon$，选择$\epsilon$的有效策略是根据每层神经元的数目取$\epsilon=\frac{\sqrt{6}}{\sqrt{L_{in} + L_{out}}}~(L_{in} = s_l,L_{out}=s_{l+1})$，例如：</p>

<div class="highlight"><pre><code class="matlab"><span class="n">Theta1</span> <span class="p">=</span>  <span class="nb">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">INIT_EPSILON</span><span class="p">)</span> <span class="o">-</span> <span class="n">INIT_EPSILON</span><span class="p">;</span>
<span class="n">Theta2</span> <span class="p">=</span>  <span class="nb">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">INIT_EPSILON</span><span class="p">)</span> <span class="o">-</span> <span class="n">INIT_EPSILON</span><span class="p">;</span>
</code></pre></div>

<h2 id="section-3">思考问题</h2>

<h2 id="section-4">参考资料</h2>

<p><a href="https://class.coursera.org/ml-007">Machine Learning （Andrew Ng）</a>  </p>
]]></content:encoded>
    </item>
    
    <item>
      <title>机器学习: 感知器算法</title>
      <link href="http://jiyeqian.bitbucket.org/2014/10/machine-learning-perceptron-learning-algorithm"/>
      <pubDate>2014-10-21T10:09:00Z</pubDate>
      <author>Jiye Qian</author>
      <guid>http://jiyeqian.bitbucket.org/2014/10/machine-learning-perceptron-learning-algorithm</guid>
      <content:encoded><![CDATA[<h2 id="section">问题描述</h2>

<p>对于线性二分类问题<br />
\begin{equation}
y = \left\{
\begin{aligned}
&amp; +1 &amp; \sum\nolimits_{i=1}^dw_ix_i &gt; \mbox{threshold} \\
&amp; -1 &amp; \sum\nolimits_{i=1}^dw_ix_i &lt; \mbox{threshold}
\end{aligned}
\right. 
\end{equation}</p>

<p>也可以记为
\begin{equation}
h(x) = \mbox{sign}\left(\sum_{i=1}^dw_ix_i - \mbox{threshold}\right) = \mbox{sign}\left(\sum_{i=0}^dw_ix_i\right) = \mbox{sign}\left(\mathbf{w^Tx}\right)
\end{equation} <br />
其中，$w_0 = -\mbox{threshold}, x_0 = +1$。</p>

<p>问：如何估计模型模型$h(x)$的参数$\mathbf{w}$？  <br />
答：感知器算法（PLA，Perceptron Learning Algorithm）。</p>

<h2 id="section-1">感知器算法</h2>

<p>感知器算法通过迭代更新模型参数，直到没有错分的样本点。  </p>

<blockquote>
  <h4 id="pla">PLA</h4>
  <hr />
  <p>repeat until a full cycle of not encountering mistakes { <br />
1. 找到参数$\mathbf w_t$时对应错分的样本点$\left(\mathbf x_{n(t)}, y_{n(t)}\right)$，$\mbox{sign}\left(\mathbf w^T \mathbf x_{n(t)}\right) \neq y_{n(t)}$；     <br />
2. 修正参数，$\mathbf w_{t+1}\leftarrow \mathbf w_t + y_{n(t)}\mathbf x_{n(t)}$。  <br />
}</p>
</blockquote>

<p><img src="../../../assets/images/2014-10-21-pla_1.png" alt="PLA对参数更新示例" />   <br />
PLA更新模型参数可以用上图解释： <br />
1. 当$y=+1$时，若分错，$\mathbf w^T \mathbf x &lt; 0$，表示$\mathbf{w}$和$\mathbf x$的夹角太大（大于90度），需要调整$\mathbf w$，使其与$\mathbf x$的夹角变小；<br />
2. 当$y=-1$时，若分错，$\mathbf w^T \mathbf x &gt; 0$，表示$\mathbf{w}$和$\mathbf x$的夹角太小（小于90度），需要调整$\mathbf w$，使其与$\mathbf x$的夹角变大。   </p>

<p>通过参数更新规则可知  <br />
\[
\mathbf w_{t+1} ＝ \mathbf w_t + y_{n}\mathbf x_{n}
\]<br />
两边转置后同时乘上$y_n\mathbf x_n$，可得
\[
y_n\mathbf w^T_{t+1} \mathbf x_n \geq y_n\mathbf w^T_t\mathbf x_n
\]
这表明，参数更新始终在试图纠正模型参数。  </p>

<p><strong>注意事项</strong>：$\mathbf w$是决策界（判别面）的法向量。</p>

<h2 id="section-2">理论分析</h2>

<blockquote>
  <ol>
    <li>PLA算法会终止么？</li>
    <li>能否从候选模式$h$中学习到目标模式$f$？</li>
  </ol>
</blockquote>

<p>如果数据集$\mathcal D$线性可分，存在一个完美$\mathbf w_f$，使得对数据集中所有样本点$y_n = \mbox{sign}\left(\mathbf w_f^T\mathbf x_n\right)$，对于任意一个样本点总有</p>

<p>\begin{equation}
y_{n(t)}\mathbf w_f^T\mathbf x_{n(t)} \geq \min_n y_n\mathbf x_f^T\mathbf x_n &gt; 0
\end{equation}</p>

<p>根据感知器算法的更新规则可知
\begin{equation}
\begin{aligned}
\mathbf w_f^T\mathbf w_{t+1} 
&amp; =  \mathbf w_f^T\left(\mathbf w_t + y_{n(t)}\mathbf x_{n(t)}\right) \\
&amp; \geq \mathbf w_f^T\mathbf w_t + \min_n y_n\mathbf x_f^T\mathbf x_n \\
&amp; &gt; \mathbf w_f^T\mathbf w_t 
\end{aligned}
\end{equation}
由此可见，参数更新后，$\mathbf w_{t+1}$<em>可能会</em>更加接近$\mathbf w_f$。但是，还不能确定是由于向量间夹角变小还是$\mathbf w_{t+1}$长度变大导致的内积增加。
\begin{equation}
\begin{aligned}
\|\mathbf w_{t+1}\|^2
&amp; = \|\mathbf w_t + y_{n(t)}\mathbf x_{n(t)}\|^2 \\
&amp; = \|\mathbf w_t\|^2 + \|y_{n(t)}\mathbf x_{n(t)}\|^2 + 2y_{n(t)}\mathbf w_t^T\mathbf x_{n(t)} \\
&amp; \leq \|\mathbf w_t\|^2 + \|y_{n(t)}\mathbf x_{n(t)}\|^2 \\
&amp; \leq \|\mathbf w_t\|^2 + \max_n\|\mathbf x_{n}\|^2
\end{aligned}
\end{equation}
由此可见，每次更新的时候，向量长度的增加是有限的，最多增加样本最长向量的长度。</p>

<p>事实上，从$\mathbf w_0$开始，经过$T$次迭代后有
\begin{equation}
\frac{\mathbf w_f^T}{\|\mathbf w_f\|}\frac{\mathbf w_T}{\|\mathbf w_T\|}\geq
\sqrt T \times \mbox{constant}
\label{eq:need_to_be_done_1}
\end{equation}
PLA算法迭代次数的上界满足$T\leq R^2/\rho^2$，其中
\begin{equation}
R^2 = \max_n \|\mathbf x_n\|^2,~~\rho=\min_n y_n\frac{\mathbf w_f^T}{\|\mathbf w_f\|}\mathbf x_n
\label{eq:need_to_be_done_2}
\end{equation}</p>

<p>相关的证明如下：  <br />
<img src="../../../assets/images/2014-10-21-pla_2.jpg" alt="相关的证明" /></p>

<p><strong>结论：</strong>     <br />
1. 对于线性可分的样本集合，通过PLA修正模型参数，$\mathbf w_f$和$\mathbf w_t$的内积增长快，$\mathbf w_t$的长度增长慢，$\mathbf w_t$越来越靠近$\mathbf w_f$，最终算最终收敛； <br />
2. 对于未知的样本集，作用在其上的PLA算法可能长时间不收敛，导致这样的情况可能是迭代次数不够（理论上的参数$T$由于目标模型$f$未知而难以估计）或者样本集存在噪声（线性不可分）。</p>

<h2 id="pocket-pla">Pocket PLA</h2>

<p>判断样本集是否线性可分的复杂度为NP-hard。实际上，通常样本集存在一些噪声，不可能严格的线性可分，PLA算法无法满足收敛条件。Pocket PLA通过改变PLA迭代结束的条件和参数更新规则仍然可以估计模型参数。</p>

<blockquote>
  <h4 id="pocket-pla-1">Pocket PLA</h4>
  <hr />
  <p>repeat until enough iterations {<br />
1. 随机抽取参数$\mathbf w_t$时对应错分的样本点$\left(\mathbf x_{n(t)}, y_{n(t)}\right)$，$\mbox{sign}\left(\mathbf w^T \mathbf x_{n(t)}\right) \neq y_{n(t)}$；     <br />
2. 修正参数，$\mathbf w_{t+1}\leftarrow \mathbf w_t + y_{n(t)}\mathbf x_{n(t)}$；<br />
3. 如果$\mathbf w_{t+1}$在样本集上的表现优于$\hat{\mathbf w}$，$\hat{\mathbf w}\leftarrow \mathbf w_{t+1}$。 <br />
}</p>
</blockquote>

<p>从第3步可知，Pocket PLA的算法时间复杂度要高于PLA，如果对于线性可分的样本集，Pocket PLA比PLA慢。</p>

<h2 id="section-3">思考问题</h2>

<ol>
  <li>PLA和梯度下降法有无联系？</li>
  <li>在分类性能上PLA求解的线性模型和Logistic回归有何差别？</li>
  <li>如何推导公式\eqref{eq:need_to_be_done_1}\eqref{eq:need_to_be_done_2}？ <a href="https://class.coursera.org/ntumlone-002/forum/thread?thread_id=28">答案</a></li>
</ol>

<h2 id="section-4">參考資料</h2>

<ol>
  <li><a href="https://class.coursera.org/ntumlone-002">機器學習基石(Machine Learning Foundations)</a>    </li>
  <li><a href="http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf">Convergence Proof for the Perceptron Algorithm</a></li>
</ol>

]]></content:encoded>
    </item>
    
    <item>
      <title>机器学习: 正则化</title>
      <link href="http://jiyeqian.bitbucket.org/2014/10/machine-learning-regularization"/>
      <pubDate>2014-10-20T00:00:00+08:00</pubDate>
      <author>Jiye Qian</author>
      <guid>http://jiyeqian.bitbucket.org/2014/10/machine-learning-regularization</guid>
      <content:encoded><![CDATA[<h2 id="section">为什么需要正则化</h2>

<p>过拟合（overfitting）是指模型可以在训练集上表现出色，在新数据上性能却很差。解决过拟合问题的方法：</p>

<ul>
  <li>减少特征数目：手工选择特征、利用模型选择；</li>
  <li>正则化（regularization）：保留所有特征，但是减小$\theta_j$，使特征对预测$y$贡献小。</li>
</ul>

<p>正则化通过在代价函数中加入正则化项限制模型参数，避免过拟合。</p>

<p>线性回归和Logistic回归的代价函数追加的正则化项是$\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2$。</p>

<ul>
  <li>$\lambda$称为正则化参数，增大$\lambda$以减小过拟合；</li>
  <li>$\lambda$过大（$10^{10}$）时，有$\theta_j\approx 0~~(j = 1,2,\ldots,n)$，则$h_\theta(x) = \theta_0$，这会导致欠拟合（underfitting）；</li>
  <li>正则化只作用于$j\ge 1$的非常数项，实际上，正则化常数项对结果影响也不大。</li>
</ul>

<p>bias &amp; variance:</p>

<ul>
  <li>high bias: 欠拟合；</li>
  <li>high variance: 过拟合。</li>
</ul>

<h2 id="section-1">正则化线性回归</h2>

<h3 id="section-2">代价函数</h3>

<p>\begin{equation}
J(\theta) = \frac{1}{2m}\left( \sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)^2 + \lambda\sum_{j=1}^n\theta_j^2 \right)
\end{equation}</p>

<h3 id="section-3">梯度下降法估计参数</h3>

<p>repeat until convergence {
\[
\begin{aligned}
\theta_0 &amp; := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_0^{(i)} \\ <br />
\theta_j &amp; := \theta_j - \alpha\left(\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_j^{(i)} + \frac{\lambda}{m}\theta_j\right)~~~~~~(j = 1, 2, \ldots, n)
\end{aligned}
\]
}</p>

<p>迭代过程可以化为如下形式：
\begin{equation}
\theta_j := \theta_j\left(1 - \alpha\frac{\lambda}{m} \right) - \alpha\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_j^{(i)}~~~~~~(j = 1, 2, \ldots, n)
\end{equation}</p>

<p>通常$1 - \alpha\frac{\lambda}{m} &lt; 1$，与非正则化的梯度下降法比较，$\theta_j$减小更快。</p>

<h3 id="section-4">正规方程估计参数</h3>

<p>\begin{equation}
\theta = \left(X^TX + \lambda
\begin{vmatrix}
0  &amp;   &amp;        &amp; \\
   &amp; 1 &amp;        &amp; \\
   &amp;   &amp; \ddots &amp; \\
   &amp;   &amp;        &amp; 1
\end{vmatrix}
\right)^{-1}X^Ty
\end{equation}</p>

<blockquote>
  <p>可以证明，加入正则化项后矩阵始终可逆。</p>
</blockquote>

<h2 id="logistic">正则化Logistic回归</h2>

<h3 id="section-5">代价函数</h3>

<p>\begin{equation}
J(\theta) = -\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right)\log \left(1-h_\theta\left(x^{(i)}\right)\right)\right) ＋ \frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2
\end{equation}</p>

<h3 id="section-6">梯度下降法估计参数</h3>

<p>repeat until convergence {
\[
\begin{aligned}
\theta_0 &amp; := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_0^{(i)} \\ <br />
\theta_j &amp; := \theta_j - \alpha\left(\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_j^{(i)} + \frac{\lambda}{m}\theta_j\right)~~~~~~(j = 1, 2, \ldots, n)
\end{aligned}
\]
}</p>

<h3 id="matlab">Matlab实现</h3>

<p>第一步：实现Logistic函数</p>

<div class="highlight"><pre><code class="matlab"><span class="k">function</span><span class="w"> </span>g <span class="p">=</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span>z<span class="p">)</span><span class="w"></span>
<span class="n">g</span> <span class="p">=</span> <span class="mf">1.0</span> <span class="o">./</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="nb">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">));</span>
<span class="k">end</span>
</code></pre></div>

<p>第二步：实现代价函数（包含梯度计算）</p>

<div class="highlight"><pre><code class="matlab"><span class="k">function</span><span class="w"> </span>[J, grad] <span class="p">=</span><span class="w"> </span><span class="nf">costFunctionReg</span><span class="p">(</span>theta, X, y, lambda<span class="p">)</span><span class="w"></span>
<span class="n">m</span> <span class="p">=</span> <span class="nb">length</span><span class="p">(</span><span class="n">y</span><span class="p">);</span> <span class="c">% number of training examples</span>

<span class="n">h</span> <span class="p">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="p">);</span>
<span class="n">J</span> <span class="p">=</span> <span class="o">-</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span> <span class="o">.*</span> <span class="nb">log</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">.*</span> <span class="nb">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">))</span> <span class="o">+</span> <span class="c">...</span>
    <span class="n">lambda</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">sum</span><span class="p">(</span><span class="n">theta</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">.^</span> <span class="mi">2</span><span class="p">);</span>
<span class="n">grad</span> <span class="p">=</span> <span class="p">(</span><span class="n">X</span><span class="o">&#39;</span> <span class="o">*</span> <span class="p">(</span><span class="n">h</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">lambda</span> <span class="o">*</span> <span class="p">[</span><span class="mi">0</span><span class="p">;</span> <span class="n">theta</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)])</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>

<span class="k">end</span>
</code></pre></div>

<p>第三步：估计参数</p>

<div class="highlight"><pre><code class="matlab"><span class="n">initial_theta</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">1</span><span class="p">);</span>
<span class="n">lambda</span> <span class="p">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="n">options</span> <span class="p">=</span> <span class="n">optimset</span><span class="p">(</span><span class="s">&#39;GradObj&#39;</span><span class="p">,</span> <span class="s">&#39;on&#39;</span><span class="p">,</span> <span class="s">&#39;MaxIter&#39;</span><span class="p">,</span> <span class="mi">400</span><span class="p">);</span>
<span class="p">[</span><span class="n">theta</span><span class="p">,</span> <span class="n">J</span><span class="p">,</span> <span class="n">exit_flag</span><span class="p">]</span> <span class="p">=</span> <span class="c">...</span>
	<span class="n">fminunc</span><span class="p">(@(</span><span class="n">t</span><span class="p">)(</span><span class="n">costFunctionReg</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lambda</span><span class="p">)),</span> <span class="n">initial_theta</span><span class="p">,</span> <span class="n">options</span><span class="p">);</span>
</code></pre></div>

<h2 id="section-7">思考问题</h2>

<ol>
  <li>如何推导正则化线性回归的正规方程解？</li>
  <li>求解正则化线性回归的正规方程时，为何矩阵始终可逆？</li>
  <li>如何选取合适的$\lambda$？</li>
</ol>

<h2 id="section-8">参考资料</h2>

<p><a href="https://class.coursera.org/ml-007">Machine Learning （Andrew Ng）</a>  </p>
]]></content:encoded>
    </item>
    
    <item>
      <title>机器学习: 多分类问题</title>
      <link href="http://jiyeqian.bitbucket.org/2014/10/machine-learning-multiple-classification"/>
      <pubDate>2014-10-20T00:00:00+08:00</pubDate>
      <author>Jiye Qian</author>
      <guid>http://jiyeqian.bitbucket.org/2014/10/machine-learning-multiple-classification</guid>
      <content:encoded><![CDATA[<h2 id="section">问题描述</h2>

<p>许多分类器主要是为了解决二分类问题，比如Logistic回归和SVM，用1和0两个类别标签表示属于或不属于某个类别（而属于另一个类别）。多分类问题是对两个以上类别标签的数据集分类。常用的方法有：</p>

<ul>
  <li>多个二分类器的组合；</li>
  <li>直接采用多分类器。</li>
</ul>

<h2 id="one-vs-all">one-vs-all</h2>

<p><img src="../../../assets/images/2014-10-20-machine-learning-multiple-classification_1.png" alt="one-vs-all classification" /></p>

<p>one-vs-all也叫one-vs-rest。</p>

<p>可用one-vs-all策略组合Logistic回归解决多分类问题：</p>

<ol>
  <li>针对每一类$i$训练一个Logistic分类器$h_\theta^{(i)}(x)$，这是第$i$类与其它类别的二分类问题；</li>
  <li>新输入$x$所属的类别满足$\max_ih_\theta^{(i)}(x)$（属于概率最大的那个类别）。</li>
</ol>

<p>神经网络是扩展的 one-vs-all 方法。</p>

<h2 id="section-1">参考资料</h2>

<p><a href="https://class.coursera.org/ml-007">Machine Learning （Andrew Ng）</a> </p>
]]></content:encoded>
    </item>
    
    <item>
      <title>机器学习: Logistic回归</title>
      <link href="http://jiyeqian.bitbucket.org/2014/10/machine-learning-logistic-regression"/>
      <pubDate>2014-10-19T00:00:00+08:00</pubDate>
      <author>Jiye Qian</author>
      <guid>http://jiyeqian.bitbucket.org/2014/10/machine-learning-logistic-regression</guid>
      <content:encoded><![CDATA[<h2 id="section">模型介绍</h2>

<p><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png" alt="Logistic function" />    </p>

<div class="caption" id="fig1">图1 Logistic函数</div>

<p>Logistic回归（Logistic Regression）用于解决二分类问题，而非回归问题。Logistic函数将回归问题转化为了分类问题。</p>

<p>Logistic回归模型：
\begin{equation}
h_\theta(x) = g\left(\theta^Tx\right)
\end{equation}
其中
\begin{equation}
g\left(z\right) = \frac{1}{1 + e^{-z}}
\end{equation}</p>

<blockquote>
  <p>上式也称为sigmoid function、logistic function，如<a href="#fig1">图1</a>所示。</p>
</blockquote>

<p>分类问题的判别条件如下：
\begin{equation}
y = \left\{
\begin{aligned}
&amp; 1 &amp; h_\theta(x) \ge 0.5 \\
&amp; 0 &amp; h_\theta(x) &lt; 0.5
\end{aligned}
\right. 
\end{equation}
也就是，若$\theta^Tx \ge 0$，则$y = 1$；若$\theta^Tx &lt; 0$，则$y = 0$。Logistic回归模型可以看作是计算属于类别1的概率
\begin{equation}
h_\theta\left(x\right) = \frac{1}{1 + e^{-\theta^Tx}} = P\left(y=1\big|x; \theta\right)
\end{equation}</p>

<p>因此，对于而分类问题，有
\begin{equation}
P\left(y=0\big|x; \theta\right) ＝ 1 - P\left(y=1\big|x; \theta\right) ＝ 1 - h_\theta\left(x\right)
\end{equation}</p>

<p>$\theta^Tx = 0$称为决策界（Decision Boundary）。决策界可以时线性的，也可以是非线性的。</p>

<p><img src="../../../assets/images/2014-10-19-logistic_regression_2.png" alt="线性决策界" /> </p>

<div class="caption">图2 线性决策界</div>

<p><img src="../../../assets/images/2014-10-19-logistic_regression_3.png" alt="非线性决策界" /></p>

<div class="caption">图3 非线性决策界</div>

<h2 id="section-1">代价函数</h2>

<p>线性回归的代价函数框架为：</p>

<p>\begin{equation}
J(\theta) = \frac{1}{m}\sum_{i=1}^{m}\frac{1}{2}{\left(h_{\theta}\left(x^{(i)}\right) - y^{(i)}\right)^2}
\end{equation}</p>

<p>令</p>

<p>\begin{equation}
Cost\left(h_\theta(x), y\right) = \frac{1}{2}\left( h_\theta(x) - y \right) ^ 2
\end{equation}</p>

<p>当$h_\theta(x)$为Logistic回归模型（<a href="#fig4">图4</a>左）和线性回归模型（<a href="#fig4">图4</a>右）时，分别为非凸函数和凸函数，非凸函数不能用梯度下降法找到全局最优解。因此，Logistic回归需要采用新的代价函数才能用梯度下降法求解。</p>

<p><img src="../../../assets/images/2014-10-19-logistic_regression_1.png" alt="代价函数图" /></p>

<div class="caption" id="fig4">图4 代价函数</div>

<p>Logistic回归采用</p>

<p>\begin{equation}
Cost\left(h_\theta(x), y\right) = \left\{
\begin{aligned}
&amp; -\log\left(h_\theta(x)\right) &amp; y=1 \\
&amp; -\log\left(1-h_\theta(x)\right) &amp; y=0
\end{aligned}
\right. 
\end{equation}</p>

<p>上式等价于</p>

<p>\begin{equation}
Cost\left(h_\theta(x), y\right) = -y\log\left(h_\theta(x)\right) - (1 - y)\log\left(1-h_\theta(x)\right)
\end{equation}</p>

<p>因此，Logistic回归的代价函数为</p>

<p>\begin{equation}
\begin{aligned}
J(\theta)<br />
= &amp; \frac{1}{m}\sum_{i=1}^{m}Cost\left(h_\theta\left(x^{(i)}\right), y^{(i)}\right) \\
= &amp; -\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right)\log \left(1-h_\theta\left(x^{(i)}\right)\right)\right)
\end{aligned}
\label{eq:cost_function_logistic_regression}
\end{equation}</p>

<blockquote>
  <p>若套用线性回归的代价函数，则$J(\theta)$非凸，不利于优化算法。该代价函数可从统计中最大似然估计（maximum likehood estimation）的角度推导。</p>
</blockquote>

<h2 id="section-2">参数估计</h2>

<p>参数估计是通过最小化代价函数\eqref{eq:cost_function_logistic_regression}，求解模型参数$\theta$。</p>

<p>\[
\min_\theta J(\theta)
\]</p>

<h3 id="section-3">梯度下降法</h3>

<p>貌似线性回归的梯度下降法的结构，但注意$h_\theta$的定义不同。</p>

<p>repeat until convergence {
\[
\theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)x_j^{(i)}~~~~~~(j = 0, 1, \ldots, n)
\]
}</p>

<p><strong>注意事项</strong></p>

<ul>
  <li>特征尺度规范化对加速Logistic回归的梯度下降法依然有效；</li>
  <li>其它注意事项同梯度下降法求解线性回归参数。</li>
</ul>

<p><img src="../../../assets/images/2014-10-19-logistic_regression_4.png" alt="采用Matlab的优化框架求解模型参数" /></p>

<div class="caption">图5 采用Matlab的优化框架求解模型参数</div>

<h3 id="section-4">其它算法</h3>

<ul>
  <li>Conjugate gradient</li>
  <li>BFGS</li>
  <li>L-BFGS</li>
</ul>

<blockquote>
  <p>这几种算法不需要手工选择学习率$\alpha$，通常比梯度下降法快，但是更复杂。</p>
</blockquote>

<h2 id="section-5">思考问题</h2>

<ol>
  <li>代价函数如何求偏导？</li>
</ol>

<h2 id="section-6">应用范例</h2>

<h2 id="section-7">参考资料</h2>

<ol>
  <li><a href="https://class.coursera.org/ml-007">Machine Learning （Andrew Ng）</a>  </li>
  <li><a href="http://en.wikipedia.org/wiki/Logistic_regression">Wikipedia: Logistic regression</a></li>
</ol>

]]></content:encoded>
    </item>
    
    <item>
      <title>机器学习: 线性回归</title>
      <link href="http://jiyeqian.bitbucket.org/2014/10/machine-learning-linear-regression"/>
      <pubDate>2014-10-19T00:00:00+08:00</pubDate>
      <author>Jiye Qian</author>
      <guid>http://jiyeqian.bitbucket.org/2014/10/machine-learning-linear-regression</guid>
      <content:encoded><![CDATA[<h2 id="section">模型介绍</h2>

<p><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/438px-Linear_regression.svg.png" alt="linear regression" /></p>

<p>线性回归（Linear Regression）的回归函数（hypothesis）为
\begin{equation}
h_\theta(x) = \theta^Tx = \theta_0x_0 + \theta_1x_1 + \cdots  + \theta_nx_n
\label{eq:hypothesis_linear_regression}
\end{equation}
其中，常数项$x_0 = 1$。</p>

<p>代价函数（cost function）为
\begin{equation}
J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}{\left(h_{\theta}\left(x^{(i)}\right) - y^{(i)}\right)^2}
\label{eq:cost_function_linear_regression}
\end{equation}
其中，$x^{(i)} = \left(x_1^{(i)}, x_2^{(i)}, \cdots, x_n^{(i)}\right)$， $m$为样本数量，$n$为特征数量。</p>

<h2 id="section-1">参数估计</h2>

<p>参数估计是通过最小化代价函数\eqref{eq:cost_function_linear_regression}，求解模型参数$\theta$。
\[
\min_\theta J(\theta)
\]
通常有两种解法：万能的梯度下降法（gradient descent）和正规方程（normal equation）的解析解（也就是线性回归的最小二乘解）。</p>

<h3 id="section-2">梯度下降法</h3>

<p>repeat until convergence {
\[
\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)~~~~~~(j = 0, 1, \ldots, n)
\]
}<br />
也就是<br />
repeat until convergence {
\[
\theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)x_j^{(i)}~~~~~~(j = 0, 1, \ldots, n)
\]
}  <br />
每轮循环的时候同时更新$\theta_j$（下图左边为正确更新方式。<em>不能</em>在同一轮循环中，先更新部分$\theta_j$，再利用已更新的$\theta_j$更新其它$\theta_j$）。  <br />
<img src="../../../assets/images/2014-10-19-linear_regression_1.png" alt="同时更新$\theta_j$" /> <br />
特征的尺度如果相差太大，需要进行尺度规范化提高收敛速度，常用的规范化方法是
\[
\hat x_i = \frac{x_i - x_{mean}}{x_{max}-x_{min}}
\]
或者
\[
\hat x_i = \frac{x_i - x_{mean}}{x_{std}}
\]</p>

<p><strong>注意事项：</strong>   </p>

<ul>
  <li>线性回归的代价函数$J(\theta)$不存在局部极值（local optima），存在全局极值；</li>
  <li>将所有特征归一（feature scaling）到统一的尺度$-1\le x_i\le 1$有助于提高梯度下降法的速度（不归一化$x_0$）；</li>
  <li>学习率$\alpha$太小收敛慢，太大可能错过极值点而不收敛，过大的$\alpha$甚至可能导致$J(\theta)$不降反升；</li>
  <li>在迭代过程中保持$\alpha$不变，梯度下降步长也会自动减小，因为梯度会不断减小。</li>
</ul>

<p>尺度归一化效果： <br />
<img src="../../../assets/images/2014-10-19-linear_regression_2.png" alt="尺度归一化效果示例" /><br />
梯度下降步长自动减小：<br />
<img src="../../../assets/images/2014-10-19-linear_regression_3.png" alt="梯度下降步长自动减小" /></p>

<h3 id="section-3">正规方程求解</h3>

<p>正规方程求解也就是线性回归的最小二乘解为
\begin{equation}
\theta = \left(X^TX\right)^{-1}X^Ty
\end{equation}</p>

<p><strong>注意事项：</strong>     </p>

<p>$X^TX$可能不可逆，导致的原因可能是冗余特征（redundant features）和特征数目过多（$n$太多而$m$太少），解决的办法：   </p>

<ul>
  <li>冗余特征线性相关（e.g. $x_1 = 2x_2$）：删除线性相关特征；</li>
  <li>特征数目过多（e.g. $m \le n$）：删除特征、正则化（regularization）。</li>
</ul>

<p>Matlab Code：</p>

<div class="highlight"><pre><code class="matlab"><span class="n">pinv</span><span class="p">(</span><span class="n">X</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">y</span>
</code></pre></div>

<p>其中，pinv函数可以处理$X^TX$不可逆的情况，而inv函数不行。</p>

<h3 id="section-4">求解方法比较</h3>

<table>
  <thead>
    <tr>
      <th>Gradient Descent</th>
      <th>Normal Equation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>需要$\alpha$</td>
      <td>不需要$\alpha$</td>
    </tr>
    <tr>
      <td>需要迭代</td>
      <td>不需要迭代</td>
    </tr>
    <tr>
      <td>当特征数$n$很大时（$10^6$）工作良好</td>
      <td>$n$很大时很慢</td>
    </tr>
    <tr>
      <td>特征需要尺度规范化</td>
      <td>特征不需要尺度规范化</td>
    </tr>
  </tbody>
</table>

<h2 id="section-5">多项式回归</h2>

<p>当$x_1 = x, x_2 = x^2, x_3 = x^3, \ldots $时，多项式回归（polynomial regression）\eqref{eq:hypothesis_polynomial_regression}可以直接转化为线性模型\eqref{eq:hypothesis_linear_regression}。
\begin{equation}
h_\theta(x) = \theta^Tx = \theta_0x_0 + \theta_1x + \theta_2x^2 ＋ \theta_3x^3 \cdots  + \theta_nx^n
\label{eq:hypothesis_polynomial_regression}
\end{equation}</p>

<p><strong>注意事项：</strong> <br />
构造多项式特征，借助多项式回归，可以利用线性回归模型解决非线性问题。</p>

<h2 id="section-6">思考问题</h2>

<ol>
  <li>如何通过收敛判断梯度下降法是否应该停止迭代？</li>
  <li>特征尺度规范化（feature scaling）为何能提高梯度下降法收敛速度？</li>
  <li>为什么正规方程估计参数不需要特征尺度规范化？</li>
</ol>

<h2 id="section-7">应用范例</h2>

<h2 id="section-8">参考资料</h2>

<p><a href="https://class.coursera.org/ml-007">Machine Learning （Andrew Ng）</a>    <br />
<a href="http://en.wikipedia.org/wiki/Linear_regression">Wikipedia: Linear regression</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Introduction to DSP Development</title>
      <link href="http://jiyeqian.bitbucket.org/2014/03/introduction-to-dsp-development"/>
      <pubDate>2014-03-28T00:00:00+08:00</pubDate>
      <author>Jiye Qian</author>
      <guid>http://jiyeqian.bitbucket.org/2014/03/introduction-to-dsp-development</guid>
      <content:encoded><![CDATA[<h2 id="section">基本概念</h2>

<p>DaVinci: The Texas Instruments DaVinci is a system on a chip family that combines a DSP core based on the TMS320 C6000 VLIW DSP family with an ARM architecture CPU core.</p>

<p>PSP: Platform Support Package</p>

<p>MAR:  Memory Attribute Register</p>

<p>EVM: EValuation Module.</p>

<p>DVEVM: Digital Video Evaluation Module.</p>

<p>DMAI: Davinci Multimedia Application Interface.</p>

<p>DVTB: Digital Video Test Bench.</p>

<p><a href="http://rtsc.eclipse.org/docs-tip/RTSC_Packaging_Primer" title="RTSC Packaging Primer">RTSC</a>: Real-Time Software Components. 实时软件组件。</p>

<p>XDCtools: XDCtools is the actual product that contains all the tools necessary for you to use the SYS/BIOS components and configure your application. XDCtools provides configuration tools you use to create and build a static configuration as part of your application. This *.cfg configuration file specifies:        </p>

<ul>
  <li>Which modules from XDCtools, SYS/BIOS, and other components to include in the runtime image.      </li>
  <li>What static instances of RTOS objects to create. For example, these include tasks and semaphores.        </li>
  <li>Settings for parameter values for modules and objects.      </li>
  <li>XDCtools provides critical APIs that are used by SYS/BIOS and other related software components. These include memory allocation, logging, and system control.</li>
</ul>

<p><img src="http://processors.wiki.ti.com/images/9/92/Config_flow1.png" alt="XDCTools" /></p>

<p><a href="http://www.ti.com/pdfs/wtbu/ti_hlos.pdf" title="Telephony software: the hidden glue in an HLOS-based smartphone">HLOS</a>: High-Level Operating System.</p>

<p>RTOS: Real-Time Operating System. </p>

<p>XDC: eXpress DSP Components</p>

<p>GPP: General-Purpose Processors.</p>

<p>DSP/BIOS:</p>

<p>SYS/BIOS: </p>

<p>DSP/BIOS™ LINK(<strong>END OF LIFE</strong>): 用于GPP与DSP通信。</p>

<p><a href="http://processors.wiki.ti.com/index.php/SysLink_UserGuide" title="SysLink UserGuide">SysLink</a>: The SysLink product provides software connectivity between multiple processors. SysLink is the next generation of DSP Link. The SysLink product is used in conjunction with the IPC product. <a href="http://omappedia.org/wiki/Syslink_Project" title="Syslink Project">SysLink is the next generation IPC driver developed for OMAP4 and beyond</a>. <a href="http://processors.wiki.ti.com/index.php/SysLink_FAQs" title="SysLink FAQs">The SysLink and IPC product are complementary</a>. SysLink comprises of the following sub-components:</p>

<ul>
  <li>System Manager        </li>
  <li>Processor Manager        </li>
  <li>Inter-Processor Communication (IPC)       </li>
  <li>Utility modules  </li>
</ul>

<p>The common header files for IPC implemented by both the IPC product (on RTOS-side) and the SysLink product (on HLOS-side) are available within ti/ipc package.
The SysLink product, in addition to implementing the HLOS-side of all IPC modules available within the RTOS IPC product, adds the following: ProcMgr, FrameQ (HLOS &amp; RTOS), RingIO (HLOS &amp; RTOS).</p>

<p>Some of the key features of SysLink are:    </p>

<ul>
  <li>Messaging: Ability to exchange fixed size control messages with Co-Processors
Dynamic memory management: Ability to dynamically map files to Co-Processor’s address space</li>
  <li>Dynamic loading: Ability to dynamically load new components on Co-Processors at run time</li>
  <li>Power Management: Static and dynamic power management for DSP</li>
  <li>Zero-copy shared memory: Ability to “pass” data buffers to other processors by simply providing its location in shared memory.</li>
  <li>TILER-based memory allocation: Allocate 2-D buffers with mirroring and rotation options, suitable for video</li>
  <li>Remote function calls: One processor can invoke functions on a remote processor.</li>
</ul>

<p>SysLink Architecture：
<img src="http://omappedia.org/images/3/38/SysLink_architecture.png" alt="SysLink Architecture" /></p>

<p>SysLink Usage：
<img src="http://omappedia.org/images/c/cb/RCM_usage.png" alt="SysLink Usage" /></p>

<p>实现ARM和DSP之间通信的底层软件，Codec Engine就是建立在这个底层软件之上。</p>

<p><em>Note, SysLink 2.x is intended for SYS/BIOS 6 users and does not support WinCE. DSP/BIOS 5 users (including most DVSDK users) should continue to use DSP Link releases!</em> </p>

<p><a href="http://software-dl.ti.com/dsps/dsps_public_sw/sdo_sb/targetcontent/ipc/index.html" title="IPC Product Releases">IPC</a>: Inter-Processer Communication. IPC只能在运行SYS/BIOS的CPU上使用。</p>

<p><em>Note, IPC 3.x is incompatible with SysLink. SysLink and most EZSDK users should continue to use IPC 1.x releases!</em></p>

<p>EZSDK:</p>

<p>DVSDK: Digital Video Software Development Kit.</p>

<p>RDK:
<img src="http://www.deyisupport.com/cfs-file.ashx/__key/communityserver-discussions-components-files/39/6087.RDK.jpg" alt="RDK框架图" /></p>

<p>IPNC RDK: The IPNC RDK is a Reference Design Kit for TI812x platform and is optimized for IPNC multi-resolution applications. RDK allows a user to create different multi-resolution data flows involving video camera, capture, video processing (VS, Noise Filter, Encode, LDC etc) and video display. The IPNC-RDK is implemented using TI’s Multi-Channel “Link” FrameWork (McFW).<br />
NDK: Network Developers Kit.</p>

<p>GLSDK:</p>

<p><a href="http://processors.wiki.ti.com/index.php/MCSDK_Image_Processing_Demonstration_Guide" title="Multicore Software Development Kit Image Processing Demonstration Guide">MCSDK</a>: MultiCore Software Development Kit.</p>

<p>AVSDK:</p>

<p>QNX SDK:</p>

<p>MFP: Multimedia Framework Products.</p>

<p>McFW: Multi-Channel “Link” FrameWork</p>

<p><a href="http://en.wikipedia.org/wiki/XDAIS_algorithms" title="XDAIS algorithms">xDAIS</a>: eXpress DSP Algorithm Interoperability Standard. The standard was first introduced in 1999 and was created to facilitate integration of DSP algorithms into systems without re-engineering cost. The XDAIS standard address the issues of algorithm resource allocation and consumption on a DSP. Algorithms that comply with the standard are tested and awarded an “eXpressDSP-compliant” mark upon successful completion of the test. For instance, all XDAIS compliant algorithms must implement an Algorithm Interface, called IALG. For those algorithms utilizing DMA, the IDMA interface must be implemented. Further, specific rules are provided for each family of TI DSP. DSP算法工程师要写出能被ARM通过Codec Engine调用的算法，必须保证自己的算法接口符合这个标准。</p>

<p>xDM: XDAIS-DM(Digital Media). An extension of XDAIS, a superset of the XDAIS standard.</p>

<p>IALG Interface is a set of standard interface functions exported by an XDAIS algorithm.</p>

<p><a href="http://software-dl.ti.com/dsps/dsps_public_sw/sdo_sb/targetcontent/fc/index.html" title="Framework Components Product Releases">FC</a>: Framework Components. FC is comprised of XDAIS algorithm resource managers and functional interfaces, which provides libraries/APIs to grant resources (e.g. memory and DMA resources) to algorithms using the interfaces (IALG, IDMA3) defined in XDAIS.     </p>

<ul>
  <li>DSKT2 - used for creating and interacting with XDAIS algorithms.    </li>
  <li>RMAN - General purpose resource manager (Introduced in FC 2.00)      </li>
  <li>ECPY - DMA-based memory copy functional library (Introduced in FC 3.20)      </li>
  <li>DMAN3 - used for sharing QDMA channels from an EDMA3 device with XDAIS algorithms. <em>(Deprecated as of FC 3.23)</em>       </li>
  <li>ACPY3 - DMA-based memory copy functional library. <em>(Deprecated as of FC 3.23)</em> </li>
</ul>

<p><em>Note, FC 3.x is intended for SYS/BIOS 6 users. DSP/BIOS 5 users (including most DVSDK users) should continue to use FC 2.x releases!</em>     </p>

<p><a href="http://processors.wiki.ti.com/index.php/Codec_Engine" title="Codec Engine">CE</a>: Codec Engine. Codec Engine is a set of APIs that you use to instantiate and run XDAIS algorithms. A VISA interface is provided as well for interacting with XDM-compliant XDAIS algorithms.</p>

<p><em>Note, CE 3.x is intended for SYS/BIOS 6 users and does not yet support WinCE. WinCE and DSP/BIOS 5 users (including most DVSDK users) should continue to use CE 2.x releases!</em></p>

<p><a href="http://processors.wiki.ti.com/index.php/Codec_Engine_Overview" title="Codec Engine Overview">DVSDK, CE, FC and xDAIS</a>: </p>

<ul>
  <li>[Bottom-up] Start with the XDAIS and XDM products, which define a consistent set of algorithm interfaces, and enable the software stack above. Stepping up the stack, there is Framework Components (FC). FC provides libraries/APIs to grant resources (e.g. memory and DMA resources) to algorithms using the interfaces (IALG, IDMA3) defined in XDAIS. Then, we step up into Codec Engine (CE). CE enables applications to easily invoke these algorithms (using FC under the covers). And finally, the big one, the DVSDK. This bundles all of the above and more, and provides examples of fully running applications - real codecs, real I/O, etc.</li>
  <li>[Top-down] A core component of the DVSDK is the Codec Engine (CE). Looking further down the stack is Framework Components (FC). And finally, FC fundamentally operates on algorithms which comply with the XDAIS and XDM. </li>
</ul>

<p><a href="http://processors.wiki.ti.com/index.php/How_is_SYS/BIOS_related_to_XDCtools_and_RTSC%3F" title="How is SYS/BIOS related to XDCtools and RTSC?">How is SYS/BIOS related to XDCtools and RTSC?</a></p>

<p><a href="http://processors.wiki.ti.com/index.php/Category:UIA" title="Category:UIA">UIA</a>: Unified Instrumentation Architecture. UIA defines a set of tools, APIs, transports, interfaces and guidelines that enable developers to instrument embedded software in a way that is portable and flexible, and enables the creation of advanced tooling features that can be used both in the lab and in the field.</p>

<p><a href="http://processors.wiki.ti.com/index.php/Multicore_System_Analyzer" title="Multicore System Analyzer">MCSA</a>: Multicore System Analyzer. System Analyzer is a real-time tool for analyzing, visualizing and profiling application running on single or multi core systems.</p>

<p><em>Multicore System Analyzer (MCSA) has recently been renamed to “System Analyzer”.</em>  </p>

<p>VPSS: </p>

<p>DMVAL: Digital Media Video Analytics Library</p>

<p>SPIs: System Programming Interfaces.</p>

<p>MCASP: Multi-Channel Audio Seria Port.</p>

<p>MCBSP: Multi-Channel Buffered Seria Port.</p>

<p>HDVPSS: High-Definition Video Process Sub System. 主要用于视频的capture、deinterlacing、scaler、up/down sample、graphics、display等，由Media Controller Dula ARM Cortex-M3系统中的VPSS-M3控制。</p>

<p>HDVICP2: High-Definition Video Image Coprocesser模块，是一个硬核编解码加速器，主要用于视频编解码，支持H.264、MPEG、MJPEG、AVS等协议，由Media Controller Dula ARM Cortex-M3系统中的VIDEO-M3控制。</p>

<p>ISS: Image Sub System，是IPNC类产品中的视频前端接口，主要用于前端Sensor 的Capture、resize、scaler、H3A等，也是由Media Controller Dula ARM Cortex-M3系统中的VPSS-M3控制（更正：此处根据代码猜测ISS既可以由VIDEO-M3控制也可以由VPSS-M3控制，理由是：在IPNC的RDK中ISS是由VPSS-M3控制，但在DVR RDK中ISS是由VIDEO来控制的。从TI对M3功能的规划来说，更倾向于VPSS-M3控制，理由：VPSS-M3主要负责采集显示等功能，而VIDEO侧重于编解码控制）。</p>

<p>OSAL: Operating System Abstraction Library.</p>

<p>OCMC: On-Chip Memory Controller</p>

<h2 id="ti-">TI 芯片系列</h2>

<p><img src="http://www.deyisupport.com/resized-image.ashx/__size/550x0/__key/CommunityServer-Discussions-Components-Files/39/7002.36.bmp" alt="DM8148命名规则" />
硅版本号在做引导程序的时候可能会用到，需注意配置要求。</p>

<h2 id="sysbios">SYS/BIOS</h2>

<h3 id="cachemar">Cache_MAR###_###设置</h3>

<ul>
  <li>功能：通过设置，使得外存（external memory）地址（对应的存储区）是可缓存的（cacheable）<sup id="fnref:sysbios1"><a href="#fn:sysbios1" class="footnote">1</a></sup>。</li>
  <li>原理：Cache.MAR128_159可设置的地址范围是0x80000000 - 0x9FFFFFFF，该地址对应存储区是512M（0x9FFFFFFF-0x80000000+0x01=512M），Cache.MAR128_159的类型是32位的UInt32（159-128+1=32），32bit掩码，每bit可设置存储区大小16M（512/32＝16）<sup id="fnref:sysbios2"><a href="#fn:sysbios2" class="footnote">2</a></sup>。</li>
  <li>实例：当Cache.MAR128_159＝0x1时，表示0x80000000开始的16M存储区可被缓存（MAR的低位对应低地址0x80000000还是高地址0x9FFFFFFF有待考证？？）。<a href="http://blog.csdn.net/emilyhuping/article/details/8602550" title="TI dsp的memory映射cache">每个F代表64M可缓存的区域，Cache.MAR128_159=0x000000FF代表从0x80000000开始的128M区域可被缓存</a>（64 * 2 = 128）。</li>
</ul>

<h2 id="linux-utilsplinuxutils"><a href="http://software-dl.ti.com/dsps/dsps_public_sw/sdo_sb/targetcontent/linuxutils/index.html" title="Linux Utils Overview">Linux Utils</a></h2>

<h3 id="cmempcmem1"><a href="http://processors.wiki.ti.com/index.php/CMEM_Overview" title="CMEM Overview">CMEM</a></h3>

<p>CMEM是确保能分配到<em>连续物理内存</em>的一组API，同时提供虚拟地址到物理地址的转换，以及用户模式的高速缓存（cache）管理。连续物理内存用于多核之间（DSP、ARM）的共享缓存（buffer），以及硬件加速器、DMA。</p>

<p>在操纵连续物理内存之前，需要安装内核模块<code>cmemk.ko</code>初始化可连续分配物理内存的区域：</p>

<div class="highlight"><pre><code class="bash">/sbin/insmod cmemk.ko <span class="nv">pools</span><span class="o">=</span>4x30000,2x500000 <span class="nv">phys_start</span><span class="o">=</span>0x0 <span class="nv">phys_end</span><span class="o">=</span>0x3000000
</code></pre></div>

<ul>
  <li>phys_start和phys_end必须是16进制，pools必须是10进制（会用到x，不能用16进制）。</li>
  <li>上例作用：初始化两个内存池（pool），pool的id分别为0、1，第一个是4个30000字节的buffer，第二个是2个500000字节的buffer，所有buffer的起始地址位于0x0~0x3000000的区域。</li>
  <li>由于存在对齐限定，每个pool占用的空间会大于或等于内核模块命令指定的空间（Pool buffers are aligned on a module-dependent boundary, and their sizes are rounded up to this same boundary. This applies to each buffer within a pool. ）。</li>
  <li>可通过命令<code>cat /proc/cmem</code>查看CMEM管理pool和buffer的状态。</li>
</ul>

<p>CMEM 2.00加入了对分配堆内存（heap）的支持，堆内存的边界对齐依赖于模块实现和用户设定（用户设定大于模块定义边界时，用户设定才有效）。可用堆内存的总大小是指定物理地址区域内存减去内存池占用内存的大小（phys_end - phys_start + 1 - size_of_pools）。使用堆内存的缺点是比内存池容易产生碎片。通过几轮分配释放过程，内存碎片问题可能导致再也无法成功分配内存，这通常导致codec creation failure。在开发测试阶段推荐使用堆内存，在产品实现阶段推荐使用内存池。</p>

<p>CMEM 2.23 增加了程序退出时清理（clean up）未释放内存（buffer）的功能。</p>

<p><code>cmemk.ko</code>的其他参数<sup id="fnref:CMEM1"><a href="#fn:CMEM1" class="footnote">3</a></sup>：</p>

<ul>
  <li><code>useHeapIfPoolUnavailable=[0|1]</code>：when set to 1, will cause pool-based allocations to fallback to a heap-based allocation if no pool buffer of sufficient size is available.</li>
  <li><code>allowOverlap=[0|1]</code>： when set to 1, causes cmemk.ko to not fail when it detects that a CMEM memory block location conflicts with the Linux kernel memory, and instead an informational message is printed on the console. </li>
</ul>

<p>内存分配函数<sup id="fnref:CMEM1:1"><a href="#fn:CMEM1" class="footnote">3</a></sup>：</p>

<div class="highlight"><pre><code class="c"><span class="kt">void</span><span class="o">*</span> <span class="n">CMEM_alloc</span><span class="p">(</span><span class="kt">size_t</span> <span class="n">size</span><span class="p">,</span> <span class="n">CMEM_AllocParams</span> <span class="o">*</span><span class="n">params</span><span class="p">)</span>
</code></pre></div>

<ul>
  <li><code>size</code>：分配buffer的大小。</li>
  <li><code>params</code>：CMEM_AllocParams有三个元素。<code>type</code>：CMEM_HEAP, CMEM_POOL, or CMEM_CMA；<code>flags</code>：CMEM_CACHED or CMEM_NONCACHED；<code>alignment</code>：only used for heap allocations, must be power of 2。</li>
  <li>之前必须先调用<code>CMEM_init()</code>。</li>
</ul>

<p>Pool allocations can be requested explicitly by pool number, or more generally by just a size. For size-based allocations, the pool which best fits the requested size is automatically chosen. </p>

<h2 id="ipc">IPC</h2>

<h3 id="notify-moduletisdoipcnotify-module-ipc1">Notify module/ti.sdo.ipc.Notify module <sup id="fnref:IPC1"><a href="#fn:IPC1" class="footnote">4</a></sup></h3>

<p>The ti.sdo.ipc.Notify module manages the multiplexing/demultiplexing of software interrupts over hardware interrupts.</p>

<p>调用任何Notify的API之前，必须先调用<code>Ipc_start()</code>函数。如果<code>Ipc.setupNotify</code>设为<code>FALSE</code>，须在<code>Ipc_start()</code>之外调用<code>Notify_start()</code>。</p>

<p>为了接收消息，每个处理器要通过<code>Notify_registerEvent()</code>注册一个或多个回调函数到eventId，回调函数的格式：</p>

<div class="highlight"><pre><code class="c"><span class="n">Void</span> <span class="nf">cbFxn</span><span class="p">(</span><span class="n">UInt16</span> <span class="n">procId</span><span class="p">,</span> <span class="n">UInt16</span> <span class="n">lineId</span><span class="p">,</span> <span class="n">UInt32</span> <span class="n">eventId</span><span class="p">,</span> <span class="n">UArg</span> <span class="n">arg</span><span class="p">,</span> <span class="n">UInt32</span> <span class="n">payload</span><span class="p">);</span>
</code></pre></div>

<p>The <code>Notify_registerEvent()</code> function (like most other Notify APIs) uses a ti.sdo.utils.MultiProc ID and line ID to target a specific interrupt line to/from a specific processor on a device.</p>

<div class="highlight"><pre><code class="c"><span class="n">Int</span> <span class="n">status</span><span class="p">;</span>
<span class="n">armProcId</span> <span class="o">=</span> <span class="n">MultiProc_getId</span><span class="p">(</span><span class="s">&quot;ARM&quot;</span><span class="p">);</span>
<span class="n">Ipc_start</span><span class="p">();</span>
<span class="cm">/* Register cbFxn with Notify. It will be called when ARM</span>
<span class="cm"> * sends event number EVENTID to line #0 on this processor.</span>
<span class="cm"> * The argument 0x1010 is passed to the callback function. */</span>
<span class="n">status</span> <span class="o">=</span> <span class="n">Notify_registerEvent</span><span class="p">(</span><span class="n">armProcId</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">EVENTID</span><span class="p">,</span> <span class="p">(</span><span class="n">Notify_FnNotifyCbck</span><span class="p">)</span><span class="n">cbFxn</span><span class="p">,</span> <span class="mh">0x1010</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">status</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">System_abort</span><span class="p">(</span><span class="s">&quot;Notify_registerEvent failed</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div>

<p>The line ID number is typically 0 (zero), but is provided for use on systems that have multiple interrupt lines between processors.</p>

<p><code>Notify_registerEvent()</code>可将多个回调函数注册到一个事件（event）。每个事件只注册一个回调函数时，调用<code>Notify_registerEventSingle()</code>可获得更好的性能。一旦事件被注册，远程处理器可通过<code>Notify_sendEvent()</code>发送事件。If the specified event and interrupt line are both enabled, all callback functions registered to the event will be called sequentially.</p>

<div class="highlight"><pre><code class="c"><span class="k">while</span> <span class="p">(</span><span class="n">seq</span> <span class="o">&lt;</span> <span class="n">NUMLOOPS</span><span class="p">)</span> <span class="p">{</span>
<span class="n">Semaphore_pend</span><span class="p">(</span><span class="n">semHandle</span><span class="p">,</span> <span class="n">BIOS_WAIT_FOREVER</span><span class="p">);</span>
<span class="cm">/* Semaphore_post is called by callback function*/</span>
<span class="n">status</span> <span class="o">=</span> <span class="n">Notify_sendEvent</span><span class="p">(</span><span class="n">armProcId</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">EVENTID</span><span class="p">,</span> <span class="n">seq</span><span class="p">,</span> <span class="n">TRUE</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div>

<p>上例中，<code>seq</code>作为回调函数的参数<code>payload</code>发送（执行回调函数时，<code>payload = seq</code>）。<code>payload</code>固定为32个bit长。<code>Notify_sendEvent()</code>的第5个参数设为<code>TRUE</code>， Notify driver会等待之前发送给相同<code>even ID</code>事件被收到的响应（acknowledgement）。</p>

<h2 id="xdc">XDC</h2>

<h3 id="xdcmakefilexdc1">XDC的Makefile示例<sup id="fnref:XDC1"><a href="#fn:XDC1" class="footnote">5</a></sup></h3>

<div class="highlight"><pre><code class="makefile"><span class="nv">CGTOOLS</span> <span class="o">=</span> C:/CCStudio_v3.3/C6000/cgtools

<span class="nv">CC</span> <span class="o">=</span> <span class="k">$(</span>CGTOOLS<span class="k">)</span>/bin/cl6x
<span class="nv">LNK</span> <span class="o">=</span> <span class="k">$(</span>CGTOOLS<span class="k">)</span>/bin/lnk6x
<span class="nv">RTS</span> <span class="o">=</span> <span class="k">$(</span>CGTOOLS<span class="k">)</span>/lib/rts6400.lib

<span class="nv">CONFIG</span> <span class="o">=</span> mycfg
<span class="nv">XDCTARGET</span> <span class="o">=</span> ti.targets.C64
<span class="nv">XDCPLATFORM</span> <span class="o">=</span> ti.platforms.sim6xxx

<span class="nf">%/linker.cmd %/compiler.opt </span><span class="o">:</span> <span class="m">%.cfg</span>
	xs xdc.tools.configuro -c <span class="k">$(</span>CGTOOLS<span class="k">)</span> -t <span class="k">$(</span>XDCTARGET<span class="k">)</span> -p <span class="k">$(</span>XDCPLATFORM<span class="k">)</span> <span class="nv">$&lt;</span>

<span class="nf">%.obj </span><span class="o">:</span> <span class="m">%.c $(CONFIG)/compiler.opt</span>
	<span class="k">$(</span>CC<span class="k">)</span> -@<span class="k">$(</span>CONFIG<span class="k">)</span>/compiler.opt -c <span class="nv">$&lt;</span>

<span class="nf">hello.out </span><span class="o">:</span> <span class="m">hello.obj $(CONFIG)/linker.cmd</span>
	<span class="k">$(</span>LNK<span class="k">)</span> -o hello.out -c hello.obj <span class="k">$(</span>CONFIG<span class="k">)</span>/linker.cmd <span class="k">$(</span>RTS<span class="k">)</span>
</code></pre></div>

<h3 id="xdc-1">XDC的配置</h3>

<p>Tconf was the precursor to XDC. The configuration file uses simple JavaScript (EcmaScript) syntax. For more information about configuration, see the XDC Consumer User’s Guide (SPRUEX4).</p>

<p>查看帮助文档：</p>

<div class="highlight"><pre><code class="bash">xs xdc.tools.cdoc.sg
</code></pre></div>

<p><code>config.bld</code> 中记录了系统的内存等资源的分配，分配方法结合芯片的datasheet（例如tms320dm8127）中Memory Map Summary章节。</p>

<p>大概流程：<code>*.bld + *.cfg ==&gt; *.cmd</code>。</p>

<p>最终生成执行文件的时候，可以在*.map中查看响应配置是否一致。</p>

<h2 id="edma">EDMA</h2>

<p>The EDMA allows data transfer to/from any addressable memory spaces.</p>

<p>EDMA <a href="http://downloads.ti.com/dsps/dsps_registered_sw/sdo_sb/targetcontent/psp/edma3_lld/edma3_lld_1_03/edma3_lld_1_03_00/packages/ti/sdo/edma3/drv/docs/html/edma3__drv_8h.html" title="edma3_drv.h File Reference">API</a></p>

<p><a href="http://processors.wiki.ti.com/index.php/TI81XX_PSP_EDMA_Driver_User_Guide">TI81XX PSP EDMA Driver User Guide</a></p>

<h2 id="cache">Cache</h2>

<h3 id="cachesram">Cache与SRAM</h3>
<p>Cache 是位于 CPU与主存储器DRAM（Dynamic RAM，动态存储器）之间的少量超高速静态存储器 SRAM（static RAM），其是为了解决 CPU 与 主存之间速度匹配问题而设置的，不能由用户直接寻址访问。SRAM是存取速度较快、价格较高的硬件，Cache是基于SRAM的告诉IO系统。SRAM可以直接寻址访问。如果要使用L1DSRAM或L1PSRAM， 则应该相应减小cache段大小。L2 is configured as SRAM (addressable internal memory). L2 cache can be enabled in the program code by issuing the appropriate chip support library(CSL). </p>

<p>Cache大小可通过cmd设置，那些外存可被Cache可通过Mar设置。</p>

<p>我的理解是SRAM是物理存储，速度快、价格高，Cache是基于高速SRAM的一套系统高速IO机制。SRAM可分区（通过cmd文件）：分为Cache ＋ SRAM，Cache的使用由系统控制，SRAM可由用户控制，比如把相应的段塞入；掩码MAR设置的作用是哪些内存对应的内容可被系统Cache！</p>

<h3 id="dont-forget-the-mar-bits">Don’t forget the MAR bits!</h3>

<p>By setting the MAR register to 1 you are telling the 64x+ core that it is allowed to cache data in the corresponding 16MB section of memory. If, for example, you did not set any of the MAR bits corresponding to your DDR, then none of the data in DDR would actually get cached! So as you can imagine, this is a very important step.</p>

<p>You only need to configure the MAR bits for the external memory. When it comes to the internal memory the corresponding MAR bits are already set for you (i.e. L2 SRAM can be allocated in L1D cache). Furthermore you would never want to turn them off as there is no need since data coherence is maintained by the hardware between L1D and L2.</p>

<p>The L2 cache is disabled (0k) meaning that all the L2 memory will be available as SRAM. </p>

<p>If you wish to programmatically configure/reconfigure the cache at run-time you should use the BCACHE module that is part of DSP/BIOS. </p>

<p>In the version 7.0 release of the C6000 code generation tools, a new cache layout tool, clt6x, is included. clt6x will take dynamic profile information in the form of a weighted call graph (WCG) and create a preferred function order command file that can be input into the linker to guide the placement of function subsections.</p>

<p><a href="http://processors.wiki.ti.com/index.php/Enabling_64x%2B_Cache">Enabling 64x+ Cache</a><br />
<a href="http://processors.wiki.ti.com/index.php/Cache_Management">Cache Management</a></p>

<h2 id="section-1">内部指令</h2>

<h3 id="mem4--amem4">_mem4 与 _amem4</h3>
<p><code>_amem4</code>取值按地址4字节对齐，可以这样理解<code>_amem4(p_str) = _amem4(p_str &amp; 0xfffffffc)</code>。</p>

<h2 id="section-2">代码优化</h2>

<h3 id="section-3">反馈信息</h3>

<p>Known Max Trip Count Factor ＝ MUST_ITERATE的第3参数/Loop Unroll Multiple    <br />
Known Maximum Trip Count是Known Max Trip Count Factor的整数倍</p>

<h3 id="pipeline">Pipeline</h3>

<div class="highlight"><pre><code class="c"><span class="c1">// N强制设置为8的倍数，自动流水线</span>
<span class="n">N</span> <span class="o">&amp;=</span> <span class="mh">0xfffff8</span><span class="p">;</span>
<span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span> <span class="c1">//...}</span>

<span class="c1">// 但是，但是，但是，这样不行：</span>
<span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">&amp;</span> <span class="mh">0xfffff8</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span> <span class="c1">//...}</span>
</code></pre></div>

<div class="highlight"><pre><code class="c"><span class="c1">// EP_SIZE为常量，自动流水线</span>
<span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">EP_SIZE</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span> <span class="c1">//...}</span>
</code></pre></div>

<h2 id="edma-1">EDMA</h2>

<p>Again, as we’ve already documented, use ACPY3 for algorithms and some application code when the need is to only do a memory to memory transfer without synchronization. Use LLD when synchronization, indexing, linking and chaining is desired.</p>

<h2 id="section-4">参考资料</h2>
<p><a href="http://processors.wiki.ti.com/index.php/Main_Page">Texas Instruments Wiki</a><br />
<a href="http://www.cnblogs.com/just4fun/archive/2011/03/14/1983292.html">TI DaVinci(达芬奇)入门</a></p>

<div class="footnotes">
  <ol>
    <li id="fn:sysbios1">
      <p>TI SYS/BIOS v6.35 Real-time Operating System User’s Guide (spruex3m: 6.6.2 Configure Parameters to Set MAR Registers) <a href="#fnref:sysbios1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:sysbios2">
      <p>SYS/BIOS API Documenation <a href="#fnref:sysbios2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:CMEM1">
      <p>Linux Utils Application Programming Interface (API)  <a href="#fnref:CMEM1" class="reversefootnote">&#8617;</a> <a href="#fnref:CMEM1:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:IPC1">
      <p>SYS/BIOS Inter-Processor Communication (IPC) 1.25 User’s Guide (sprugo6e) <a href="#fnref:IPC1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:XDC1">
      <p>XDC Getting Started Guide <a href="#fnref:XDC1" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Machine Learning</title>
      <link href="http://jiyeqian.bitbucket.org/2013/11/machine-learning"/>
      <pubDate>2013-11-20T00:00:00+08:00</pubDate>
      <author>Jiye Qian</author>
      <guid>http://jiyeqian.bitbucket.org/2013/11/machine-learning</guid>
      <content:encoded><![CDATA[<p>Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed.</p>

<h2 id="linear-regression">线性回归（Linear Regression）</h2>

<p>回归函数（hypothesis）：
\begin{equation}
h_\theta(x) = \theta^Tx = \theta_0x_0 + \theta_1x_1 + \cdots  + \theta_nx_n
\end{equation}</p>

<blockquote>
  <p>常数项$x_0 = 1$。
线性回归模型适用于多项式回归（polynomial regression），例如当$x_1 = x, x_2 = x^2, x_3 = x^3, \ldots $时。</p>
</blockquote>

<p>代价函数（cost function）：
\begin{equation}
J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}{\left(h_{\theta}\left(x^{(i)}\right) - y^{(i)}\right)^2}
\label{eq:cost_function_linear_regression}
\end{equation}</p>

<blockquote>
  <p>$m$为样本数量，$n$为特征数量。</p>
</blockquote>

<h3 id="section">参数估计方法</h3>

<p>\[
\min_\theta J(\theta)
\]</p>

<p>梯度下降法（gradient descent）：</p>

<p>Repeat {
\[
\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)~~~~~~(j = 0, 1, \ldots, n)
\]
}</p>

<p>也就是：</p>

<p>Repeat {
\[
\theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)x_j^{(i)}~~~~~~(j = 0, 1, \ldots, n)
\]
}</p>

<blockquote>
  <p>每轮循环的时候同时更新$\theta_j$（不能在同一轮循环中，先更新部分$\theta_j$，再利用已更新的$\theta_j$更新其它$\theta_j$）。</p>
</blockquote>

<ul>
  <li>线性回归的代价函数$J(\theta)$不存在局部极值（local optima）。</li>
  <li>将所有特征归一（feature scaling）到统一的尺度$-1\le x_i\le 1$有助于提高梯度下降法的速度（不要把$x_0$归一化）。</li>
  <li>学习率$\alpha$太小收敛慢，太大可能错过极值点而不收敛。</li>
</ul>

<p>归一化方法1：</p>

<p>\[
\hat {x_i} = \frac{x_i - x_{mean}}{x_{max}-x_{min}}
\]</p>

<p>归一化方法2：</p>

<p>\[
\hat {x_i} = \frac{x_i - x_{mean}}{x_{std}}
\]</p>

<p>也可用正规方程求解（normal equation）参数：</p>

<p>\begin{equation}
\theta = \left(X^TX\right)^{-1}X^Ty
\end{equation}</p>

<blockquote>
  <p>这也是线性回归的最小二乘解。</p>
</blockquote>

<p>处理$X^TX$不可逆的方法：   </p>

<ul>
  <li>冗余特征（redundant features）线性相关：……。</li>
  <li>特征数目过多（e.g. $m \le n$）：删除特征、正则化（regularization）。</li>
</ul>

<p>Matlab Code：</p>

<div class="highlight"><pre><code class="matlab"><span class="n">pinv</span><span class="p">(</span><span class="n">X</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">y</span>
</code></pre></div>

<table>
  <thead>
    <tr>
      <th>Gradient Descent</th>
      <th>Normal Equation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>需要$\alpha$</td>
      <td>不需要$\alpha$</td>
    </tr>
    <tr>
      <td>需要迭代</td>
      <td>不需要迭代</td>
    </tr>
    <tr>
      <td>当特征数$n$很大时（$10^6$）工作良好</td>
      <td>$n$很大时很慢</td>
    </tr>
  </tbody>
</table>

<h2 id="logistic-logistic-regression">Logistic 回归（Logistic Regression）</h2>

<p>Logistic 回归用于解决二分类问题，而非回归问题。</p>

<p>回归模型：</p>

<p>\begin{equation}
h_\theta(x) = g\left(\theta^Tx\right)
\end{equation}</p>

<blockquote>
  <p>若$h_\theta(x) \ge 0.5$，则$y = 1$；若$h_\theta(x) &lt; 0.5$，则$y = 0$。</p>
</blockquote>

<p>其中：
\begin{equation}
g\left(z\right) = \frac{1}{1 + e^{-z}}
\end{equation}</p>

<blockquote>
  <p>上式也称为sigmoid function、logistic function。</p>
</blockquote>

<p>代价函数：</p>

<p>\begin{equation}
J(\theta) = -\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right)\log \left(1-h_\theta\left(x^{(i)}\right)\right)\right)
\end{equation}</p>

<blockquote>
  <p>若套用线性回归的代价函数\eqref{eq:cost_function_linear_regression}，则$J(\theta)$非凸，不利于优化算法。该代价函数可从统计中最大似然估计（maximum likehood estimation）的角度推导。</p>
</blockquote>

<h3 id="section-1">参数估计方法</h3>

<p>\[
\min_\theta J(\theta)
\]</p>

<p>梯度下降法（gradient descent）：</p>

<p>Repeat {
\[
\theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)x_j^{(i)}~~~~~~(j = 0, 1, \ldots, n)
\]
}</p>

<blockquote>
  <p>这貌似线性回归的梯度下降法，但$h_\theta$的定义不同。</p>
</blockquote>

<p>其它优化算法：</p>

<ul>
  <li>Conjugate gradient</li>
  <li>BFGS</li>
  <li>L-BFGS</li>
</ul>

<blockquote>
  <p>其它优化算法不需要手工选择学习率$\alpha$，通常也更快，但是更复杂。</p>
</blockquote>

<h3 id="one-vs-all">处理多分类问题（One-vs-all）</h3>

<ul>
  <li>针对每一类训练一个logistic 分类器$h_\theta^{(i)}(x)$，这是第$i$类与其它类别的二分类问题。</li>
  <li>新输入$x$所属的类别满足$\max_ih_\theta^{(i)}(x)$（属于概率最大的那个类别）。</li>
</ul>

<h2 id="regularization">正则化（Regularization）</h2>

<p>过拟合（overfitting）：出色的拟合训练集却不能合适的拟合新数据。</p>

<p>解决过拟合问题的方法：</p>

<ul>
  <li>减少特征数目：手工选择特征、利用模型选择。</li>
  <li>正则化方法：保留所有特征，但是减小$\theta_j$，使特征对预测$y$贡献小。</li>
</ul>

<h3 id="section-2">正则化线性回归</h3>

<p>代价函数：</p>

<p>\begin{equation}
J(\theta) = \frac{1}{2m}\left( \sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)^2 + \lambda\sum_{j=1}^n\theta_j^2 \right)
\end{equation}</p>

<blockquote>
  <p>增大$\lambda$以减小过拟合；但是，$\lambda$过大（$10^{10}$）时，有$\theta_j\approx 0~~(j = 1,2,\ldots,n)$，则$h_\theta(x) = \theta_0$，这会导致欠拟合（underfitting）。</p>
</blockquote>

<p>梯度下降法估计参数：</p>

<p>Repeat {
\[
\begin{aligned}
\theta_0 &amp; := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_0^{(i)} \\ <br />
\theta_j &amp; := \theta_j - \alpha\left(\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_j^{(i)} + \frac{\lambda}{m}\theta_j\right)~~~~~~(j = 1, 2, \ldots, n)
\end{aligned}
\]
}</p>

<blockquote>
  <p>正则化只作用于$j\ge 1$的非常数项。</p>
</blockquote>

<p>迭代过程可以化为如下形式：
\begin{equation}
\theta_j := \theta_j\left(1 - \alpha\frac{\lambda}{m} \right) - \alpha\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_j^{(i)}~~~~~~(j = 1, 2, \ldots, n)
\end{equation}</p>

<p>通常$1 - \alpha\frac{\lambda}{m} &lt; 1$，与非正则化的梯度下降法比较，$\theta_j$减小。</p>

<p>正规方程估计参数：</p>

<p>\begin{equation}
\theta = \left(X^TX + \lambda
\begin{vmatrix}
0  &amp;   &amp;        &amp; \\
   &amp; 1 &amp;        &amp; \\
   &amp;   &amp; \ddots &amp; \\
   &amp;   &amp;        &amp; 1
\end{vmatrix}
\right)^{-1}X^Ty
\end{equation}</p>

<h3 id="logistic-">正则化Logistic 回归</h3>

<p>代价函数：</p>

<p>\begin{equation}
J(\theta) = -\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right)\log \left(1-h_\theta\left(x^{(i)}\right)\right)\right) ＋ \frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2
\end{equation}</p>

<p>梯度下降法估计参数：</p>

<p>Repeat {
\[
\begin{aligned}
\theta_0 &amp; := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_0^{(i)} \\ <br />
\theta_j &amp; := \theta_j - \alpha\left(\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_j^{(i)} + \frac{\lambda}{m}\theta_j\right)~~~~~~(j = 1, 2, \ldots, n)
\end{aligned}
\]
}</p>

<h3 id="logistic--1">正则化Logistic 回归的实现</h3>

<p>第一步：实现Logistic函数</p>

<div class="highlight"><pre><code class="matlab"><span class="k">function</span><span class="w"> </span>g <span class="p">=</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span>z<span class="p">)</span><span class="w"></span>
<span class="n">g</span> <span class="p">=</span> <span class="mf">1.0</span> <span class="o">./</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="nb">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">));</span>
<span class="k">end</span>
</code></pre></div>

<p>第二步：实现代价函数（包含梯度计算）</p>

<div class="highlight"><pre><code class="matlab"><span class="k">function</span><span class="w"> </span>[J, grad] <span class="p">=</span><span class="w"> </span><span class="nf">costFunctionReg</span><span class="p">(</span>theta, X, y, lambda<span class="p">)</span><span class="w"></span>
<span class="n">m</span> <span class="p">=</span> <span class="nb">length</span><span class="p">(</span><span class="n">y</span><span class="p">);</span> <span class="c">% number of training examples</span>
<span class="n">grad</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">theta</span><span class="p">));</span>

<span class="n">sigmoid_y_predict</span> <span class="p">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="p">);</span>
<span class="n">J</span> <span class="p">=</span> <span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="n">y</span> <span class="o">.*</span> <span class="nb">log</span><span class="p">(</span><span class="n">sigmoid_y_predict</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">.*</span> <span class="nb">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigmoid_y_predict</span><span class="p">))</span> <span class="o">+</span> <span class="c">...</span>
	<span class="p">(</span><span class="n">lambda</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">))</span> <span class="o">*</span> <span class="n">sum</span><span class="p">(</span><span class="n">theta</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">.^</span> <span class="mi">2</span><span class="p">);</span>

<span class="n">delta_y</span> <span class="p">=</span> <span class="n">sigmoid_y_predict</span> <span class="o">-</span> <span class="n">y</span><span class="p">;</span>
<span class="n">grad</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="p">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">delta_y</span> <span class="o">.*</span> <span class="n">X</span><span class="p">(:,</span> <span class="mi">1</span><span class="p">));</span>
<span class="n">grad</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="p">=</span> <span class="p">(</span><span class="n">X</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">delta_y</span> <span class="o">+</span> <span class="n">lambda</span> <span class="o">*</span> <span class="n">theta</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">))</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>
<span class="k">end</span>
</code></pre></div>

<p>第三步：估计参数</p>

<div class="highlight"><pre><code class="matlab"><span class="n">initial_theta</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">1</span><span class="p">);</span>
<span class="n">lambda</span> <span class="p">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="n">options</span> <span class="p">=</span> <span class="n">optimset</span><span class="p">(</span><span class="s">&#39;GradObj&#39;</span><span class="p">,</span> <span class="s">&#39;on&#39;</span><span class="p">,</span> <span class="s">&#39;MaxIter&#39;</span><span class="p">,</span> <span class="mi">400</span><span class="p">);</span>
<span class="p">[</span><span class="n">theta</span><span class="p">,</span> <span class="n">J</span><span class="p">,</span> <span class="n">exit_flag</span><span class="p">]</span> <span class="p">=</span> <span class="c">...</span>
	<span class="n">fminunc</span><span class="p">(@(</span><span class="n">t</span><span class="p">)(</span><span class="n">costFunctionReg</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lambda</span><span class="p">)),</span> <span class="n">initial_theta</span><span class="p">,</span> <span class="n">options</span><span class="p">);</span>
</code></pre></div>

<h2 id="neural-networks">神经网络（Neural Networks）</h2>

<h3 id="section-3">代价函数</h3>

<p>\begin{equation}
\begin{aligned}
J(\Theta) = -\frac{1}{m} &amp; \sum_{i=1}^{m}\sum_{k=1}^{K}\left(y_k^{(i)}\log \left(h_\Theta\left(x^{(i)} \right) \right)_k + \left(1 - y_k^{(i)}\right)\log\left(1 -  \left(h_\Theta\left(x^{(i)} \right) \right)_k \right) \right) \\ 
+ \frac{\lambda}{2m} &amp;  \sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}\left(\Theta_{ji}^{(l)}\right)^2
\end{aligned}
\end{equation}</p>

<blockquote>
  <p>$h_\Theta (x) \in \mathbb{R}^K$，$(h_\Theta (x))_i$是第$i$个输出；$s_l$表示第$l$层神经元的个数（不含bias unit）；共$m$个样本，$K$个输出，$L$层。</p>
</blockquote>

<h3 id="section-4">参数估计方法</h3>

<p>\[
\min_\Theta J(\Theta)
\]</p>

<p>BP算法（BackPropagation algorithm）</p>

<p>各层误差估计：
<!--
\begin{equation}
\left\\{
\begin{aligned}
\delta^{(L)} &amp;= a^{(L)} - y  \\\
\delta^{(i)} &amp;= \left(\Theta^{(i)} \right)^T\delta^{(i+1)} ~.*~ g'\left(z^{(i)}\right)~~~(i = 2, 4, \ldots, L-1)
\end{aligned}
\right.
\label{eq:error_bpxxx}
\end{equation}
--></p>

<p>\begin{equation}
\delta^{(l)} = \left\{
\begin{aligned}
&amp; a^{(l)} - y &amp; (l = L)~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\
&amp; \left(\Theta^{(l)} \right)^T\delta^{(l+1)} ~.*~ g’\left(z^{(l)}\right) &amp; (l = L-1, L-2, \ldots, 2) 
\end{aligned}
\right. 
\label{eq:error_bp}
\end{equation}</p>

<blockquote>
  <p>$g’\left(z^{(l)}\right) = a^{(l)} ~.*~ \left(1 - a^{(l)}\right)$；$z^{(l)} = \Theta^{(l)}a^{(l)}$；$\delta_j^{(l)}$表示第$l$层第$j$个节点的误差；对于bias节点$\delta_0^{(l)}=0$。</p>
</blockquote>

<p>BP算法步骤：</p>

<p>训练集：$\left\{\left(x^{(1)}, y^{(1)}\right),\ldots,\left(x^{(m)}, y^{(m)}\right)\right\}$。</p>

<p>初始化：$\Delta _{ij}^{(l)} = 0$(对所有的$i,j,l$)。</p>

<p>For $i=1$ to $m$：</p>

<ol>
  <li>$a^{(1)} = x^{(i)}$；</li>
  <li>前向计算$a^{(l)}~~(l = 2, 3,\ldots, L)$；</li>
  <li>利用\eqref{eq:error_bp}反向计算误差；</li>
  <li>$\Delta ^{(l)} := \Delta ^{(l)} + \delta^{(l+1)}\left(a^{(l)}\right)^T$（$a^{(l)}$向量也须补1）；</li>
  <li>计算$D_{ij}^{(l)} = \frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta)$：</li>
</ol>

<p>\[
\begin{aligned}
D_{ij}^{(l)} &amp; := \frac{1}{m}\Delta _{ij}^{(l)} + \frac{\lambda}{m}\Theta _{ij}^{(l)} &amp; (j \neq 0) \\
D_{ij}^{(l)} &amp; := \frac{1}{m}\Delta _{ij}^{(l)} &amp; (j = 0)
\end{aligned} 
\]</p>

<p>算法技巧：矩阵展成（unroll）向量：</p>

<ol>
  <li>thetaVec = [Theta1(:); Theta2(:); Theta3(:)]；</li>
  <li>将向量化的待估参数作为costFunction的参数；</li>
  <li>costFunction内部再将向量还原为矩阵计算梯度；</li>
  <li>梯度向量化输出DVec = [D1(:); D2(:); D3(:)]。</li>
</ol>

<p>算法技巧：梯度检查（gradient checking）：</p>

<p>梯度检测方法也可推广到其它需要计算代价函数及其梯度的地方，比如logistic回归的代价函数。</p>

<div class="highlight"><pre><code class="matlab"><span class="k">for</span> <span class="nb">i</span> <span class="p">=</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">n</span>
   <span class="n">thetaPlus</span> <span class="p">=</span> <span class="n">theta</span><span class="p">;</span>
   <span class="n">thetaPlus</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="n">thetaPlus</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="o">+</span> <span class="n">EPSILON</span><span class="p">;</span>
   <span class="n">thetaMinus</span> <span class="p">=</span> <span class="n">theta</span><span class="p">;</span>
   <span class="n">thetaMinus</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="n">thetaMinus</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> – <span class="n">EPSILON</span><span class="p">;</span>
   <span class="n">gradApprox</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="p">(</span><span class="n">J</span><span class="p">(</span><span class="n">thetaPlus</span><span class="p">)</span> – <span class="n">J</span><span class="p">(</span><span class="n">thetaMinus</span><span class="p">));</span>
<span class="k">end</span>
</code></pre></div>

<blockquote>
  <p>theta 是$\Theta$的向量化，正常情况有gradApprox$\approx$DVec，通过比较gradApprox 与BP 算法所得DVec 的差距判断BP 算法的代价函数及其优化算法是否有subtle bugs。</p>
</blockquote>

<div class="highlight"><pre><code class="matlab"><span class="c">% If your backpropagation implementation is correct, then the relative difference will be small (less than 1e-9). </span>
<span class="n">diff</span> <span class="p">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">gradApprox</span> <span class="o">-</span> <span class="n">DVec</span><span class="p">)</span> <span class="o">/</span> <span class="n">norm</span><span class="p">(</span><span class="n">gradApprox</span> <span class="o">+</span> <span class="n">DVec</span><span class="p">);</span>
</code></pre></div>

<blockquote>
  <p>梯度检查应当在训练神经网络之前，可以通过构造一个新的较小规模的神经网络进行检验；若每次训练都检测梯度，速度很慢。</p>
</blockquote>

<p>注意事项：</p>

<p>不可将$\Theta _{ij}^{(l)}$初始化为$0$，若初始化为$0$，每层的所有神经元都是一样的。随机数初始化$-\epsilon\leq\Theta _{ij}^{(l)}\leq\epsilon$，选择$\epsilon$的有效策略是根据每层神经元的数目取$\epsilon=\frac{\sqrt{6}}{\sqrt{L_{in} + L_{out}}}~(L_{in} = s_l,L_{out}=s_{l+1})$，例如：</p>

<div class="highlight"><pre><code class="matlab"><span class="n">Theta1</span> <span class="p">=</span>  <span class="nb">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">INIT_EPSILON</span><span class="p">)</span> <span class="o">-</span> <span class="n">INIT_EPSILON</span><span class="p">;</span>
<span class="n">Theta2</span> <span class="p">=</span>  <span class="nb">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">INIT_EPSILON</span><span class="p">)</span> <span class="o">-</span> <span class="n">INIT_EPSILON</span><span class="p">;</span>
</code></pre></div>

<h2 id="section-5">机器学习系统设计</h2>

<p>交叉验证数据集用于模型选择，测试集用于模型评估。</p>

<p>带有正则化的大规模神经网络解决过拟合问题比小规模神经网络有效，缺点是计算复杂。</p>

<p>交叉验证数据集用于选择最优化的非模型参数（例如正则化参数$\lambda$），训练和测试数据集用于调试算法本身的错误。交叉验证数据集也用于调整logistic回归的阈值。</p>

<p>大规模数据在满足如下两个条件时能取得很好的性能：</p>

<ol>
  <li>特征$x$含有预测$y$所需的足够信息（该领域的专家有信心通过$x$预测$y$）；</li>
  <li>学习算法本身能够表示复杂的模型。</li>
</ol>

<p>For reference:
Accuracy = (true positives + true negatives) / (total examples)
Precision = (true positives) / (true positives + false positives)
Recall = (true positives) / (true positives + false negatives)
F1 score = (2 * precision * recall) / (precision + recall)</p>

<h2 id="support-vector-machine">支持向量机（Support Vector Machine）</h2>

<p>An SVM with only the linear kernel is comparable to logistic regression, so it will likely underfit the data as well.</p>

<h2 id="k-mean">K-mean</h2>
<p>随机选择多轮聚类初始中心开始K-mean，已最好一轮作为聚类结果，可以避免局部极值，得到更好的聚类效果。这种方法对聚类数比较小（2～10）有效，当聚类数目很大时，每轮之间效果差异不明显</p>

<h2 id="pca">PCA</h2>
<p>PCA 之前要mean normalization &amp; feature scaling</p>

<p>validtion &amp; test data sets 也用 training 的PCA映射矩阵（scaling 亦如此）</p>

<h2 id="cf">CF</h2>
<p>When using gradient descent to train a collaborative filtering system, it is NOT okay to initialize all the parameters (x(i) and θ(j)) to zero. You need to initialize them to different values so that you learn different features and parameters (i.e., perform symmetry breaking).</p>

<h2 id="section-6">问题</h2>

<ol>
  <li>特征归一化对正规方程估计线性回归参数有何影响？</li>
  <li>为何减少特征可以避免过拟合？</li>
  <li>Any logical function over binary-valued (0 or 1) inputs $x_1$ and $x_2$ can be (approximately) represented using some neural network.</li>
  <li>A two layer (one input layer, one output layer; no hidden layer) neural network can NOT represent the XOR function.</li>
  <li>多项式构造的特征与“提取的特征”有何区别？</li>
</ol>

<h2 id="section-7">參考資料</h2>

<!--[Andrew	 Ng 的机器学习课程](https://class.coursera.org/ml-004/class/index) -->
<p><a href="http://cs229.stanford.edu/materials.html">CS229 Machine Learning Course Materials</a></p>

]]></content:encoded>
    </item>
    
    <item>
      <title>機器學習基礎</title>
      <link href="http://jiyeqian.bitbucket.org/2013/05/introduction-to-machine-learning"/>
      <pubDate>2013-05-18T00:00:00+08:00</pubDate>
      <author>Jiye Qian</author>
      <guid>http://jiyeqian.bitbucket.org/2013/05/introduction-to-machine-learning</guid>
      <content:encoded><![CDATA[<p>本文的參考文獻主要是<sup id="fnref:YuCourse"><a href="#fn:YuCourse" class="footnote">1</a></sup></p>

<h2 id="section">內容提綱</h2>

<p><a href="#paradigms">機器學習的類型</a>  <br />
<a href="#statical_linear_regression_model">統計線性迴歸模型</a></p>

<p><span id="paradigms"></span></p>

<h3 id="yucourse01">機器學習的類型<sup id="fnref:YuCourse01"><a href="#fn:YuCourse01" class="footnote">2</a></sup></h3>

<ul>
  <li>監督學習（Supervised learning）: 已知$\{x_i, y_i\}$，學習$y=f(x; \theta)$。
    <ol>
      <li>分類（Classification）: $y$是類別號, 例如：主題；</li>
      <li>迴歸（Regression）: $y$是連續變量，例如：溫度、收入；</li>
      <li>排序（Ranking）: $y$是順序編號；</li>
      <li>結構預測（Structured prediction）: 例如：對語句中的詞性標註。</li>
    </ol>
  </li>
  <li>非監督學習（Unsupervised learning）: 已知$\{x_i\}$，學習$y=f(x; \theta)$。
    <ol>
      <li>密度估計（Density estimation）: $y$是密度函數$y=p(x; \theta)$；</li>
      <li>異常檢測（Abnormal detection）: $y$是異常；</li>
      <li>聚類（Data Clustering）</li>
      <li>維數約簡（Dimensionality Reduction）</li>
    </ol>
  </li>
  <li>強化學習（Reinforcement Learning）<sup id="fnref:Rai"><a href="#fn:Rai" class="footnote">3</a></sup>  <sup id="fnref:YaserCourse01"><a href="#fn:YaserCourse01" class="footnote">4</a></sup>  <sup id="fnref:DudaBook"><a href="#fn:DudaBook" class="footnote">5</a></sup>   <sup id="fnref:AndrewCourse2008_16"><a href="#fn:AndrewCourse2008_16" class="footnote">6</a></sup>： 外部環境對輸出只給出評價信息而非正確答案，學習機制通過強化受獎勵的動作來改善自身的性能。
    <ol>
      <li>機器人控制</li>
      <li>自主導航</li>
      <li>遊戲   	</li>
    </ol>
  </li>
  <li>半監督學習（Semi-supervised Learning）：少量已標註的數據$\{x_i, y_i\}和大量未標註的數據$\{x_i\}$…</li>
  <li>主動學習（Active Learning）：主動選擇訓練樣本</li>
  <li>Transfer Learning</li>
</ul>

<!--more-->

<p><span id="statical_linear_regression_model"></span>  </p>

<h3 id="statical-linear-regression-modelyucourse02">統計線性迴歸模型（Statical Linear Regression Model）<sup id="fnref:YuCourse02"><a href="#fn:YuCourse02" class="footnote">7</a></sup></h3>

<p>\begin{equation}
  Y = \beta_0 + \beta_1 X + \epsilon
\end{equation}
$\beta_0$為截斷或偏移（intercept or bias），$\beta_1$為斜率（slope），$\epsilon$為隨機噪聲。</p>

<ul>
  <li>如何從數據估計線性模型的參數$\beta_0$和$\beta_1$？</li>
  <li>如何評價該線性模型是否足夠好？</li>
  <li>如何評價參數$\beta_0$和$\beta_1$的重要性？</li>
</ul>

<p>若訓練數據為$(X_i,Y_i) (i = 1,…,n)$，參數$\beta_0$和$\beta_1$的最小二乘迴歸估計為</p>

<p>\begin{equation}
  [\hat{\beta_0}, \hat{\beta_1}] = \arg\min_{\beta_0, \beta_1}\sum_{i=1}^{n} \left(Y_i - (\beta_0 + \beta_1 X_i)\right)
\end{equation}</p>

<p>統計模型可表示為</p>

<p>\begin{equation}
  Y_i = \beta_0 + \beta_1 X_i + \epsilon_i ~~~(i=1,\ldots,n)
\end{equation}</p>

<p>其中$\epsilon_i$為0均值噪聲，理想的情況是$\epsilon_i$服從高斯分佈$\epsilon_i\thicksim N(0,\sigma^2)$。</p>

<blockquote>
  <p>最小二乘迴歸對異常值敏感。</p>
</blockquote>

<p>殘差$r_i = Y_i - (\hat{\beta_0} + \hat{\beta_1} X_i)$應當為隨機噪聲且接近$\epsilon_i$，否著可以通過增加其它特徵，提升模型性能。</p>

<p>如果殘差非隨機，可以增加非線性特徵項，提升模型性能。設非線性基函數為$[f_1(X_i),\ldots,f_p(X_i)]$，可將基礎的線性模型擴展為</p>

<p>\begin{equation}
  Y_i = \beta_0 + \sum_{j=1}^{p} \beta_j f_j (X_i) + \epsilon_i ~~~(i=1,\ldots,n)
\end{equation}</p>

<p>例如，線性模型可擴展為如下兩種形式</p>

<p>\[
  Y_i = \beta_0 + \beta_1 X_i + \beta_2 X_i^2 + \epsilon_i
\]</p>

<p>以及</p>

<p>\[
  Y_i = \beta_0 + \beta_1 X_i + \beta_2 \max (0, X_i - 68) + \beta_3 \max (0, X_i - 72) + \epsilon_i
\]</p>

<p>由此可見，利用非線性基函數，線性模型可以描述非線性關係。</p>

<p>考慮高維空間$X_i \in R^p$的線性迴歸</p>

<p>\begin{equation}
  Y_i = \beta^T X_i + \epsilon_i ~~~ \epsilon_i \thicksim N(0, \sigma^2)
\end{equation}</p>

<p>其中$\beta = [\beta_1,\ldots,\beta_p] \in R^p$，參數$\beta$的最小二乘估計為</p>

<p>\begin{equation}
	\hat{\beta} = (X^T X)^{-1} X^T Y
\end{equation}</p>

<p>$X$為$n \times p$矩陣，每行（row）是$X_i$的一個數據點，$Y$是$n \times 1$的向量。殘差$r_i \approx \epsilon_i $</p>

<p>\begin{equation}
	r_i = Y_i - {\hat{\beta}}^T X_i 
\end{equation}</p>

<ul>
  <li>若噪聲隨機，如何估計$\sigma^2$?</li>
  <li>噪聲是高斯分布的嗎？</li>
  <li>噪聲的方差相等麽？</li>
  <li>當方差不等時，如何處理？</li>
</ul>

<p>假設噪聲的方差相等，其估計值為</p>

<p>\begin{equation}
	{\hat{\sigma}}^2 = \frac{1}{n-p} \sum_{i=1}^n r_i^2
\end{equation}</p>

<ul>
  <li>分母為$n-p$而非$n$是為了滿足無偏估計</li>
  <li>由於$p$個參數，$residue &lt; noise$</li>
</ul>

<p>如果方差不等，可對其建模</p>

<p>\[
r_i^2 \approx v_0 + v_1 (X_i - 45) + v_2 (X_i - 45)^2
\]</p>

<p>最優的無偏線性估計是加權最小二乘</p>

<p>\begin{equation}
	\hat{\beta} = \arg \min_{\beta} \sum_{i=1}^n w_i (Y_i - \beta^T X_i)^2 
\end{equation}</p>

<p>其中，$w_i = \frac{1}{\sigma_i^2}$。</p>

<h3 id="section-1">模型參數重要性評估</h3>

<h2 id="section-2">參考資料</h2>

<div class="footnotes">
  <ol>
    <li id="fn:YuCourse">
      <p><a href="http://wenku.baidu.com/course/view/49e8b8f67c1cfad6195fa705">Machine Learning（余凯&amp;张潼）</a>  <a href="#fnref:YuCourse" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:YuCourse01">
      <p><a href="http://wenku.baidu.com/course/study/53b9fd0a79563c1ec5da7107#&amp;93e38a6227d3240c8447ef9b">Machine Learning（余凯&amp;张潼）：Introduction to ML and review of linear algebra, probability, statistics (kai) P18</a> <a href="#fnref:YuCourse01" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:Rai">
      <p><a href="http://www.cs.utah.edu/~piyush/teaching/29-11-slides.pdf">Reinforcement Learning (1): Discrete MDP, Value Iteration, Policy Iteration</a> <a href="#fnref:Rai" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:YaserCourse01">
      <p><a href="http://v.163.com/movie/2012/2/3/C/M8FH262HJ_M8FTVDQ3C.html">Learning From Data（Yaser）：The Learning Problem</a>      <a href="#fnref:YaserCourse01" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:DudaBook">
      <p>模式分類（P13） <a href="#fnref:DudaBook" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:AndrewCourse2008_16">
      <p><a href="http://v.163.com/movie/2008/1/2/N/M6SGF6VB4_M6SGKSC2N.html">Machine Learning（Andrew Ng，2008）：16</a> <a href="#fnref:AndrewCourse2008_16" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:YuCourse02">
      <p><a href="http://wenku.baidu.com/course/study/53b9fd0a79563c1ec5da7107#&amp;ea8add07a6c30c2259019e9b">Machine Learning（余凯&amp;张潼）：linear model (tong) </a>       <a href="#fnref:YuCourse02" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
