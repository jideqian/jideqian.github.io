<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN" lang="zh-CN">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="author" content="Jiye Qian" />
    <title>机器学习: 神经网络</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <link href="/feed/" rel="alternate" title="Jiye Qian" type="application/atom+xml" />
    <link rel="stylesheet" href="/assets/css/style.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default_inline.css" />
    <link rel="stylesheet" href="/assets/css/coderay.css" />

    <script type="text/javascript" src="/assets/js/jquery-1.7.1.min.js"></script>
    <script type="text/javascript" src="/assets/js/outliner.js"></script>

    <!-- MathJax for LaTeX -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        "HTML-CSS": { extensions: ["handle-floats.js"] },
        TeX: { equationNumbers: { autoNumber: "AMS" } },
        tex2jax: {
            inlineMath: [['$$$', '$$$'], ['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        }
    });
    </script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3Faa6c4cba1a96584b9d529cb356a6aef2' type='text/javascript'%3E%3C/script%3E"));
</script>


  </head>
<!--  <body>
-->
  <script type="text/javascript">
    function setTimeSpan(){
    	var date = new Date();
    	timeSpan.innerText=date.format('yyyy-MM-dd hh:mm:ss');
    }

    Date.prototype.format = function(format)
		{
    var o =
    	{
    	    "M+" : this.getMonth()+1, //month
    	    "d+" : this.getDate(),    //day
    	    "h+" : this.getHours(),   //hour
    	    "m+" : this.getMinutes(), //minute
    	    "s+" : this.getSeconds(), //second
    	    "q+" : Math.floor((this.getMonth()+3)/3),  //quarter
    	    "S" : this.getMilliseconds() //millisecond
    	}
    	if(/(y+)/.test(format))
    	format=format.replace(RegExp.$1,(this.getFullYear()+"").substr(4 - RegExp.$1.length));
    	for(var k in o)
    	if(new RegExp("("+ k +")").test(format))
    	format = format.replace(RegExp.$1,RegExp.$1.length==1 ? o[k] : ("00"+ o[k]).substr((""+ o[k]).length));
    	return format;
		}
  </script>
  <body onLoad="setInterval(setTimeSpan,1000);">
    <div id="container">
      <div id="main" role="main">
        <header>
        <h1>机器学习: 神经网络</h1>
        </header>
        <nav id="real_nav">
        
          <span><a title="Home" href="/">Home</a></span>
        
          <span><a title="Categories" href="/categories/">Categories</a></span>
        
          <span><a title="Tags" href="/tags/">Tags</a></span>
        
          <span><a title="Logs" href="/logs/">Logs</a></span>
        
          <span><a title="About" href="/about/">About</a></span>
        
          <span><a title="Subscribe" href="/feed/">Subscribe</a></span>
        
        </nav>
        <article class="content">
        <section class="meta">
<span class="time">
  <time datetime="2014-10-22">2014-10-22</time>
</span>

 |
<span class="categories">
  categories
  
  <a href="/categories/#研究学术" title="研究学术">研究学术</a>&nbsp;
  
</span>


 |
<span class="tags">
  tags
  
  <a href="/tags/#机器学习" title="机器学习">机器学习</a>&nbsp;
  
  <a href="/tags/#神经网络" title="神经网络">神经网络</a>&nbsp;
  
</span>

</section>
<section class="post">
<h2 id="section">简介</h2>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-10-21-neural-networks_1.png"><img src="/assets/images/2014-10-21-neural-networks_1.png" alt="神经网络结构" /></a><div class="caption">Figure 1:  神经网络结构 [<a href="/assets/images/2014-10-21-neural-networks_1.png">PNG</a>]</div></div></div>

<p>神经网络是神经元分层级联构成的网络，每个神经元对应一个计算模型。最左边和最右边的层分别称为输入（input）和输出（output）层，中间两层为隐藏层（hidden layer）。</p>

<p>当特征数目巨大时，简单的Logistic回归无法满足需求。神经网络用于解决复杂的非线性问题，可以看成是Logistic回归的组合，上图中每个橙色的神经元（除输入层之外）都对应一个Logistic方程。</p>

<p>对于分类问题，输入层输入原始数据，隐藏层可视为特征提取，输出层的每个神经元对应一个类别标签。输入数据所属的类别是输出层概率最大神经元对应的类别标签。</p>

<p>神经网络通过前向传播计算给定输入对应的输出，通过反向传播估计权值矩阵。</p>

<h2 id="section-1">前向传播</h2>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-10-21-neural-networks_2.png"><img src="/assets/images/2014-10-21-neural-networks_2.png" alt="神经网络前向传播计算" /></a><div class="caption">Figure 2:  神经网络前向传播计算 [<a href="/assets/images/2014-10-21-neural-networks_2.png">PNG</a>]</div></div></div>

<p>神经网络前向传播，从输入到输出，逐层计算。上图所示<a href="#ng_ml_nnr_2014">(Ng, 2014, p. 23)</a>，假设权值矩阵$\Theta^{(i-1)}$已知，第$i$层可通过第$i-1$层和权值矩阵计算。</p>

<p>\begin{equation}
\mathbf a^{(i)} = g\left(\mathbf\Theta^{(i-1)}\mathbf a^{(i-1)}\right)
\end{equation}</p>

<p>$g$是<a href="/2014/10/machine-learning-logistic-regression/#mjx-eqn-eqsigmoid-function">Logistic函数</a>，每层额外增加了一个$\mathbf a_0^{(i)}= 1$的偏移（bias），$\mathbf\Theta^{(i-1)}$的行数为第$i$层神经元个数，列数为第$i-1$层神经元个数加$1$。</p>

<p>如果上图只看隐藏层和输出层，就相当于一个Logistic回归模型。</p>

<h2 id="section-2">反向参数估计</h2>

<p>神经网络通过反向传播估计权值矩阵$\Theta$。</p>

<h3 id="section-3">代价函数</h3>

<p>\begin{equation}
\begin{aligned}
J(\Theta) = -\frac{1}{m} &amp; \sum_{i=1}^{m}\sum_{k=1}^{K}\left(y_k^{(i)}\log \left(h_\Theta\left(x^{(i)} \right) \right)_k + \left(1 - y_k^{(i)}\right)\log\left(1 -  \left(h_\Theta\left(x^{(i)} \right) \right)_k \right) \right) \\ 
+\frac{\lambda}{2m} &amp;  \sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}\left(\Theta_{ji}^{(l)}\right)^2
\end{aligned}
\end{equation}</p>

<blockquote>
  <p>$h_\Theta (x) \in \mathbb{R}^K$，$(h_\Theta (x))_i$是第$i$个输出；$s_l$表示第$l$层神经元的个数（不含bias unit）；共$m$个样本，$K$个输出，$L$层。</p>
</blockquote>

<h3 id="section-4">参数估计方法</h3>

<p>\[
\min_\Theta J(\Theta)
\]</p>

<p>BP算法（BackPropagation algorithm）</p>

<p>各层误差估计：
<!--
\begin{equation}
\left\\{
\begin{aligned}
\delta^{(L)} &amp;= a^{(L)} - y  \\\
\delta^{(i)} &amp;= \left(\Theta^{(i)} \right)^T\delta^{(i+1)} ~.*~ g'\left(z^{(i)}\right)~~~(i = 2, 4, \ldots, L-1)
\end{aligned}
\right.
\label{eq:error_bpxxx}
\end{equation}
--></p>

<p>\begin{equation}
\delta^{(l)} = \left\{
\begin{aligned}
&amp; a^{(l)} - y &amp; (l = L)  \\
&amp; \left(\Theta^{(l)} \right)^T\delta^{(l+1)} ~.*~ g’\left(z^{(l)}\right) &amp; (l = L-1, L-2, \ldots, 2) 
\end{aligned}
\right. 
\label{eq:error_bp}
\end{equation}</p>

<p>其中，$g’\left(z^{(l)}\right) = a^{(l)} ~.*~ \left(1 - a^{(l)}\right)$；$z^{(l)} = \Theta^{(l)}a^{(l)}$；$\delta_j^{(l)}$表示第$l$层第$j$个节点的误差；对于bias节点$\delta_0^{(l)}=0$。</p>

<blockquote>
  <h4 id="bp-">BP 算法</h4>
  <hr />
  <p>训练集：$\left\{\left(x^{(1)}, y^{(1)}\right),\ldots,\left(x^{(m)}, y^{(m)}\right)\right\}$。 <br />
初始化：$\Delta_{ij}^{(l)} = 0$(对所有的$i,j,l$)。<br />
For $i=1$ to $m$ {  <br />
1. $a^{(1)} = x^{(i)}$； <br />
2. 算$a^{(l)}~~(l = 2, 3,\ldots, L)$；<br />
3. 利用\eqref{eq:error_bp}反向计算误差； <br />
4. $\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}\left(a^{(l)}\right)^T$（$a^{(l)}$向量也须补1）； <br />
5. 计算$D_{ij}^{(l)} = \frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta)$：
\[
\begin{aligned}
D_{ij}^{(l)} &amp; := \frac{1}{m}\Delta_{ij}^{(l)} + \frac{\lambda}{m}\Theta_{ij}^{(l)} &amp; (j \neq 0) \\
D_{ij}^{(l)} &amp; := \frac{1}{m}\Delta_{ij}^{(l)} &amp; (j = 0)
\end{aligned} 
\]
}</p>
</blockquote>

<p>算法技巧：矩阵展成（unroll）向量：</p>

<ol>
  <li>thetaVec = [Theta1(:); Theta2(:); Theta3(:)]；</li>
  <li>将向量化的待估参数作为costFunction的参数；</li>
  <li>costFunction内部再将向量还原为矩阵计算梯度；</li>
  <li>梯度向量化输出DVec = [D1(:); D2(:); D3(:)]。</li>
</ol>

<p>算法技巧：梯度检查（gradient checking）：</p>

<p>梯度检测方法也可推广到其它需要计算代价函数及其梯度的地方，比如logistic回归的代价函数。</p>

<div class="highlight"><pre><code class="language-matlab" data-lang="matlab"><span class="k">for</span> <span class="nb">i</span> <span class="p">=</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">n</span>
   <span class="n">thetaPlus</span> <span class="p">=</span> <span class="n">theta</span><span class="p">;</span>
   <span class="n">thetaPlus</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="n">thetaPlus</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="o">+</span> <span class="n">EPSILON</span><span class="p">;</span>
   <span class="n">thetaMinus</span> <span class="p">=</span> <span class="n">theta</span><span class="p">;</span>
   <span class="n">thetaMinus</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="n">thetaMinus</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> – <span class="n">EPSILON</span><span class="p">;</span>
   <span class="n">gradApprox</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="p">(</span><span class="n">J</span><span class="p">(</span><span class="n">thetaPlus</span><span class="p">)</span> – <span class="n">J</span><span class="p">(</span><span class="n">thetaMinus</span><span class="p">));</span>
<span class="k">end</span></code></pre></div>

<blockquote>
  <p>theta 是$\Theta$的向量化，正常情况有gradApprox$\approx$DVec，通过比较gradApprox 与BP 算法所得DVec 的差距判断BP 算法的代价函数及其优化算法是否有subtle bugs。</p>
</blockquote>

<div class="highlight"><pre><code class="language-matlab" data-lang="matlab"><span class="c">% If your backpropagation implementation is correct, then the relative difference will be small (less than 1e-9). </span>
<span class="n">diff</span> <span class="p">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">gradApprox</span> <span class="o">-</span> <span class="n">DVec</span><span class="p">)</span> <span class="o">/</span> <span class="n">norm</span><span class="p">(</span><span class="n">gradApprox</span> <span class="o">+</span> <span class="n">DVec</span><span class="p">);</span></code></pre></div>

<blockquote>
  <p>梯度检查应当在训练神经网络之前，可以通过构造一个新的较小规模的神经网络进行检验；若每次训练都检测梯度，速度很慢。</p>
</blockquote>

<p>注意事项：</p>

<p>不可将$\Theta_{ij}^{(l)}$初始化为$0$，若初始化为$0$，每层的所有神经元都是一样的。随机数初始化$-\epsilon\leq\Theta_{ij}^{(l)}\leq\epsilon$，选择$\epsilon$的有效策略是根据每层神经元的数目取$\epsilon=\frac{\sqrt{6}}{\sqrt{L_{in} + L_{out}}}~(L_{in} = s_l,L_{out}=s_{l+1})$，例如：</p>

<div class="highlight"><pre><code class="language-matlab" data-lang="matlab"><span class="n">Theta1</span> <span class="p">=</span>  <span class="nb">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">INIT_EPSILON</span><span class="p">)</span> <span class="o">-</span> <span class="n">INIT_EPSILON</span><span class="p">;</span>
<span class="n">Theta2</span> <span class="p">=</span>  <span class="nb">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">INIT_EPSILON</span><span class="p">)</span> <span class="o">-</span> <span class="n">INIT_EPSILON</span><span class="p">;</span></code></pre></div>

<h2 id="section-5">思考问题</h2>

<h2 id="section-6">参考资料</h2>

<ol class="bibliography"><li><span id="ng_ml_nnr_2014">Ng, A. (2014). Neural Networks: Representation. <i>Machine Learning</i>. Coursera.</span>
</li></ol>

</section>
<section align="right">
<br/>
<span>
  <a  href="/2014/10/machine-learning-perceptron-learning-algorithm" class="pageNav"  >上一篇</a>
  &nbsp;&nbsp;&nbsp;
  <a  href="/2014/10/css-essential" class="pageNav"  >下一篇</a>
</span>
</section>

	
	<ul class="ds-recent-visitors"></ul>
	<div class="ds-thread" data-thread-key="/2014/10/machine-learning-neural-networks" data-url="http://jiyeqian.bitbucket.org/2014/10/machine-learning-neural-networks" data-title="机器学习: 神经网络">
	</div>
	<script type="text/javascript">
	var first_image = document.getElementsByClassName("post")[0].getElementsByTagName("img")[0]; 
	if (first_image != undefined) {
	document.getElementsByClassName("ds-thread")[0].setAttribute("data-image", first_image.src);
	}
	</script>
		
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"jiyeqian"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>


<!-- <script type="text/javascript"> -->
<!-- $(function(){ -->
<!--   $(document).keydown(function(e) { -->
<!--     var url = false; -->
<!--         if (e.which == 37 || e.which == 72) {  // Left arrow and H -->
<!--          -->
<!--         url = '/2014/10/machine-learning-perceptron-learning-algorithm'; -->
<!--          -->
<!--         } -->
<!--         else if (e.which == 39 || e.which == 76) {  // Right arrow and L -->
<!--          -->
<!--         <1!-- url = 'http://jiyeqian.bitbucket.org/2014/10/css-essential'; --1> -->
<!--         url = '/2014/10/css-essential'; -->
<!--          -->
<!--         } else if (e.which == 75) {  // K -->
<!--           url = '#'; -->
<!--         } else if (e.which == 74) { // J -->
<!--         url = '/2014/10/machine-learning-neural-networks/#timeSpan'; -->
<!--         } -->
<!--         if (url) { -->
<!--             window.location = url; -->
<!--         } -->
<!--   }); -->
<!-- }) -->
<!-- </script> -->

        </article>
      </div>

    <footer>
        <p><small>
            Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> | Copyright 2014 - 2014 by <a href="/about/">Jiye Qian</a> | <span class="label label-info" id="timeSpan"></span></small></p>
    </footer>

    </div>
  </body>
</html>
