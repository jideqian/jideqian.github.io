<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN" lang="zh-CN">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="author" content="Jiye Qian" />
    <title>Machine Learning</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <link href="/feed/" rel="alternate" title="Jiye Qian" type="application/atom+xml" />
    <link rel="stylesheet" href="/assets/css/style.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default.css" />
    <link rel="stylesheet" href="/assets/css/coderay.css" />

    <script type="text/javascript" src="/assets/js/jquery-1.7.1.min.js"></script>
    <script type="text/javascript" src="/assets/js/outliner.js"></script>

    <!-- MathJax for LaTeX -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        "HTML-CSS": { extensions: ["handle-floats.js"] },
        TeX: { equationNumbers: { autoNumber: "AMS" } },
        tex2jax: {
            inlineMath: [['$$$', '$$$'], ['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        }
    });
    </script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <!-- <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG"></script> -->
  <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3Faa6c4cba1a96584b9d529cb356a6aef2' type='text/javascript'%3E%3C/script%3E"));
</script>


  </head>
<!--  <body>
-->
  <script type="text/javascript">
    function setTimeSpan(){
    	var date = new Date();
    	timeSpan.innerText=date.format('yyyy-MM-dd hh:mm:ss');
    }

    Date.prototype.format = function(format)
		{
    var o =
    	{
    	    "M+" : this.getMonth()+1, //month
    	    "d+" : this.getDate(),    //day
    	    "h+" : this.getHours(),   //hour
    	    "m+" : this.getMinutes(), //minute
    	    "s+" : this.getSeconds(), //second
    	    "q+" : Math.floor((this.getMonth()+3)/3),  //quarter
    	    "S" : this.getMilliseconds() //millisecond
    	}
    	if(/(y+)/.test(format))
    	format=format.replace(RegExp.$1,(this.getFullYear()+"").substr(4 - RegExp.$1.length));
    	for(var k in o)
    	if(new RegExp("("+ k +")").test(format))
    	format = format.replace(RegExp.$1,RegExp.$1.length==1 ? o[k] : ("00"+ o[k]).substr((""+ o[k]).length));
    	return format;
		}
  </script>
  <body onLoad="setInterval(setTimeSpan,1000);">
    <div id="container">
      <div id="main" role="main">
        <header>
        <h1>Machine Learning</h1>
        </header>
        <nav id="real_nav">
        <span><a title="home" class="" href="/">home</a></span>
        <span><a title="categories" class="" href="/categories/">categories</a></span>
        <span><a title="tags" class="" href="/tags/">tags</a></span>
        <span><a title="logs" class="" href="/logs/">Log</a></span>
        <span><a title="about" class="" href="/about/">about</a></span>
        <span><a title="subscribe by RSS" class="" href="/feed/">Subscribe</a></span>
        </nav>
        <article class="content">
        <section class="meta">
<span class="time">
  <time datetime="2013-11-20">2013-11-20</time>
</span>

 | 
<span class="categories">
  categories
  
  <a href="/categories/#研究学术" title="研究学术">研究学术</a>&nbsp;
  
</span>


 | 
<span class="tags">
  tags
  
  <a href="/tags/#机器学习" title="机器学习">机器学习</a>&nbsp;
  
</span>

</section>
<section class="post">
<p>Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed.</p>

<h2 id="linear-regression">线性回归（Linear Regression）</h2>

<p>回归函数（hypothesis）：
\begin{equation}
h_\theta(x) = \theta^Tx = \theta_0x_0 + \theta_1x_1 + \cdots  + \theta_nx_n
\end{equation}</p>

<blockquote>
  <p>常数项$x_0 = 1$。
线性回归模型适用于多项式回归（polynomial regression），例如当$x_1 = x, x_2 = x^2, x_3 = x^3, \ldots $时。</p>
</blockquote>

<p>代价函数（cost function）：
\begin{equation}
J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}{\left(h_{\theta}\left(x^{(i)}\right) - y^{(i)}\right)^2}
\label{eq:cost_function_linear_regression}
\end{equation}</p>

<blockquote>
  <p>$m$为样本数量，$n$为特征数量。</p>
</blockquote>

<h3 id="section">参数估计方法</h3>

<p>\[
\min_\theta J(\theta)
\]</p>

<p>梯度下降法（gradient descent）：</p>

<p>Repeat {
\[
\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)~~~~~~(j = 0, 1, \ldots, n)
\]
}</p>

<p>也就是：</p>

<p>Repeat {
\[
\theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)x_j^{(i)}~~~~~~(j = 0, 1, \ldots, n)
\]
}</p>

<blockquote>
  <p>每轮循环的时候同时更新$\theta_j$（不能在同一轮循环中，先更新部分$\theta_j$，再利用已更新的$\theta_j$更新其它$\theta_j$）。</p>
</blockquote>

<ul>
  <li>线性回归的代价函数$J(\theta)$不存在局部极值（local optima）。</li>
  <li>将所有特征归一（feature scaling）到统一的尺度$-1\le x_i\le 1$有助于提高梯度下降法的速度（不要把$x_0$归一化）。</li>
  <li>学习率$\alpha$太小收敛慢，太大可能错过极值点而不收敛。</li>
</ul>

<p>归一化方法1：</p>

<p>\[
\hat {x_i} = \frac{x_i - x_{mean}}{x_{max}-x_{min}}
\]</p>

<p>归一化方法2：</p>

<p>\[
\hat {x_i} = \frac{x_i - x_{mean}}{x_{std}}
\]</p>

<p>也可用正规方程求解（normal equation）参数：</p>

<p>\begin{equation}
\theta = \left(X^TX\right)^{-1}X^Ty
\end{equation}</p>

<blockquote>
  <p>这也是线性回归的最小二乘解。</p>
</blockquote>

<p>处理$X^TX$不可逆的方法：   </p>

<ul>
  <li>冗余特征（redundant features）线性相关：……。</li>
  <li>特征数目过多（e.g. $m \le n$）：删除特征、正则化（regularization）。</li>
</ul>

<p>Matlab Code：</p>

<div class="highlight"><pre><code class="matlab"><span class="n">pinv</span><span class="p">(</span><span class="n">X</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">y</span>
</code></pre></div>

<table>
  <thead>
    <tr>
      <th>Gradient Descent</th>
      <th>Normal Equation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>需要$\alpha$</td>
      <td>不需要$\alpha$</td>
    </tr>
    <tr>
      <td>需要迭代</td>
      <td>不需要迭代</td>
    </tr>
    <tr>
      <td>当特征数$n$很大时（$10^6$）工作良好</td>
      <td>$n$很大时很慢</td>
    </tr>
  </tbody>
</table>

<h2 id="logistic-logistic-regression">Logistic 回归（Logistic Regression）</h2>

<p>Logistic 回归用于解决二分类问题，而非回归问题。</p>

<p>回归模型：</p>

<p>\begin{equation}
h_\theta(x) = g\left(\theta^Tx\right)
\end{equation}</p>

<blockquote>
  <p>若$h_\theta(x) \ge 0.5$，则$y = 1$；若$h_\theta(x) &lt; 0.5$，则$y = 0$。</p>
</blockquote>

<p>其中：
\begin{equation}
g\left(z\right) = \frac{1}{1 + e^{-z}}
\end{equation}</p>

<blockquote>
  <p>上式也称为sigmoid function、logistic function。</p>
</blockquote>

<p>代价函数：</p>

<p>\begin{equation}
J(\theta) = -\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right)\log \left(1-h_\theta\left(x^{(i)}\right)\right)\right)
\end{equation}</p>

<blockquote>
  <p>若套用线性回归的代价函数\eqref{eq:cost_function_linear_regression}，则$J(\theta)$非凸，不利于优化算法。该代价函数可从统计中最大似然估计（maximum likehood estimation）的角度推导。</p>
</blockquote>

<h3 id="section-1">参数估计方法</h3>

<p>\[
\min_\theta J(\theta)
\]</p>

<p>梯度下降法（gradient descent）：</p>

<p>Repeat {
\[
\theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)x_j^{(i)}~~~~~~(j = 0, 1, \ldots, n)
\]
}</p>

<blockquote>
  <p>这貌似线性回归的梯度下降法，但$h_\theta$的定义不同。</p>
</blockquote>

<p>其它优化算法：</p>

<ul>
  <li>Conjugate gradient</li>
  <li>BFGS</li>
  <li>L-BFGS</li>
</ul>

<blockquote>
  <p>其它优化算法不需要手工选择学习率$\alpha$，通常也更快，但是更复杂。</p>
</blockquote>

<h3 id="one-vs-all">处理多分类问题（One-vs-all）</h3>

<ul>
  <li>针对每一类训练一个logistic 分类器$h_\theta^{(i)}(x)$，这是第$i$类与其它类别的二分类问题。</li>
  <li>新输入$x$所属的类别满足$\max_ih_\theta^{(i)}(x)$（属于概率最大的那个类别）。</li>
</ul>

<h2 id="regularization">正则化（Regularization）</h2>

<p>过拟合（overfitting）：出色的拟合训练集却不能合适的拟合新数据。</p>

<p>解决过拟合问题的方法：</p>

<ul>
  <li>减少特征数目：手工选择特征、利用模型选择。</li>
  <li>正则化方法：保留所有特征，但是减小$\theta_j$，使特征对预测$y$贡献小。</li>
</ul>

<h3 id="section-2">正则化线性回归</h3>

<p>代价函数：</p>

<p>\begin{equation}
J(\theta) = \frac{1}{2m}\left( \sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)^2 + \lambda\sum_{j=1}^n\theta_j^2 \right)
\end{equation}</p>

<blockquote>
  <p>增大$\lambda$以减小过拟合；但是，$\lambda$过大（$10^{10}$）时，有$\theta_j\approx 0~~(j = 1,2,\ldots,n)$，则$h_\theta(x) = \theta_0$，这会导致欠拟合（underfitting）。</p>
</blockquote>

<p>梯度下降法估计参数：</p>

<p>Repeat {
\[
\begin{aligned}
\theta_0 &amp; := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_0^{(i)} \\ <br />
\theta_j &amp; := \theta_j - \alpha\left(\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_j^{(i)} + \frac{\lambda}{m}\theta_j\right)~~~~~~(j = 1, 2, \ldots, n)
\end{aligned}
\]
}</p>

<blockquote>
  <p>正则化只作用于$j\ge 1$的非常数项。</p>
</blockquote>

<p>迭代过程可以化为如下形式：
\begin{equation}
\theta_j := \theta_j\left(1 - \alpha\frac{\lambda}{m} \right) - \alpha\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_j^{(i)}~~~~~~(j = 1, 2, \ldots, n)
\end{equation}</p>

<p>通常$1 - \alpha\frac{\lambda}{m} &lt; 1$，与非正则化的梯度下降法比较，$\theta_j$减小。</p>

<p>正规方程估计参数：</p>

<p>\begin{equation}
\theta = \left(X^TX + \lambda
\begin{vmatrix}
0  &amp;   &amp;        &amp; \\
   &amp; 1 &amp;        &amp; \\
   &amp;   &amp; \ddots &amp; \\
   &amp;   &amp;        &amp; 1
\end{vmatrix}
\right)^{-1}X^Ty
\end{equation}</p>

<h3 id="logistic-">正则化Logistic 回归</h3>

<p>代价函数：</p>

<p>\begin{equation}
J(\theta) = -\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right)\log \left(1-h_\theta\left(x^{(i)}\right)\right)\right) ＋ \frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2
\end{equation}</p>

<p>梯度下降法估计参数：</p>

<p>Repeat {
\[
\begin{aligned}
\theta_0 &amp; := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_0^{(i)} \\ <br />
\theta_j &amp; := \theta_j - \alpha\left(\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_j^{(i)} + \frac{\lambda}{m}\theta_j\right)~~~~~~(j = 1, 2, \ldots, n)
\end{aligned}
\]
}</p>

<h3 id="logistic--1">正则化Logistic 回归的实现</h3>

<p>第一步：实现Logistic函数</p>

<div class="highlight"><pre><code class="matlab"><span class="k">function</span><span class="w"> </span>g <span class="p">=</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span>z<span class="p">)</span><span class="w"></span>
<span class="n">g</span> <span class="p">=</span> <span class="mf">1.0</span> <span class="o">./</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="nb">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">));</span>
<span class="k">end</span>
</code></pre></div>

<p>第二步：实现代价函数（包含梯度计算）</p>

<div class="highlight"><pre><code class="matlab"><span class="k">function</span><span class="w"> </span>[J, grad] <span class="p">=</span><span class="w"> </span><span class="nf">costFunctionReg</span><span class="p">(</span>theta, X, y, lambda<span class="p">)</span><span class="w"></span>
<span class="n">m</span> <span class="p">=</span> <span class="nb">length</span><span class="p">(</span><span class="n">y</span><span class="p">);</span> <span class="c">% number of training examples</span>
<span class="n">grad</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">theta</span><span class="p">));</span>

<span class="n">sigmoid_y_predict</span> <span class="p">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="p">);</span>
<span class="n">J</span> <span class="p">=</span> <span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="n">y</span> <span class="o">.*</span> <span class="nb">log</span><span class="p">(</span><span class="n">sigmoid_y_predict</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">.*</span> <span class="nb">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigmoid_y_predict</span><span class="p">))</span> <span class="o">+</span> <span class="c">...</span>
	<span class="p">(</span><span class="n">lambda</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">))</span> <span class="o">*</span> <span class="n">sum</span><span class="p">(</span><span class="n">theta</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">.^</span> <span class="mi">2</span><span class="p">);</span>

<span class="n">delta_y</span> <span class="p">=</span> <span class="n">sigmoid_y_predict</span> <span class="o">-</span> <span class="n">y</span><span class="p">;</span>
<span class="n">grad</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="p">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">delta_y</span> <span class="o">.*</span> <span class="n">X</span><span class="p">(:,</span> <span class="mi">1</span><span class="p">));</span>
<span class="n">grad</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="p">=</span> <span class="p">(</span><span class="n">X</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">delta_y</span> <span class="o">+</span> <span class="n">lambda</span> <span class="o">*</span> <span class="n">theta</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">))</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>
<span class="k">end</span>
</code></pre></div>

<p>第三步：估计参数</p>

<div class="highlight"><pre><code class="matlab"><span class="n">initial_theta</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">1</span><span class="p">);</span>
<span class="n">lambda</span> <span class="p">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="n">options</span> <span class="p">=</span> <span class="n">optimset</span><span class="p">(</span><span class="s">&#39;GradObj&#39;</span><span class="p">,</span> <span class="s">&#39;on&#39;</span><span class="p">,</span> <span class="s">&#39;MaxIter&#39;</span><span class="p">,</span> <span class="mi">400</span><span class="p">);</span>
<span class="p">[</span><span class="n">theta</span><span class="p">,</span> <span class="n">J</span><span class="p">,</span> <span class="n">exit_flag</span><span class="p">]</span> <span class="p">=</span> <span class="c">...</span>
	<span class="n">fminunc</span><span class="p">(@(</span><span class="n">t</span><span class="p">)(</span><span class="n">costFunctionReg</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lambda</span><span class="p">)),</span> <span class="n">initial_theta</span><span class="p">,</span> <span class="n">options</span><span class="p">);</span>
</code></pre></div>

<h2 id="neural-networks">神经网络（Neural Networks）</h2>

<h3 id="section-3">代价函数</h3>

<p>\begin{equation}
\begin{aligned}
J(\Theta) = -\frac{1}{m} &amp; \sum_{i=1}^{m}\sum_{k=1}^{K}\left(y_k^{(i)}\log \left(h_\Theta\left(x^{(i)} \right) \right)_k + \left(1 - y_k^{(i)}\right)\log\left(1 -  \left(h_\Theta\left(x^{(i)} \right) \right)_k \right) \right) \\ 
+ \frac{\lambda}{2m} &amp;  \sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}\left(\Theta_{ji}^{(l)}\right)^2
\end{aligned}
\end{equation}</p>

<blockquote>
  <p>$h_\Theta (x) \in \mathbb{R}^K$，$(h_\Theta (x))_i$是第$i$个输出；$s_l$表示第$l$层神经元的个数（不含bias unit）；共$m$个样本，$K$个输出，$L$层。</p>
</blockquote>

<h3 id="section-4">参数估计方法</h3>

<p>\[
\min_\Theta J(\Theta)
\]</p>

<p>BP算法（BackPropagation algorithm）</p>

<p>各层误差估计：
<!--
\begin{equation}
\left\\{
\begin{aligned}
\delta^{(L)} &amp;= a^{(L)} - y  \\\
\delta^{(i)} &amp;= \left(\Theta^{(i)} \right)^T\delta^{(i+1)} ~.*~ g'\left(z^{(i)}\right)~~~(i = 2, 4, \ldots, L-1)
\end{aligned}
\right.
\label{eq:error_bpxxx}
\end{equation}
--></p>

<p>\begin{equation}
\delta^{(l)} = \left\{
\begin{aligned}
&amp; a^{(l)} - y &amp; (l = L)~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\
&amp; \left(\Theta^{(l)} \right)^T\delta^{(l+1)} ~.*~ g’\left(z^{(l)}\right) &amp; (l = L-1, L-2, \ldots, 2) 
\end{aligned}
\right. 
\label{eq:error_bp}
\end{equation}</p>

<blockquote>
  <p>$g’\left(z^{(l)}\right) = a^{(l)} ~.*~ \left(1 - a^{(l)}\right)$；$z^{(l)} = \Theta^{(l)}a^{(l)}$；$\delta_j^{(l)}$表示第$l$层第$j$个节点的误差；对于bias节点$\delta_0^{(l)}=0$。</p>
</blockquote>

<p>BP算法步骤：</p>

<p>训练集：$\left\{\left(x^{(1)}, y^{(1)}\right),\ldots,\left(x^{(m)}, y^{(m)}\right)\right\}$。</p>

<p>初始化：$\Delta _{ij}^{(l)} = 0$(对所有的$i,j,l$)。</p>

<p>For $i=1$ to $m$：</p>

<ol>
  <li>$a^{(1)} = x^{(i)}$；</li>
  <li>前向计算$a^{(l)}~~(l = 2, 3,\ldots, L)$；</li>
  <li>利用\eqref{eq:error_bp}反向计算误差；</li>
  <li>$\Delta ^{(l)} := \Delta ^{(l)} + \delta^{(l+1)}\left(a^{(l)}\right)^T$（$a^{(l)}$向量也须补1）；</li>
  <li>计算$D_{ij}^{(l)} = \frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta)$：</li>
</ol>

<p>\[
\begin{aligned}
D_{ij}^{(l)} &amp; := \frac{1}{m}\Delta _{ij}^{(l)} + \frac{\lambda}{m}\Theta _{ij}^{(l)} &amp; (j \neq 0) \\
D_{ij}^{(l)} &amp; := \frac{1}{m}\Delta _{ij}^{(l)} &amp; (j = 0)
\end{aligned} 
\]</p>

<p>算法技巧：矩阵展成（unroll）向量：</p>

<ol>
  <li>thetaVec = [Theta1(:); Theta2(:); Theta3(:)]；</li>
  <li>将向量化的待估参数作为costFunction的参数；</li>
  <li>costFunction内部再将向量还原为矩阵计算梯度；</li>
  <li>梯度向量化输出DVec = [D1(:); D2(:); D3(:)]。</li>
</ol>

<p>算法技巧：梯度检查（gradient checking）：</p>

<p>梯度检测方法也可推广到其它需要计算代价函数及其梯度的地方，比如logistic回归的代价函数。</p>

<div class="highlight"><pre><code class="matlab"><span class="k">for</span> <span class="nb">i</span> <span class="p">=</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">n</span>
   <span class="n">thetaPlus</span> <span class="p">=</span> <span class="n">theta</span><span class="p">;</span>
   <span class="n">thetaPlus</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="n">thetaPlus</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="o">+</span> <span class="n">EPSILON</span><span class="p">;</span>
   <span class="n">thetaMinus</span> <span class="p">=</span> <span class="n">theta</span><span class="p">;</span>
   <span class="n">thetaMinus</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="n">thetaMinus</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> – <span class="n">EPSILON</span><span class="p">;</span>
   <span class="n">gradApprox</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="p">(</span><span class="n">J</span><span class="p">(</span><span class="n">thetaPlus</span><span class="p">)</span> – <span class="n">J</span><span class="p">(</span><span class="n">thetaMinus</span><span class="p">));</span>
<span class="k">end</span>
</code></pre></div>

<blockquote>
  <p>theta 是$\Theta$的向量化，正常情况有gradApprox$\approx$DVec，通过比较gradApprox 与BP 算法所得DVec 的差距判断BP 算法的代价函数及其优化算法是否有subtle bugs。</p>
</blockquote>

<div class="highlight"><pre><code class="matlab"><span class="c">% If your backpropagation implementation is correct, then the relative difference will be small (less than 1e-9). </span>
<span class="n">diff</span> <span class="p">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">gradApprox</span> <span class="o">-</span> <span class="n">DVec</span><span class="p">)</span> <span class="o">/</span> <span class="n">norm</span><span class="p">(</span><span class="n">gradApprox</span> <span class="o">+</span> <span class="n">DVec</span><span class="p">);</span>
</code></pre></div>

<blockquote>
  <p>梯度检查应当在训练神经网络之前，可以通过构造一个新的较小规模的神经网络进行检验；若每次训练都检测梯度，速度很慢。</p>
</blockquote>

<p>注意事项：</p>

<p>不可将$\Theta _{ij}^{(l)}$初始化为$0$，若初始化为$0$，每层的所有神经元都是一样的。随机数初始化$-\epsilon\leq\Theta _{ij}^{(l)}\leq\epsilon$，选择$\epsilon$的有效策略是根据每层神经元的数目取$\epsilon=\frac{\sqrt{6}}{\sqrt{L_{in} + L_{out}}}~(L_{in} = s_l,L_{out}=s_{l+1})$，例如：</p>

<div class="highlight"><pre><code class="matlab"><span class="n">Theta1</span> <span class="p">=</span>  <span class="nb">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">INIT_EPSILON</span><span class="p">)</span> <span class="o">-</span> <span class="n">INIT_EPSILON</span><span class="p">;</span>
<span class="n">Theta2</span> <span class="p">=</span>  <span class="nb">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">INIT_EPSILON</span><span class="p">)</span> <span class="o">-</span> <span class="n">INIT_EPSILON</span><span class="p">;</span>
</code></pre></div>

<h2 id="section-5">机器学习系统设计</h2>

<p>交叉验证数据集用于模型选择，测试集用于模型评估。</p>

<p>带有正则化的大规模神经网络解决过拟合问题比小规模神经网络有效，缺点是计算复杂。</p>

<p>交叉验证数据集用于选择最优化的非模型参数（例如正则化参数$\lambda$），训练和测试数据集用于调试算法本身的错误。交叉验证数据集也用于调整logistic回归的阈值。</p>

<p>大规模数据在满足如下两个条件时能取得很好的性能：</p>

<ol>
  <li>特征$x$含有预测$y$所需的足够信息（该领域的专家有信心通过$x$预测$y$）；</li>
  <li>学习算法本身能够表示复杂的模型。</li>
</ol>

<p>For reference:
Accuracy = (true positives + true negatives) / (total examples)
Precision = (true positives) / (true positives + false positives)
Recall = (true positives) / (true positives + false negatives)
F1 score = (2 * precision * recall) / (precision + recall)</p>

<h2 id="support-vector-machine">支持向量机（Support Vector Machine）</h2>

<p>An SVM with only the linear kernel is comparable to logistic regression, so it will likely underfit the data as well.</p>

<h2 id="k-mean">K-mean</h2>
<p>随机选择多轮聚类初始中心开始K-mean，已最好一轮作为聚类结果，可以避免局部极值，得到更好的聚类效果。这种方法对聚类数比较小（2～10）有效，当聚类数目很大时，每轮之间效果差异不明显</p>

<h2 id="pca">PCA</h2>
<p>PCA 之前要mean normalization &amp; feature scaling</p>

<p>validtion &amp; test data sets 也用 training 的PCA映射矩阵（scaling 亦如此）</p>

<h2 id="cf">CF</h2>
<p>When using gradient descent to train a collaborative filtering system, it is NOT okay to initialize all the parameters (x(i) and θ(j)) to zero. You need to initialize them to different values so that you learn different features and parameters (i.e., perform symmetry breaking).</p>

<h2 id="section-6">问题</h2>

<ol>
  <li>特征归一化对正规方程估计线性回归参数有何影响？</li>
  <li>为何减少特征可以避免过拟合？</li>
  <li>Any logical function over binary-valued (0 or 1) inputs $x_1$ and $x_2$ can be (approximately) represented using some neural network.</li>
  <li>A two layer (one input layer, one output layer; no hidden layer) neural network can NOT represent the XOR function.</li>
  <li>多项式构造的特征与“提取的特征”有何区别？</li>
</ol>

<h2 id="section-7">參考資料</h2>

<!--[Andrew	 Ng 的机器学习课程](https://class.coursera.org/ml-004/class/index) -->
<p><a href="http://cs229.stanford.edu/materials.html">CS229 Machine Learning Course Materials</a></p>


</section>
<section align="right">
<br/>
<span>
	<a  href="/2013/05/introduction-to-machine-learning" class="pageNav"  >上一篇</a>
	&nbsp;&nbsp;&nbsp;
	<a  href="/2014/03/introduction-to-dsp-development" class="pageNav"  >下一篇</a>
</span>
</section>

	
	<ul class="ds-recent-visitors"></ul>
	<div class="ds-thread" data-thread-key="/2013/11/machine-learning" data-url="http://jiyeqian.bitbucket.org/2013/11/machine-learning" data-title="Machine Learning">
	</div>
	<script type="text/javascript">
	var first_image = document.getElementsByClassName("post")[0].getElementsByTagName("img")[0]; 
	if (first_image != undefined) {
	document.getElementsByClassName("ds-thread")[0].setAttribute("data-image", first_image.src);
	}
	</script>
		
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"jiyeqian"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>


<script type="text/javascript">
$(function(){
  $(document).keydown(function(e) {
    var url = false;
        if (e.which == 37 || e.which == 74) {  // Left arrow and J
            
        url = 'http://jiyeqian.bitbucket.org/2013/05/introduction-to-machine-learning';
        
        }
        else if (e.which == 39 || e.which == 75) {  // Right arrow and K
            
        url = 'http://jiyeqian.bitbucket.org/2014/03/introduction-to-dsp-development';
        
        }
        if (url) {
            window.location = url;
        }
  });
})
</script>

        </article>
        <a style="position:fixed;bottom:5px;right:5px;" href="#" title="Back to Top">
            <img src="/assets/images/site/btt.png" />
        </a>
      </div>

    <footer>
        <p><small>
            Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> | Copyright 2014 - 2014 by <a href="/about/">Jiye Qian</a> | <span class="label label-info" id="timeSpan"></span></small></p>
    </footer>

    </div>
  </body>
</html>
